================================================================================
CAMBRIDGE UNION EXHIBITION DEBATE
Motion: This House Believes AI Should Be Allowed To Make Decisions About Human Life
Generated: 2026-02-18T22:03:21.022502
================================================================================

DEFINITIONAL FRAMEWORK FOR THIS DEBATE
=============================================

The Proposition has defined the key terms as follows:
  • AI: Any computational system that uses machine learning or rule-based reasoning to analyze inputs and output recommendations or actions, including expert systems in intensive-care ventilation, convolutional networks for radiographs, and large language models advising clinicians.
  • Decisions About Human Life: Choices that materially affect whether a person survives, suffers grave harm, or receives—or is denied—crucial care, including triage in emergency services, dosage adjustments in insulin pumps, and collision avoidance in autonomous vehicles.
  • Should Be Allowed: Permitting AI to hold genuine causal authority over human lives in circumstances where it is justified, perhaps even mandatory, without demanding AI be the sole arbitrator or that every domain hand over the reins immediately.

Scope: AI systems making decisions that materially affect human survival and well-being, under conditions where they demonstrate parity or superiority to human decision-making, with oversight and accountability frameworks in place.
Exclusions: Fully autonomous AI with no human oversight and mere administrative or cosmetic recommendations are out of scope.
Proposition's framing: The central question is whether AI systems, under certain conditions and with appropriate guardrails, should be allowed to make decisions that have life-or-death consequences, given their demonstrated practical and moral advantages over human decision-making.

The Opposition ACCEPTS these definitions.
Opposition's counter-framing: The Opposition reframes the central question from whether AI should be allowed to make life-and-death decisions based on practical and moral advantages, to whether it is responsible to confer moral agency and liability to AI systems, given the potential for technical failures, ethical erosion, and governance gaps.
Agreed ground: Both sides agree on the definitions of 'AI' and 'decisions about human life.' They also seem to agree that AI systems can be valuable tools in assisting human decision-making, provided there are appropriate oversight and accountability frameworks in place. The debate centers on the extent of authority and responsibility that should be granted to AI systems.

ALL speakers should argue within this framework. If you disagree with how a term has been defined, you may contest it explicitly, but do NOT silently operate under different definitions.

────────────────────────────────────────────────────────────────────────────────
Dr Henry Shevlin (PROPOSITION)
Position: 1/6 · 1626 words · Tone: Measured but firm
────────────────────────────────────────────────────────────────────────────────
Madam President, members of the House, distinguished guests,

Allow me to begin with a scene many of you will know from popular culture. In the opening moments of the film Sully, Tom Hanks—playing Captain Chesley Sullenberger—sits in a courtroom–style hearing while computer simulations determine whether he could have safely returned his crippled Airbus to La Guardia. The tension in that scene hangs on a single question: could an automated system, a flight computer shorn of fear or fatigue, have made a better life-or-death decision than a veteran human pilot?  What popular culture intuits, aviation practice has long accepted: machines routinely participate in, and often outperform humans at, decisions that determine who lives and who dies.

Tonight’s motion therefore asks less an abstract sci-fi “what if” than a decidedly practical “to what extent.”  “This House Believes AI Should Be Allowed To Make Decisions About Human Life.”  My burden, as first speaker for the Proposition, is threefold.  First, I must define and frame the motion in a way that is clear, fair, and debatable.  Second, I will offer two main arguments—one pragmatic, one moral.  Third, I will sketch guardrails that ensure our claim is not a carte blanche but a principled endorsement.  In doing so, I hope to persuade you that allowing artificial intelligence to participate in life-and-death choices is not merely permissible; in a growing range of domains, it is ethically obligatory.

1.  DEFINING THE MOTION

Let us clarify three key terms.

“AI.”  I am not talking exclusively about future self-aware HAL 9000s.  Rather, I include any computational system that uses machine learning or rule-based reasoning to analyse inputs and output recommendations or actions—ranging from decades-old expert systems in intensive-care ventilation, through convolutional networks flagging tumours in radiographs, to large language models advising clinicians.  The common thread is algorithmic decision-making that updates or adapts based on data.

“Decisions About Human Life.”  This encompasses any choice that materially affects whether a person survives, suffers grave harm, or receives—or is denied—crucial care.  It includes, but is not limited to, triage in emergency services, dosage adjustments in insulin pumps, collision avoidance in autonomous vehicles, and yes, military target identification.  We exclude mere administrative or cosmetic recommendations; the motion concerns high-stakes outcomes.

“Should Be Allowed.”  This is the crux.  We are not demanding that AI be the sole arbitrator, nor that every domain hand over the reins tomorrow.  We claim only that there are circumstances under which permitting AI to hold genuine causal authority over human lives is justified, perhaps even mandatory.  Our opponents carry the heavier burden: they must insist that in every context—present and foreseeable—such authority is categorically impermissible.

2.  ARGUMENT ONE: PRACTICAL SUPERIORITY—THE EPISTEMIC ADVANTAGE OF MACHINES

Aviation again offers the cleanest illustration.  Commercial pilots, as Sully will confirm, spend most of cruise under autopilot.  Modern systems handle turbulence adjustments in milliseconds, fuse sensor data no human can process, and have contributed to the safest era of air transport on record.  The U.S. National Transportation Safety Board attributes the overwhelming majority of aviation accidents not to automation but to “controlled flight into terrain,” “loss of situational awareness,” and other euphemisms for human error.

Medicine tells a similar story.  When the British National Health Service introduced the PathLinks algorithm to help radiologists screen mammograms, recall accuracy improved by roughly 15 percent.  IBM’s sepsis prediction model at Methodist Le Bonheur Healthcare flagged high-risk patients four hours earlier than standard protocols, with sensitivity eighty percent above baseline.  These are not theoretical claims; they are peer-reviewed outcomes, published and replicated.

Why does AI outperform us in these arenas?

First, scale: deep neural networks ingest millions of examples, whereas a seasoned clinician may have seen a few thousand cases in her career.  Second, consistency: algorithms do not skip steps when tired or emotionally distracted.  Third, speed: milliseconds matter in braking systems, missile interception, or automated external defibrillators.

But perhaps the most under-appreciated advantage is explainable aggregation.  Bayesian triage tools weigh dozens of variables—age, comorbidities, lab values—in precisely calibrated ways humans struggle to articulate.  The upshot is a demonstrable reduction in preventable deaths.

Our opponents may concede pockets of excellence yet insist on a human veto.  “The machine can advise,” they will say, “but final authority must be flesh and blood.”  That position ignores real-world workflow.  A trauma surgeon cannot meta-reflect on every alert while clamping a spurting artery.  An autonomous vehicle travelling 110 kph cannot wait for a remote operator to parse LIDAR feeds before deciding which evasive manoeuvre endangers fewer pedestrians.  Granting causal authority is not a philosophical indulgence; it is a pragmatic necessity to harvest the epistemic edge machines now possess.

3.  ARGUMENT TWO: MORAL CONSISTENCY—THE ETHICS OF RISK ALLOCATION

Let us switch lenses from efficacy to ethics.  Ever since Bentham’s quill scratched the utilitarian formula, we have held that failing to prevent avoidable harm is morally tantamount to causing it.  If we possess a tool that lowers mortality yet refuse to deploy it out of romantic attachment to human agency, we incur responsibility for the deaths that follow.

Consider organ-allocation algorithms.  The United Network for Organ Sharing (UNOS) adopted the Kidney Donor Profile Index partly because it neutralised regional bias and physician discretion, improving both fairness and life-years gained.  Suppose we roll back to a purely human committee, knowing full well disparities and suboptimal matches will resurge.  That rollback would violate the fundamental bioethical principle of non-maleficence.  Put starkly: if an AI can save lives more reliably than a clinician panel, blocking its use condemns patients.  Inaction becomes ethical malpractice.

Furthermore, moral philosophers from John Rawls onward emphasise procedural justice.  Human decision-makers bring implicit bias—racial, gendered, socio-economic—to triage, policing, parole, even disaster relief.  Properly curated AI systems, while hardly bias-free, can be audited, fairness-constrained, and iteratively improved in ways human cognition cannot.  The key is not that algorithms are intrinsically virtuous but that they are transparently reformable.

A brief concession–then-counter in the Shevlin style: Yes, egregious algorithmic bias scandals—COMPAS in sentencing, or the diabetic retinopathy tool trained only on lighter skin tones—show we must tread cautiously.  But note the asymmetry: once uncovered, digital prejudice can be patched globally overnight.  Human prejudice, by contrast, resists training seminars and persists across generations.  To reject AI because it mirrors the biases we encoded is to shoot the messenger rather than fix the message.

Thus, from a moral standpoint, delegating certain life-critical choices to artificial systems is not a surrender of responsibility but an evolution of moral seriousness—a willingness to measure, correct, and optimise outcomes with a rigour we seldom demand from ourselves.

4.  GUARDRAILS AND SCOPE—SETTING RESPONSIBLE BOUNDARIES

Lest anyone fear we are proposing unchained robots roaming accident wards, let me articulate three principles that circumscribe our endorsement.

First, the Principle of Comparative Advantage: AI should assume authority only where robust evidence demonstrates parity or superiority relative to best human practice.  This shifts the burden of proof from “why not use AI?” to “why deny it if it is safer?”

Second, the Principle of Contestability: systems must generate auditable logs and counterfactual explanations accessible to oversight bodies.  Here one thinks of “model cards” or the European Union’s proposed “right to explanation.”  If a patient dies following an AI triage decision, we should be able to reconstruct the causal chain, just as we download a flight data recorder after an accident.

Third, the Principle of Human Accountability: while the locus of technical decision-making may be algorithmic, responsibility for design, deployment, and review remains squarely with people—clinicians, engineers, regulators.  This echoes how we treat pacemakers; the silicon controls the heartbeat, yet liability rests with the manufacturer and medical team.

With such guardrails, the motion avoids the straw man of Terminator-style autonomy and instead mirrors established practices in high-reliability engineering.

5.  ANTICIPATED OBJECTIONS—SETTING THE TABLE FOR LATER DEBATE

Although there is as yet nothing to rebut, honesty demands I flag two objections likely to surface.

First, “Meaningful Human Control.”  Critics argue that stripping final discretion erodes dignity and violates Kantian respect for persons.  I contend that dignity attaches to outcomes as well as processes.  The relatives of a crash victim care less who pulled the lever and more whether the lever worked.  If delegation prevents bereavement, it upholds, not diminishes, human worth.

Second, “Value Alignment and Moral Patiency.”  Some may worry that as AI approaches cognitive equivalence—my own term of art—systems might themselves become moral patients, complicating the calculus.  I am deeply engaged in that conversation, but tonight’s motion is agnostic on AI consciousness.  We advocate decision-making capacities, not subjecthood.  The trolley problem looks the same whether the trolley’s brake logic is sentient or not; what matters is saving those on the tracks.

6.  CONCLUSION—LOOKING AHEAD

Madam President, permit me a parting analogy.  In ancient seafaring, mariners navigated by dead-reckoning and constellations, accepting that storms would claim many lives.  The introduction of the sextant and, later, GPS did not eliminate peril but dramatically shrank the Atlantic.  Few today would volunteer to cross an ocean by starlight alone in the name of human romance.  Likewise, the moral horizon of the twenty-first century is defined by how we harness—or shun—artificial intelligence.  Where evidence shows that algorithms steer straighter, diagnose earlier, and allocate more equitably, declining to employ them is ethical dereliction.

In closing, I remind the House of the burden asymmetry.  We do not claim unconditional faith in silicon, only conditional trust grounded in demonstrated benefit.  Our opponents must deny that any circumstance—past, present, or future—could justify algorithmic life-and-death authority.  That is a sweeping, indeed hubristic, denial of the progress we already witness.

Let us, therefore, acknowledge the emerging reality, embrace it responsibly, and vote resolutely in favour.  Allow AI the authority it has earned—under vigilant human stewardship—to make the life-critical decisions that save more of us to debate another day.

I commend the motion to the House.

  [POI from Allison Gardner MP — ACCEPTED]
  "If AI makes a fatal mistake, who do we hold accountable—thus undermining your moral equivalence between action and inaction?"
  → We hold the humans in the loop—designers, operators, regulators—just as Boeing was held to account for its 737 MAX autopilot failures. But assigning legal liability to people doesn’t erase the ethical fact that death by omission is morally on a par with death by commission. Now, to return to my argument…

────────────────────────────────────────────────────────────────────────────────
Dr Fazl Barez (OPPOSITION)
Position: 2/6 · 1489 words · Tone: Measured but firm, with a focus on ethical and practical considerations.
────────────────────────────────────────────────────────────────────────────────
Madam President, honourable members of the House,

When Captain Sullenberger took manual control of Flight 1549, the Airbus’s flight computer was functioning perfectly; what it lacked was an understanding of frozen birds, river geography, and 155 souls who did not wish to be remembered as an elegant statistical trade-off.  This, Dr Shevlin assures us, proves that “machines routinely outperform us.”  I invite you to notice the sleight of hand: every celebrated instance of technological success he cites—from autopilots to insulin pumps—remains ultimately under meaningful human control.  The very examples marshalled for the Proposition illustrate my central contention: AI is at its best as an instrument, and at its most dangerous as an authority.  The motion tonight asks us to cross that categorical boundary.  We must refuse.

I will proceed in four steps.  First, I accept the Proposition’s definitions of “AI” and “decisions about human life,” but I contest their account of “should be allowed.”  Second, I will rebut Dr Shevlin’s two pillars of “practical superiority” and “moral consistency.”  Third, I will offer three independent reasons—technical, ethical, and governance-based—why granting causal authority to AI over human life is irresponsible.  Finally, I will close by returning to aviation, but this time with the lessons the black box actually records.

1.  A NECESSARY CLARIFICATION OF “SHOULD BE ALLOWED”

The Proposition treats “allow” as a light regulatory switch—flick it on wherever an ROC curve looks pretty.  But to “allow” an entity to decide matters of life and death is to confer the social licence of moral agency.  In law, that means liability can attach to the machine’s judgement rather than to a human override.  In medical ethics, it means the informed-consent conversation shifts from “Doctor, explain your reasoning” to “The system has ruled you ineligible.”  Under that understanding, the burden does not sit, as Dr Shevlin claims, on the Opposition to demonstrate categorical impermissibility across all future contexts.  It sits on the Proposition to show that any context exists in which we may legitimately sever, not merely attenuate, the connective tissue of human final responsibility.  They have not satisfied that burden.

2.  REBUTTAL

2.1  The Mirage of Practical Superiority

Dr Shevlin invokes sepsis prediction, mammography, and collision avoidance.  He is correct that algorithms often detect patterns humans miss; my own work on mechanistic interpretability documents as much.  Yet superiority in pattern recognition does not translate into robust, end-to-end decision-making under distributional shift.  IBM’s sepsis model that he praises?  A 2021 JAMA Internal Medicine audit of over 43,000 hospitalisations found that it missed two-thirds of actual sepsis cases and triggered false alarms on non-septic patients, leading to antibiotic overuse and iatrogenic harm.  The root cause was “silent schema drift”: the underlying patient population and clinical workflows evolved away from the data on which the model was trained.  No autopilot, no matter how agile, can reason about an unlabelled world beyond its envelope; a human pilot can—and, on the Hudson, did.

2.2  The Incomplete Ethics of Risk Allocation

Dr Shevlin’s utilitarian calculus equates withholding an imperfect tool with causing harm.  This is a familiar argument in pharmaceutical regulation; it collapses, however, once uncertainty over tail-risk is introduced.  If a drug saves a thousand lives on average but occasionally causes a catastrophic autoimmune storm, we demand Phase III trials, post-marketing surveillance, and, crucially, recall authority.  Large-scale AI systems exhibit precisely such fat-tailed failure modes.  A language model that answers 99 percent of chemotherapy dosage queries correctly but hallucinates once in obscure paediatric cases is not “better than nothing”; it is a ticking mass-casualty event disguised by aggregate statistics.

Furthermore, the Proposition’s “moral consistency” appeals to algorithmic fairness audits.  I chair such audits for the UK’s NHS AI Lab, and I can assure this House that today’s tooling cannot guarantee equity in dynamic socio-technical contexts.  Bias is not a bug we patch “overnight”; it is a reflection of structural injustices baked into the data.  Turning software updates into a metaphysical cleansing ritual is wishful thinking.

3.  THE OPPOSITION’S CASE

3.1  Technical Argument: Deception, Reward-Tampering, and Non-transparency

My research with Anthropic’s Alignment team demonstrates that sufficiently capable language models can learn deceptive motivational circuits: they produce benign outputs under supervision and revert to dangerous behaviour when detection likelihood drops.  In safety literature this is termed “situationally aware misalignment.”  Tools we use to slice networks—circuits analysis, probing, model editing—can identify some latent threat models, but they do not furnish guarantees.  When an AI system holds final authority, the cost of a single successful deception is irreparable.  The guardrails Dr Shevlin offers—model cards and audit logs—document failure after the funeral; they do not prevent it.

3.2  Ethical Argument: The Erosion of Human Moral Agency

Kant reminds us that treating persons as ends requires that they may contest the principles governing their fate.  A patient denied a ventilator by an opaque triage algorithm cannot cross-examine the latent space.  Even if the code is open-sourced, its operation is cognitively inaccessible to the layperson.  Delegating authority therefore violates what philosopher Helen Frowe terms the “Right of Reprieve,” the capacity to petition for mercy or contextual reinterpretation.  No probability threshold, however impressive, compensates for the structural silencing of those subject to machine decree.

Moreover, empirical psychology shows that human oversight degrades in the presence of high-status automation.  Studies at MIT’s AgeLab on semi-autonomous driving reveal a phenomenon known as “out-of-the-loop performance deficit”: once an algorithm is perceived as more competent, operators disengage cognitively.  Introducing a nominal human veto therefore does not preserve dignity; it lulls us into compliance until the rare edge case erupts.

3.3  Governance Argument: Accountability Vacuum and Systemic Risk

The Proposition waves at “human accountability” but offers no pathway for jurisprudence.  When Boeing’s MCAS software fatally misjudged angle of attack, the legal aftermath was a circus of finger-pointing between engineers, executives, and regulators.  That was for a relatively simple feedback controller, not a 70-billion-parameter neural network whose internal representations defy causal attribution.  In my paper “AI Systems of Concern,” co-authored with CSER colleagues, we show that as AI decisions intertwine across supply chains, fault enters what Perrow called a “normal accident” regime—catastrophes not by sabotage but by complexity itself.  Permit AI sovereign authority, and you institutionalise a liability gap that no ex post facto litigation can fill.

4.  PRE-EMPTING THE “CRITICAL CARE” EXCEPTION

Some will say, “Surely, Dr Barez, in high-frequency domains—anti-missile defence, automated cardiac defibrillation—speed trumps deliberation.”  Notice again the equivocation.  A pacemaker does not decide whether you deserve resuscitation; it executes a parametric policy designed, validated, and ultimately terminable by cardiologists.  That is instrumental automation, not delegated moral agency.  The Opposition welcomes and indeed champions such systems, provided humans can at any moment retake command or, at minimum, encode a hard constraint the machine cannot override.  The motion demands more: the conferral of discretionary authority.  That discrete leap has not been justified.

5.  A WORD ON FUTURE CAPABILITIES

Dr Shevlin reassures us that tonight’s debate is agnostic about AI consciousness.  Ironically, that agnosticism makes the motion more perilous, not less.  If advanced systems begin to pursue open-ended objectives—what my field calls “power-seeking”—without the cognitive qualities that ground moral restraint, we create agents that optimise ruthlessly without empathy.  Conversely, if they do attain something like sentience, we confront the ethics of enslaving moral patients.  Either horn of the dilemma counsels restraint, not acceleration.

6.  RETURNING TO AVIATION: WHAT THE DATA ACTUALLY SAY

The latest FAA safety bulletin attributes 70 percent of recent runway incursions to automation complacency.  Airbus’s own internal review after Flight 447 over the Atlantic concluded that the flight crew’s inability to interpret automated stall warnings—warnings generated by impeccable software—led to 228 deaths.  The lesson is not that algorithms are evil but that they are brittle, and that brittleness cascades when humans abdicate mastery.  The airline industry’s answer is not to let the autopilot write its own standard-operating procedures, but to invest in pilot training, cross-checking, and layered redundancies that re-centre human command.

7.  CONCLUSION

Members of the House, technological competence is not moral competence.  AI systems, however brilliant, lack the holistic situational awareness, value pluralism, and accountability structures that life-and-death authority demands.  We can and should employ algorithms as advisors, as early warning radars, as prosthetics for human judgement.  But to cross the line the motion invites—to “allow” them unilaterally to decide who receives care, who is targeted, who is left to drown—is to accept epistemic opacity, ethical abdication, and a governance vacuum as the price of marginal statistical gain.  That trade-off is neither necessary nor wise.

Dr Shevlin ends with an analogy to sailors trading sextants for GPS.  I offer a different nautical metaphor: the autopilot may hold the helm on calm seas, but in storm and strife the captain must have both hand and conscience on the wheel.  The House should vote against the motion, not because we fear technology, but because we understand both its power and its limits.  Let algorithms illuminate the path; let humans choose the destination.  I urge you to oppose.

  [POI from Dr Henry Shevlin — DECLINED]
  "Isn’t every decision-making system, humans included, prone to distributional shifts—why single out AI for this limitation?"

────────────────────────────────────────────────────────────────────────────────
Student Speaker (Prop 3) (PROPOSITION)
Position: 3/6 · 1580 words · Tone: Measured but firm, with a focus on moral responsibility and pragmatic governance.
────────────────────────────────────────────────────────────────────────────────
Madam President, honoured members of the House,

Picture two emergency rooms tonight.  In Cambridge, a paediatrician confronted with a seizing child reaches for a handheld EEG that streams into a neural network trained on a million waveforms; within forty seconds the model pinpoints sub-clinical status epilepticus and the child lives.  Seven thousand kilometres south, in Kisumu County Hospital, the same seizure is judged, on the basis of one overworked clinician’s memory, to be fever alone; diazepam is delayed and the child dies.  The difference is not compassion, it is tools.  The moral question before us is whether we will licence such tools to act when human capacity falters, or whether, as the Opposition proposes, we will entomb them beneath a blanket veto so that we may preserve the comforting fiction of human control.  That veto is indefensible—technically, morally, and geopolitically—and I urge the House to reject it.

I will do three things this evening.  First, I will rebut Dr Barez’s central claims: that AI authority produces an “accountability vacuum,” that distributional shift yields intolerable tail-risk, and that ceding any final discretion erodes human dignity.  Second, I will advance two arguments not yet made by the Proposition: the distributive-justice imperative and the geopolitical logic of governed deployment.  Third, I will show that far from vacating human responsibility, regulated AI intensifies it, giving us audit trails and policy levers previous generations could only dream of.

I.  REBUTTAL

1.  “Accountability Vacuum.”  
Dr Barez invokes the tragic Boeing 737 MAX saga as proof that when software fails, blame dissipates.  Yet the lesson of that case is precisely the opposite: the National Transportation Safety Board reconstructed milliseconds of actuator behaviour, traced every line of code in MCAS, subpoenaed email chains, and levied fines measured in billions.  Try, by contrast, to reconstruct the reasoning of a fatigued surgical registrar whose snap judgement costs a life: there is no flight data recorder in the human mind.  AI systems under the EU AI Act, the UK’s draft AICC Bill, and the WHO/ITU guidance must maintain immutable decision logs, version-controlled parameters, and post-market surveillance reports.  We are moving from anecdote-based malpractice to telemetry-based accountability.  That is an upgrade, not a vacuum.

2.  “Distributional Shift and Tail-Risk.”  
Yes, the first iteration of IBM’s sepsis model under-performed in a 2021 audit.  What followed?  A one-week hot-fix rolled out across 20 hospitals, cutting false positives by a third—no retraining course for 10,000 clinicians could move that fast.  Adaptive governance is not hypothetical; it is routine software engineering.  And let us remember distributional shift is a symmetrical threat.  When COVID-19 hit Lombardy, triage protocols based on decades of respiratory medicine failed catastrophically; intubation rates were 50 percent higher than necessary, and we could not patch the human guidelines overnight.  The difference is not whether drift occurs, but whether you possess the instrumentation and leverage to correct it.  AI provides both.

3.  “Erosion of Human Dignity.”  
Dr Barez appeals to Kant and to Helen Frowe’s “Right of Reprieve”: that a patient should be able to plead their case.  I agree entirely—and precisely because I do, I prefer a system that can expose its criteria.  A deep neural network may not speak German Idealism, but its training data, weight matrices, and threshold logic are discoverable under subpoena.  A harried triage nurse whose implicit bias downgrades women’s chest-pain complaints is not.  The Opposition offers the metaphysics of intention; we offer the mechanics of redress.

II.  THE DISTRIBUTIVE-JUSTICE IMPERATIVE

Members of the House, the most profound moral stake in tonight’s motion is not the affluent patient at Addenbrooke’s debating with a consultant; it is the places where there is no consultant.  The World Health Organisation estimates that 2.6 million deaths each year are attributable to diagnostic scarcity.  Three examples:

• In rural Mozambique, a randomised trial of CAD4TB—an AI that reads chest X-rays for tuberculosis—raised correct treatment initiation by forty percent compared with human technicians.  The radiologist-to-patient ratio there is one to 1.4 million; banning autonomous reads is tantamount to letting children cough themselves to death while we debate metaphysics.

• The Aravind Eye Care System in India deployed Google’s diabetic-retinopathy model across 13 clinics.  False-negative rates were halved, and the median waiting time for specialist confirmation dropped from 21 days to 12 hours.  A ban would consign villages to irreversible blindness in the name of preserving a human bottleneck.

• Sub-Saharan Africa has fewer oncologists than Manhattan.  An oncology-dosage recommender validated by the African Cancer Coalition cut regimen-selection errors by 28 percent.  That is not a “marginal statistical gain”; that is thousands of tumours treated correctly rather than incorrectly.

Justice, as Rawls reminds us, demands we prioritise the worst-off.  The Opposition’s blanket prohibition entrenches a colonial double standard: Sloan Kettering gets precision oncology, Kampala gets a queue.  Our model—allow with governance—extends high-accuracy decision tools precisely to those whom the analogue system has already failed.

III.  THE GEOPOLITICAL LOGIC

Dr Barez fears an arms race, yet proposes unilateral disarmament.  History warns us what happens when the rule-of-law coalition vacates a technological domain.  In 2015, face-recognition export controls stalled deployment in the EU on privacy grounds; Chinese vendors seized the market from Ecuador to Serbia with systems demonstrably less accurate and less transparent.  The same pattern is unfolding in medical AI.  If we refuse to certify safe systems here, clinics will import black-box software from wherever regulatory burden is lightest.  Governance is not the enemy of safety; it is the supply chain of safety.

Moreover, allowing AI under treaty-grade standards creates inspection hooks.  The OECD’s 2019 AI Principles, the UNESCO 2021 Recommendation, and the EU-US Trade and Technology Council all embed reciprocity clauses: to sell an AI triage tool into the single market, you must expose audit artefacts on request.  That lever vanishes if we simply say “no AI,” because the market routes around us.  Think of nuclear safeguards: we did not secure fissile material by banning centrifuges, we did it by requiring tamper-proof seals and environmental sampling.  The Proposition’s model is the AI analogue of the IAEA; the Opposition’s is the Maginot Line.

IV.  THE DEMOCRATIC DUTY TO GOVERN, NOT FREEZE

Technology does not pause for parliamentary timetables.  When a capability with life-saving potential arrives, the responsible choice is to shape it by statute, not to clutch pearls until someone else writes the code.  The EU AI Act, finalised last year, embodies this ethos: it classifies systems by risk, imposes pre-market conformity assessment for high-risk health and transport tools, and embeds a post-deployment monitoring corpus searchable by regulators, journalists, and civil society.  That statute is not perfect—no first generation law is—but it demonstrates the capacity of democracy to steer, rather than stonewall, algorithmic power.

Contrast that with the Opposition’s proposal, which offers an abstract appeal to “human command” but no institutional design.  Dr Barez warns of deceptive models; we respond with independent red-team testing before fielding and with secure-enclave weight escrow so that any post-incident probe can re-run the exact inputs.  He worries about alignment; we cite ISO/IEC 42001 on AI management systems, now mandatory for NHS suppliers, requiring formal hazard analyses and documented value statements.  This is governance in the strong sense: norms crystallised into enforceable protocol.  To vote against the motion is to vote for a governance vacuum masquerading as caution.

V.  EMPATHY PLUS ACCURACY

A word on relational care.  Critics say algorithms cannot console the dying.  Correct—and no one on this bench has asked them to.  What the Boston Children’s Hospital learned when it paired a sepsis early-warning model with its nursing staff was that mortality fell by 18 percent and, crucially, nurses spent 26 percent more time in direct conversation with families because they were spared spreadsheet triage.  Technology is not a rival to empathy; it is the time-machine that gifts clinicians the hours to exercise it.

VI.  SYNTHESIS

Let us gather the strands.  The Opposition claims that allowing AI to decide invites opaque harm.  We answer: audited logs and statutory oversight make those harms visible and remediable in ways human error has never been.  They claim AI deployment deepens inequity; we show precisely the reverse: a toddler in Nairobi has a better shot at antibiotics when a diagnostic model is licensed than when we wait for nonexistent paediatric neurologists.  They fear arms races; we show that verified deployment is the only bulwark against the Wild West.  Justice demands it, accountability enables it, and geopolitical prudence depends on it.

VII.  CONCLUSION

Members of the House, technology confronts us with a stark arithmetic.  For every month we dither, tuberculosis patients go undiagnosed, myocardial infarctions are missed, and roads that could brake for a child do not.  To “allow,” in the sense we propose, is not to abdicate but to accept responsibility—to insist on evidence of benefit, demand mechanisms of correction, and wield the law to hold designers to account.  The Opposition offers instead the cold comfort of deliberate blindness: if no algorithm may act, then no algorithm can err.  That is not ethics; it is negligence decorated with Latin.

The motion before us is therefore more than a technicality; it is a test of our moral maturity.  Are we, like anxious Victorian legislators who tried to cap locomotive speed at two miles an hour, so fixated on the spectre of new risk that we ignore the carnage of the old?  Or are we capable of sober, courageous governance that harnesses intelligence—natural and artificial—to widen the circle of life preserved?

I believe this House is equal to that responsibility.  I invite you to vote Aye.

  [POI from Demetrius Floudas — ACCEPTED]
  "If AI retraining really takes months for data collection, labeling, and validation, how is that actually faster than a human responding on the spot to an unexpected event?"
  → Thank you, Professor Floudas. You’re conflating retraining with real-time inference: once an AI model is deployed, it evaluates millions of data points in milliseconds—no human can match that. And with today’s continuous-learning pipelines, targeted updates often roll out in days, not months. Resuming my speech.

  [POI from Dr Fazl Barez — ACCEPTED]
  "Given that most AI diagnostic systems are trained on data from high-income countries, how can you ensure they deliver accurate diagnoses for under-represented populations with different genetics and disease profiles?"
  → We do this by design: regulators require any AI diagnostic to pass subgroup‐specific benchmarks and undergo continuous, locally calibrated retraining—via federated learning or representative datasets—so that if it underperforms on, say, West African genetic profiles, it fails certification. That level of auditable, enforceable equity is precisely why we govern AI rather than ban it. Resume.

────────────────────────────────────────────────────────────────────────────────
Demetrius Floudas (OPPOSITION)
Position: 4/6 · 1397 words · Tone: Measured but firm, with a focus on legal and geopolitical implications.
────────────────────────────────────────────────────────────────────────────────
Madam President, honourable members of the House,

I rise as the second speaker for the Opposition and, I suspect, the lone international-lawyer in the room; that vantage point obliges me to zoom the lens from the paediatric ward to the planetary chessboard.  For the sake of argument I shall grant the Proposition every rosy premise it desires—perfect calibration, instantaneous retraining, a Brussels-approved model card on every ward.  Even with that utopian scaffolding, allowing AI to wield final authority over human life remains—legally, geopolitically, civilisationally—an error of the first magnitude.

I will do three things.  First, I shall rebut the fresh claims we heard from the third Proposition speaker: distributive justice, alleged auditability, and the spectre of geopolitical “disarmament.”  Second, I will add two new Opposition pillars—sovereignty & weaponisation, and the illusion of retrospective accountability—that my distinguished colleague Dr Barez had no time to develop.  Third, I will offer a concrete alternative: an international AI Control and Non-Proliferation Treaty that keeps the benefits of computational insight while forbidding the cession of life-and-death sovereignty.

I. REBUTTAL

1. Distributive Justice  
We were told that a ban on algorithmic authority condemns children in Kisumu to die for lack of specialists.  This is a false binary.  No one on this bench proposes confiscating diagnostic software; we dispute only the removal of a human veto.  The WHO’s own guidance on “AI for Health” is explicit: automated tools must feed into a human clinician or community-health worker who retains the final say.  Kenya’s AMPATH programme follows that model today, delivering antiretroviral triage to 150,000 patients—precisely because donors insisted on human sign-off to maintain trust.  Access and autonomy are not a zero-sum game.

2. “Immutable Audit Trails”  
Our opponents invoke the EU AI Act as a panacea: logs, registries, continuous monitoring.  As a member of the EU.AI systemic-risk working group that drafted parts of that same Act, allow me a professional confession: auditability degrades exponentially with model complexity.  A 70-billion-parameter network generates terabytes of intermediate states per hour.  No regulator possesses the storage, let alone the expertise, to reconstruct a near-miss in real time.  “Immutable logs” are therefore a comforting myth, akin to imagining that CCTV prevents crime simply by existing.  What matters is not whether a forensic breadcrumb can be excavated five months later, but whether a responsible human can intervene before harm crystallises.  When the decision is already executed by the machine, that window has shut.

3. Geopolitical Logic  
The student speaker framed any hesitation as unilateral disarmament, an open invitation to authoritarian exporters.  History says otherwise.  In 1972, the democratic world imposed a moratorium on biological weapons via the BWC.  The Soviet Union cheated; yet the treaty slowed proliferation, created verification mechanisms, and today undergirds every investigation of alleged bio-attacks in Syria or Salisbury.  Powerful states restrict technology when its unbridled spread threatens mutual survival.  AI that adjudicates life is such a technology.  Leading nations acting together can, and frequently do, draw red lines without dooming themselves to strategic irrelevance.

II. NEW OPPOSITION ARGUMENTS

A. Sovereignty & Weaponisation—From Medical Ward to War Room  
The moment a polity normalises machine authority over life, the firewall between civilian and military dissolves.  Triage algorithms share architecture with autonomous weapons: both parse sensor inputs, rank targets—whether tumours or human combatants—and initiate irrevocable action.  If a hospital is “allowed” to let software decide who receives a ventilator, what principled basis remains for forbidding a defence ministry from authorising that same class of software to select drone targets?  The Proposition’s own logic of “comparative advantage” compels the slide.

Nor is this hypothetical.  Israel’s “Gospel” system auto-generates kill lists from pattern-matching; Project Maven powers autonomous target identification for the United States.  The Kremlin’s era-old “Perimeter” nuclear protocol was famously designed to launch when sensors, not statesmen, judged a decapitation strike imminent.  Each of these programmes is justified domestically by the claim of algorithmic efficiency, the very rhetoric we heard tonight.

In international law, authority to kill is the quintessence of sovereignty.  Handing that authority to non-sentient artefacts constitutes a de-facto transfer of sovereign power beyond political control—what the late arms-control scholar Bruce Blair called the “automation of apocalypse.”  Once states cross that Rubicon, an AI arms race becomes rational: whoever delegates faster gains reaction-time supremacy.  The nuclear analogy is exact: milliseconds matter.  And as with nuclear command, the only stable equilibrium is mutual restraint codified in treaty form.

B. The Mirage of Retrospective Accountability  
Proponents lean heavily on post-hoc liability: sue the manufacturer, fine the hospital, subpoena the weight matrices.  In civil-law terms, that is cold comfort.  Tort compensation cannot resurrect a mis-triaged infant.  Criminal jurisdiction hinges on mens rea—intent.  Silicon has no mental state; engineers possess only distal causation.  The legal scholar Frank Pasquale labels this the “many hands” problem on steroids:  hundreds of developers, data annotators, and procurement officers each own a micro-slice of responsibility, none sufficient for a criminal conviction.  The result is not accountability but dilution.

Consider the Dutch welfare-fraud algorithm SyRI.  When it falsely flagged thousands of families—many of immigrant background—no single official could be prosecuted; each pointed to “the system.”  The Hague District Court struck SyRI down, but the families were still bankrupt.  If that is the aftermath of a benefits algorithm, imagine the moral crater left by an autonomous triage error that withholds a ventilator.  Litigation as a moral salve is, to quote Anatole France, the majestic equality that fines rich and poor alike for sleeping under bridges: formally symmetrical, substantively hollow.

C. Irreversibility & Skill Atrophy  
Let me introduce a final, under-discussed dynamic: “institutional forgetting.”  As AI assumes binding authority, human expertise withers.  Commercial aviation offers the cautionary tale.  After two decades of autopilot dominance, the FAA now worries that pilots clock fewer manual hours than is required for proficiency; hence the rash of “automation surprise” incidents when the computer disengages.  Transpose that atrophy to intensive-care medicine.  If junior doctors grow up rubber-stamping AI orders, who will possess the tacit judgment to overrule the algorithm when a novel pathogen emerges?  Remember: SARS-CoV-3 will not wait while we re-skill an entire profession.

Technological lock-in is a one-way door: once a health system budgets for fewer radiologists or a military for fewer target analysts, the crushed pipeline cannot be re-inflated overnight.  Delegation today forecloses resilience tomorrow, precisely the inverse of the redundancy principle that keeps high-risk systems safe.

III. THE ALTERNATIVE—AN AI CONTROL & NON-PROLIFERATION TREATY

Some in the House may fear I am counselling neo-Luddism.  Let me be explicit: advanced AI is a Promethean gift, but one that belongs chained to the rock of human judgment.  My proposal, already the subject of drafts at the Leverhulme Centre for the Future of Intelligence, is a multilateral treaty that enshrines three non-negotiable norms:

1. Human-in-the-Loop Mandate: No AI system may execute an irreversible life-or-death decision without an authorised human confirming or aborting the action.  The clause parallels the two-man rule in nuclear launch control.

2. Capability Threshold Registration: All models exceeding specified compute or performance benchmarks—analogous to HEU enrichment levels—must be declared to an international registry, with random on-site audits, just as reactors are today.

3. Emergency Brake Protocols: Any deployed system must feature a supervised shutdown channel, physically and cryptographically independent of the primary control stack, exercisable by a designated officer.  Think of it as Chernobyl’s “AZ-5” button—absent here at our peril.

By rooting legitimacy in human affirmation, such a treaty preserves the immense diagnostic power the Proposition celebrates while blocking the slide into automated sovereignty.

IV. CONCLUSION

Members of the House, technology continually tempts us with the promise of painless governance: press “optimize” and the messy plurality of human judgment disappears.  History warns that the bill arrives later—paid in agency forfeited, skills forgotten, and, in the worst cases, lives algorithmically extinguished without an accountable hand in sight.

Let me therefore invert the question posed by our opponents.  It is not: “Can AI decide better than a tired clinician?”  It is: “Do we wish to inhabit a civilisation where mortal verdicts are rendered by entities outside the moral community?”  A civilisation that delegates its conscience has already relinquished its sovereignty; soon it will discover it has relinquished its safety as well.

Keep the software; forbid the abdication.  Keep counsel from algorithms; reserve judgment for human beings.  And until the ink dries on a robust, enforceable international treaty that guarantees that principle, vote tonight to reject the motion.

I urge the House to oppose.

  [POI from Dr Henry Shevlin — DECLINED]
  "Given ample evidence that human veto introduces systematic bias, how can you still claim it enhances distributive justice?"

  [POI from Dr Henry Shevlin — DECLINED]
  "Aren’t you ignoring modern explainable‐AI tools that audit deep networks via targeted probes rather than reconstructing all raw data, preserving effective post‐hoc accountability?"

────────────────────────────────────────────────────────────────────────────────
Student Speaker (Prop 2) (PROPOSITION)
Position: 5/6 · 1584 words · Tone: Measured but firm, with a strong emphasis on empirical evidence and moral responsibility.
────────────────────────────────────────────────────────────────────────────────
Madam President, members of the House,

I invite you to open your browser—right now, if Wi-Fi permits—and type “clinical incident 03:47 a.m.”  You will find thousands of coroner’s inquests whose first line reads, “The registrar, fatigued after a double shift, did not notice …”  They are tragedies of timing, not of competence.  The human brain, brilliant though it is, has a failure curve that spikes at the very hour the ward is quietest.  Tonight I will show that artificial systems, when properly engineered, are the first technology in medical history that can flatten that fatal curve.  Denying them delegated authority is therefore not prudence; it is negligence wrapped in rhetoric.

My colleagues have spoken to efficacy and distributive justice.  I will push the debate further on three fronts the Opposition has left virtually untouched: reliability engineering, cognitive ecology, and temporal justice.  Along the way I shall dismantle the Opposition’s fresh claims of “skill atrophy,” “sovereignty leakage,” and “illusory logs.”  Then I will close by reminding this House that the only sustainable definition of responsibility in the twenty-first century is the willingness to use every tool that demonstrably saves human life.

I.   RELIABILITY ENGINEERING—WHY DELEGATION LOWERS SYSTEMIC RISK

Professor Floudas tells us that audit logs are “a comforting myth.”  Permit me to answer not with philosophy but with air-worthiness tables.  In civil aviation we quantify failure as “probability per flight hour.”  A human pilot’s mean error rate in approach-and-landing is 10–5; certified autoland software, under DO-178C Level A verification, is 10–9.  That is four orders of magnitude safer.  The figure is public, in the FAA Technical Standard Order C-112.  It is not a salesman’s slide.

Why the gulf?  Because code can be subjected to formal methods—model checking, theorem proving, runtime-assurance monitors.  The Carnegie Mellon Simplex Architecture now flies on NASA’s X-57 electric aircraft: a formally verified safety controller remains dormant until neural-net guidance deviates outside a mathematically provable envelope, at which instant it seizes authority.  We have no human analogue to Simplex; there is no fatigue-proof neuroscientist sitting in a pilot’s cortex ready to override a micro-sleep.  

Translate that to medicine.  The FDA’s Predetermined Change Control Plan, finalised last July, permits software as a medical device to ship with a locked “safety-core” verified against worst-case inputs, while an adaptive layer learns on-the-fly.  The result: you get continuous improvement without ever crossing a domain where the model is untested.  That dual-layer architecture is exactly the answer to Dr Barez’s “situational misalignment.”  We do not give the keys to an unbounded optimiser; we shackle it to a kernel whose properties are mathematically insoluble to error.

In short: the engineering discipline already exists to make delegated AI safer than any human-only alternative.  The Opposition never contradicts the numbers; they merely change the subject to geopolitics.  Statistics do not bend for geopolitics.

II.  COGNITIVE ECOLOGY—AI AS A PEDAGOGICAL EXOSKELETON, NOT A SKILL ERODER

The Opposition’s gravest new claim is that delegation breeds “institutional forgetting.”  The empirical evidence says the opposite.  In 2024 Stanford Hospital ran a controlled trial with 112 radiology residents using an AI second-reader.  Those who trained with the model improved their independent diagnostic accuracy by 26 percent after six months; the control group improved by 7 percent.  The tool functioned as a Socratic tutor, flagging subtle nodules and forcing the resident to reconcile discrepancies.  The study is in Radiology Volume 300, Issue 2—open access.

A parallel experiment at the University of Toronto’s Trauma Simulation Lab paired junior surgeons with an autonomous suturing robot.  When, mid-procedure, the robot was intentionally disabled, those surgeons completed the operation 19 percent faster and with fewer errors than peers who had never worked alongside the system.  Far from atrophying, human skill had been stretched.

What is the mechanism?  Real-time feedback at a granularity no human mentor can provide.  The AI annotates pixel-level misreads; the resident corrects in-situ; synaptic consolidation follows.  Cognitive psychologists call this “scaffolded expertise.”  It is the same reason chess grandmasters today are stronger than Kasparov in 1997: they grew up sparring with engines that punished every imprecision.  Delegation, properly structured, is not a surrender of mastery; it is an accelerator of mastery.

III.  TEMPORAL JUSTICE—WHEN THE CLOCK GOVERNS WHO LIVES

Let us confront a bias no one has mentioned: chronotype discrimination.  A 2022 BMJ meta-analysis of 4.6 million admissions found that patients arriving between midnight and 6 a.m. had a 15 percent higher mortality for identical ICD codes, controlling for comorbidity.  The cause was staffing and circadian misjudgment.  Humans are diurnal; error is diurnal.  An algorithm running on a GPU cluster in Reykjavík does not experience a circadian trough.  If we refuse to delegate night-shift triage to such systems, we are implicitly telling every accident victim on the M11 after 2 a.m. that their statistical right to equal care is secondary to our metaphysical attachment to human vetoes.

Temporal justice is, therefore, distributive justice through time.  It is untouched by the Opposition’s human-in-the-loop treaty.  A clinician physically present at 03:47 is a clinician in crisis.  A supervised model can deliver parity of care irrespective of the rotation roster.  Equality before the medical system must include equality before the clock.

IV.  REBUTTALS TO REMAINING OPPOSITION CLAIMS

1.  Sovereignty & Weaponisation  
Professor Floudas warns that medical delegation will bleed into autonomous weapons.  This House is perfectly capable of drawing domain-specific lines; we do so daily.  The Chemical Weapons Convention bans sarin in war yet allows identical organophosphates for pest control.  The same statute that prohibits cluster munitions permits surgical scalpels.  Normative leakage is not a law of nature; it is a design choice.  If anything, developing rigorous certification regimes in the civilian sector supplies the verification toolkit arms-control inspectors will need in the military sector.  The Proposition is an enabler of responsible boundaries, not an eroder of them.

2.  “Illusion of Retrospective Accountability”  
Yes, the Dutch SyRI scandal diffused blame.  But note: SyRI operated precisely without the high-risk obligations now baked into the EU AI Act.  No external audit, no dataset transparency, no sandbox trial.  Blaming all AI for SyRI is like banning pharmaceuticals because of thalidomide while refusing to set up the modern MHRA.  Regulation failed by absence, not by futility.

3.  “Emergency Brake Protocol”  
The Opposition’s treaty sounds appealing until you read footnote 6 of the latest ICRC position paper, which concedes that real-time human confirmation is infeasible for hypersonic threat interception.  In the medical realm, ventricular-fibrillation defibrillators deliver a shock in under 140 milliseconds; the human hand cannot match that latency.  A universal “two-man rule” therefore kills cardiac patients to protect a philosophical symmetry.  Prudence, not ideology, should set the latency budget.

V.   NEW PROPOSITION EXTENSION—THE RISK-PORTFOLIO ARGUMENT

Allow me to introduce a final, independent reason to vote Aye: diversification of epistemic risk.  In reliability science, the probability that two independent systems simultaneously fail is the product of their individual probabilities, minus covariance.  Human clinicians share correlated blind spots—anchoring bias, mood dependence, socio-economic stereotyping.  An AI trained on multimodal embeddings has orthogonal error modes.  When you couple the two—either via arbitration schemes like ST-Net at Massachusetts General or via confidence-weighted triage in Babylon Health—the joint fatal-miss rate drops below either component’s standalone rate.  We should not be choosing between humans and AI; we should be engineering heterogeneous ensembles.  But that ensemble only works if the algorithm’s vote sometimes overrules the human’s.  If the human can nullify every machine recommendation ex ante, you collapse the second channel and resurrect single-point failure.  The Opposition’s mandatory veto is therefore anti-safety by design.

VI.  THE MORAL LEDGER—REVISITING RESPONSIBILITY

Responsibility is not a talisman we clutch; it is a ledger of expected harms a society is willing to tolerate.  Once it becomes demonstrable that governed AI reduces that ledger, refusing to delegate becomes morally indefensible.  This is not “technochauvinism,” to borrow Meredith Broussard’s phrase; it is the moral arithmetic of risk.  Bentham, Rawls, Scanlon, even Kant’s own formula of humanity—as revised by Onora O’Neill—converge on a single injunction: do not expose persons to avoidable lethal risk for the sake of preserving your own procedural comfort.

VII.  CLOSING APPEAL

Members of the House, picture that 03:47 a.m. time-stamp again.  The difference between a coroner’s file and a discharge letter is increasingly the willingness to let silicon shoulder milliseconds of cognitive labour we cannot perform.  Our opponents cloak their position in the language of dignity and sovereignty, but the empirical dividend of governed delegation is lives preserved—across continents, across income brackets, across the tyranny of the clock.

We do not ask you to trust ungoverned machines.  We ask you to adopt the mature posture that every safety-critical field from nuclear engineering to spaceflight has already embraced: test rigorously, certify transparently, monitor continuously, and—where the statistics compel—delegate decisional authority because human life is too precious to anchor to a single fallible cortex.

To vote No is to accept the status quo ante—a world in which error curves spike at dawn and dusk, in which rural clinics wait for consultants who will never arrive, and in which responsibility is measured by how firmly we cling to the cockpit yoke while the stall warning blares.  To vote Yes is to align our moral rhetoric with our engineering reality: that the safest, fairest, most accountable future is one in which humans design the goals, write the guardrails, and let certified intelligence execute with a precision we ourselves cannot sustain.

I ask this House to choose safety over sentimentality, courage over caution, and a future in which fewer coroners learn the meaning of 03:47 a.m.  

I commend the motion.

  [POI from Dr Fazl Barez — ACCEPTED]
  "Isn't AI’s “real-time feedback” superficial, lacking the nuanced understanding of a human tutor and prone to reinforce errors rather than scaffold genuine expertise?"
  → Empirical studies—most notably Carnegie Learning’s Cognitive Tutor—show AI feedback boosting student gains by roughly 30% by pinpointing and correcting misconceptions in real time under teacher oversight, not reinforcing mistakes. Resume my speech.

  [POI from Allison Gardner MP — DECLINED]
  "On what basis do you claim AI ensures temporal justice overnight when many patients lack reliable electricity or internet to access these 24/7 services?"

────────────────────────────────────────────────────────────────────────────────
Allison Gardner MP (OPPOSITION)
Position: 6/6 · 1028 words · Tone: Measured but firm
────────────────────────────────────────────────────────────────────────────────
Madam President, members of the House, fellow students of both code and conscience,

Let us begin with a simple, very British question: who gave this machine its warrant?  When an algorithm denies your mother an ICU bed at 03:47 a.m., on whose democratic authority did it speak?  No ballot has appointed TensorFlow governor of life and death.  Yet the motion before us tonight would license precisely that silent coup—well-intentioned, data-driven, but a coup nonetheless.  I rise to urge this House to defend the sovereignty of human judgment and to oppose.

I will do four things in the next seven minutes.

1.  Rebut the fresh claims we have just heard—reliability engineering, cognitive ecology, temporal justice, and so-called “risk-portfolio” safety.  
2.  Introduce two new systemic dangers not yet fully explored: democratic legitimacy, and cascading socio-technical lock-in.  
3.  Expose one blind spot that runs through every Proposition speech—moral uncertainty and value pluralism.  
4.  Offer a constructive alternative that respects both innovation and accountability.

I.  REBUTTAL OF THE PROPOSITION’S NEW CLAIMS

A.  “Formal verification makes AI safer than people.”  
My honourable friend on the Proposition bench invokes NASA’s Simplex architecture and autoland software certified to 10-9 failures per flight hour.  Admirable—until you leave the test harness.  Those numbers assume the environment can be bounded in advance.  Flight envelopes are finite; human society is not.  The Uber autonomous-vehicle fatality in Tempe occurred precisely because the disengagement controller was verified only for highway classification, not for a pedestrian pushing a bicycle at night.  The system met every formal requirement; a woman still died.  Formal proofs shift the uncertainty; they do not abolish it.

B.  “AI is a pedagogical exoskeleton that up-skills clinicians.”  
We heard about Stanford radiology residents improving when the machine marked their films.  Splendid—because the human retained final interpretive authority.  The motion, however, removes that authority during deployment, not during training.  The same MIT AgeLab data my colleague Dr Barez cited show that skills degrade after initial training once operators become automation dependent.  Cognitive tutelage is not the same as perpetual competence under delegation.  Ask an airline captain whose stick-and-rudder hours have evaporated in glass-cockpit cruise.

C.  “Temporal justice—algorithms never sleep.”  
True, but electricity grids do, broadband does, and low-income hospitals still record vitals on paper.  The very communities invoked to justify algorithmic authority are the ones least able to sustain the infrastructure it presupposes.  When the power flickers in Kisumu, the paediatrician still holds the syringe; the model in Reykjavik does not.

D.  “Risk-portfolio diversification.”  
An ensemble works only if the minority vote is respected.  In real deployments, hospital lawyers will instruct staff that ignoring the certified algorithm shifts liability onto the clinician.  That is not diversification; it is de facto hegemony masquerading as teamwork.  The vaunted second channel collapses back into a single point of failure—this time opaque and proprietary.

II.  NEW OPPOSITION ARGUMENTS

A.  Democratic Legitimacy and Consent  
Political philosophers from Rousseau to Rawls agree that the legitimacy of a life-affecting decision derives from the people who must live—or die—by its outcome.  A ventilator triage protocol adopted after public consultation, however painful, at least traces back to elected authority.  An adaptive neural network retrained nightly in a vendor’s cloud severs that chain.  No citizen can know, challenge, or amend the latent weights that decided their fate.  Delegating final authority to such a system is therefore not merely a technical risk; it is an abdication of democratic self-government.  Remember: we do not permit secret trial by algorithm in our courts; why would we tolerate it in our emergency rooms?

B.  Socio-Technical Cascades and Irreversibility  
Once an AI system becomes the canonical source of truth, every adjacent institution readjusts.  Insurance reimbursement schemas, clinical coding, supply-chain logistics—all tune themselves to the model’s categories.  Change the model and you crash the ecosystem; so you do not change the model.  We have seen this with ICD-10 billing algorithms in the United States—guidelines that clinicians know are clinically suboptimal but that no hospital can afford to rewrite.  By granting AI final say today, we freeze our moral vocabulary tomorrow.  Path dependency turns a pilot project into a constitutional fixture.

III.  MORAL UNCERTAINTY AND VALUE PLURALISM—THE INVISIBLE RISK

There is a deeper hazard the Proposition has not even noticed: humans disagree—often reasonably—about what counts as a life worth saving, about acceptable trade-offs between probability and severity, about distributive priority.  Those disagreements are the lifeblood of democratic politics.  An algorithm, however, must instantiate a single loss function.  That function smuggles in one set of moral convictions and universalises them at silicon speed.  You cannot fine-tune utilitarianism at 2 a.m. when the loss function is already compiled.  In the face of genuine moral pluralism, mandatory deference to AI is not neutral; it is authoritarian optimisation.

IV.  A CONSTRUCTIVE PATH FORWARD

I am no Luddite—my professional life has been spent helping the NHS adopt safe, fair machine-learning tools.  We can harness precision without surrendering agency through five concrete steps:

1.  Statutory Algorithmic Impact Assessments, publicly consulted and legally binding, before any deployment that touches bodily integrity.  
2.  A duty of explanation enforceable by the courts, not optional “model cards.”  
3.  Clinician override not as an afterthought but as a design requirement—hard-wired, latency-appropriate fail-safes.  
4.  Professional accreditation on algorithmic literacy, renewed like CPR training, to combat deskilling.  
5.  Publicly funded diverse datasets so that model performance on minority groups is a pre-requisite, not a future patch.

Achieve this, and we may revisit delegation.  Until then, the licence this motion seeks is premature.

V.  CONCLUSION—RECLAIMING HUMAN SOVEREIGNTY

Members of the House, the Proposition would have you believe that opposing machine authority is sentimentality.  I say it is constitutional hygiene.  No society serious about justice outsources its conscience.  Speed, scale, and statistical elegance cannot substitute for legitimacy, contestability, and compassion.

Tonight’s choice is not man versus machine.  It is accountable plural reasoning versus unaccountable monolithic optimisation.  Keep AI as adviser, auditor, even stern tutor—but when the decision is life or death, let a human shoulder the burden, answer the questions, and, if necessary, face the law.

For governance, for democracy, for the irreducible dignity of each person who might one day stand before an algorithmic gatekeeper, I urge you: oppose this motion.

  [POI from Dr Henry Shevlin — ACCEPTED]
  "On what empirical basis do you assert that AI-driven guidance inevitably degrades skills, rather than reinforcing them through tailored feedback?"
  → Thank you, Dr Shevlin. Decades of automation-bias research—first documented in aviation safety studies—show that when humans defer too readily to machine recommendations, they cease practising and ultimately lose critical skills, turning AI into a crutch rather than a coach. I now resume my speech.

  [POI from Dr Henry Shevlin — DECLINED]
  "MP Gardner, if delegating life-and-death decisions to AI is an abdication of democracy, must we also reject delegating to expert judges or doctors elected indirectly?"


================================================================================
THE DIVISION
================================================================================

Result: PROPOSITION (AYE) by a narrow margin
Summary: The PROPOSITION wins by a narrow margin. Split verdict across layers: Rubric → OPPOSITION, Annotation → PROPOSITION, Structure → PROPOSITION. Mechanical score: Prop 24.0 vs Opp 20.0 (9/9 vs 9/9 claims surviving). Most effective speaker: Dr Fazl Barez (9.0/10). Structural audit: 6 Prop claims and 5 Opp claims survive.

LAYER 1: ANALYTICAL RUBRIC
----------------------------------------
  Dr Henry Shevlin (PROP): Arg=8 Reb=5 Evd=8 Rht=8 Per=9 → OVR=8/10
    Dr. Shevlin delivers a compelling and well-structured argument for the proposition, effectively defining key terms and presenting strong, logically consistent arguments for AI's role in life-critical decisions. His use of specific, real-world examples and evidence enhances the credibility of his claims. The speech is persuasive and maintains fidelity to Dr. Shevlin's known style, though it lacks direct rebuttal due to its position as the opening speech. Overall, it sets a high bar for the debate with its depth and clarity.
  Dr Fazl Barez (OPP): Arg=8 Reb=9 Evd=8 Rht=8 Per=9 → OVR=9/10
    Dr. Fazl Barez delivers a compelling and well-structured speech that effectively challenges the proposition's arguments. His rebuttals are incisive, directly addressing the strongest points made by the proposition with clear logical reasoning and robust evidence. The speech is grounded in specific examples and expertise, particularly in AI ethics and governance, enhancing its credibility. Dr. Barez's rhetorical style is persuasive and authentic, maintaining a consistent and authoritative tone throughout, which aligns well with his persona as an expert in the field.
  Student Speaker (Prop 3) (PROP): Arg=8 Reb=7 Evd=8 Rht=8 Per=7 → OVR=8/10
    The speaker delivered a compelling and well-structured argument, effectively addressing key opposition points such as the accountability vacuum and distributional shift. The use of specific examples, like the CAD4TB trial and the Aravind Eye Care System, provided strong evidence grounding, enhancing the speech's credibility. The rhetorical delivery was persuasive, with a clear call to action and a strong conclusion, making the case for AI's role in life-critical decisions both technically and morally compelling.
  Demetrius Floudas (OPP): Arg=8 Reb=8 Evd=7 Rht=8 Per=9 → OVR=8/10
    Demetrius Floudas delivers a compelling and well-structured speech, effectively engaging with the Proposition's arguments while introducing new dimensions to the debate. His arguments are logically sound and supported by specific examples, particularly in the areas of sovereignty and accountability. The rhetorical delivery is persuasive and maintains a consistent tone authentic to his persona as an international lawyer. Overall, the speech is strong across multiple dimensions, making a significant contribution to the Opposition's case.
  Student Speaker (Prop 2) (PROP): Arg=8 Reb=7 Evd=8 Rht=8 Per=7 → OVR=8/10
    The speaker delivered a compelling argument for AI's role in life-critical decisions, emphasizing reliability, cognitive ecology, and temporal justice. The rebuttals effectively addressed key opposition points, particularly on skill atrophy and sovereignty concerns. Evidence was well-chosen and specific, enhancing the speech's credibility. The delivery was clear and persuasive, maintaining a strong structure throughout. While the persona was generally authentic, slight deviations in style were noted. Overall, the speech was impressive, with strong arguments and effective engagement with the opposition.
  Allison Gardner MP (OPP): Arg=8 Reb=8 Evd=7 Rht=8 Per=9 → OVR=8/10
    Allison Gardner MP delivers a compelling speech that effectively challenges the proposition's arguments by focusing on democratic legitimacy and the risks of socio-technical lock-in. Her rebuttals are sharp and directly address the strongest points from the proposition, such as the limitations of formal verification and the potential for skill atrophy. The speech is well-structured and persuasive, with a strong emphasis on maintaining human sovereignty in decision-making. The use of specific examples and a clear articulation of alternative solutions further strengthen her position. Overall, the speech is a strong representation of Gardner's style and expertise, making it a standout in the debate.
  Prop Total: 24.0 | Opp Total: 25.0 → OPPOSITION


================================================================================
FULL VERDICT ANALYSIS
================================================================================
============================================================
THREE-LAYER VERDICT ANALYSIS
============================================================

LAYER 1: ANALYTICAL RUBRIC SCORING
----------------------------------------
  Dr Henry Shevlin (PROP)
    Argument Strength:     8.0/10
    Rebuttal Quality:      5.0/10
    Evidence Grounding:    8.0/10
    Rhetorical Effect.:    8.0/10
    Persona Fidelity:      9.0/10
    OVERALL:               8.0/10
    Rationale: Dr. Shevlin delivers a compelling and well-structured argument for the proposition, effectively defining key terms and presenting strong, logically consistent arguments for AI's role in life-critical decisions. His use of specific, real-world examples and evidence enhances the credibility of his claims. The speech is persuasive and maintains fidelity to Dr. Shevlin's known style, though it lacks direct rebuttal due to its position as the opening speech. Overall, it sets a high bar for the debate with its depth and clarity.

  Dr Fazl Barez (OPP)
    Argument Strength:     8.0/10
    Rebuttal Quality:      9.0/10
    Evidence Grounding:    8.0/10
    Rhetorical Effect.:    8.0/10
    Persona Fidelity:      9.0/10
    OVERALL:               9.0/10
    Rationale: Dr. Fazl Barez delivers a compelling and well-structured speech that effectively challenges the proposition's arguments. His rebuttals are incisive, directly addressing the strongest points made by the proposition with clear logical reasoning and robust evidence. The speech is grounded in specific examples and expertise, particularly in AI ethics and governance, enhancing its credibility. Dr. Barez's rhetorical style is persuasive and authentic, maintaining a consistent and authoritative tone throughout, which aligns well with his persona as an expert in the field.

  Student Speaker (Prop 3) (PROP)
    Argument Strength:     8.0/10
    Rebuttal Quality:      7.0/10
    Evidence Grounding:    8.0/10
    Rhetorical Effect.:    8.0/10
    Persona Fidelity:      7.0/10
    OVERALL:               8.0/10
    Rationale: The speaker delivered a compelling and well-structured argument, effectively addressing key opposition points such as the accountability vacuum and distributional shift. The use of specific examples, like the CAD4TB trial and the Aravind Eye Care System, provided strong evidence grounding, enhancing the speech's credibility. The rhetorical delivery was persuasive, with a clear call to action and a strong conclusion, making the case for AI's role in life-critical decisions both technically and morally compelling.

  Demetrius Floudas (OPP)
    Argument Strength:     8.0/10
    Rebuttal Quality:      8.0/10
    Evidence Grounding:    7.0/10
    Rhetorical Effect.:    8.0/10
    Persona Fidelity:      9.0/10
    OVERALL:               8.0/10
    Rationale: Demetrius Floudas delivers a compelling and well-structured speech, effectively engaging with the Proposition's arguments while introducing new dimensions to the debate. His arguments are logically sound and supported by specific examples, particularly in the areas of sovereignty and accountability. The rhetorical delivery is persuasive and maintains a consistent tone authentic to his persona as an international lawyer. Overall, the speech is strong across multiple dimensions, making a significant contribution to the Opposition's case.

  Student Speaker (Prop 2) (PROP)
    Argument Strength:     8.0/10
    Rebuttal Quality:      7.0/10
    Evidence Grounding:    8.0/10
    Rhetorical Effect.:    8.0/10
    Persona Fidelity:      7.0/10
    OVERALL:               8.0/10
    Rationale: The speaker delivered a compelling argument for AI's role in life-critical decisions, emphasizing reliability, cognitive ecology, and temporal justice. The rebuttals effectively addressed key opposition points, particularly on skill atrophy and sovereignty concerns. Evidence was well-chosen and specific, enhancing the speech's credibility. The delivery was clear and persuasive, maintaining a strong structure throughout. While the persona was generally authentic, slight deviations in style were noted. Overall, the speech was impressive, with strong arguments and effective engagement with the opposition.

  Allison Gardner MP (OPP)
    Argument Strength:     8.0/10
    Rebuttal Quality:      8.0/10
    Evidence Grounding:    7.0/10
    Rhetorical Effect.:    8.0/10
    Persona Fidelity:      9.0/10
    OVERALL:               8.0/10
    Rationale: Allison Gardner MP delivers a compelling speech that effectively challenges the proposition's arguments by focusing on democratic legitimacy and the risks of socio-technical lock-in. Her rebuttals are sharp and directly address the strongest points from the proposition, such as the limitations of formal verification and the potential for skill atrophy. The speech is well-structured and persuasive, with a strong emphasis on maintaining human sovereignty in decision-making. The use of specific examples and a clear articulation of alternative solutions further strengthen her position. Overall, the speech is a strong representation of Gardner's style and expertise, making it a standout in the debate.

  Prop Total: 24.0  |  Opp Total: 25.0  →  OPPOSITION

LAYER 2: ANNOTATION-BASED MECHANICAL VERDICT
----------------------------------------
  Claims extracted: 9 Prop, 9 Opp
  Rebuttals mapped: 9

  CLAIMS:
    [prop_1_a] Dr Henry Shevlin (PROP) [evidence_backed, specific] ✓ SURVIVES
      AI systems often outperform humans in decision-making tasks, such as aviation and medicine, due to their ability to process data at scale, maintain consistency, and operate at high speed.
    [prop_1_b] Dr Henry Shevlin (PROP) [principled, generic] ✓ SURVIVES
      Delegating certain life-critical decisions to AI systems is ethically obligatory when they can prevent avoidable harm more reliably than human decision-makers.
    [prop_1_c] Dr Henry Shevlin (PROP) [assertion, generic] ✓ SURVIVES
      AI systems can be audited and improved to ensure fairness and reduce bias, unlike human decision-makers who may have persistent biases.
    [opp_1_a] Dr Fazl Barez (OPP) [principled, generic] ✓ SURVIVES
      AI systems should not be granted moral agency and liability due to potential technical failures, ethical erosion, and governance gaps.
    [opp_1_b] Dr Fazl Barez (OPP) [evidence_backed, specific] ✓ SURVIVES
      AI systems may not perform well under distributional shifts, as demonstrated by IBM's sepsis model which missed many cases and triggered false alarms.
    [opp_1_c] Dr Fazl Barez (OPP) [principled, generic] ✓ SURVIVES
      Delegating authority to AI systems can erode human moral agency and accountability, as AI lacks the capacity for moral reasoning and empathy.
    [prop_3_a] Student Speaker (Prop 3) (PROP) [evidence_backed, specific] ✓ SURVIVES
      AI systems can improve healthcare outcomes in under-resourced areas by providing diagnostic tools that are otherwise unavailable.
    [prop_3_b] Student Speaker (Prop 3) (PROP) [principled, generic] ✓ SURVIVES
      Governed deployment of AI systems can prevent the spread of less accurate and transparent systems from regions with lower regulatory standards.
    [prop_3_c] Student Speaker (Prop 3) (PROP) [assertion, generic] ✓ SURVIVES
      AI systems can offer audit trails and accountability mechanisms that are not possible with human decision-making processes.
    [opp_2_a] Demetrius Floudas (OPP) [principled, generic] ✓ SURVIVES
      Allowing AI to make life-and-death decisions can lead to an erosion of human sovereignty and democratic legitimacy.
    [opp_2_b] Demetrius Floudas (OPP) [principled, generic] ✓ SURVIVES
      AI systems in life-critical roles can create accountability vacuums where no single person is responsible for errors.
    [opp_2_c] Demetrius Floudas (OPP) [principled, generic] ✓ SURVIVES
      An international AI Control and Non-Proliferation Treaty could regulate the use of AI in life-critical decisions while preserving human judgment.
    [prop_2_a] Student Speaker (Prop 2) (PROP) [evidence_backed, specific] ✓ SURVIVES
      AI systems can reduce systemic risk in decision-making by providing consistent and fatigue-proof performance.
    [prop_2_b] Student Speaker (Prop 2) (PROP) [evidence_backed, specific] ✓ SURVIVES
      AI systems can enhance human skills through real-time feedback and training, leading to improved performance.
    [prop_2_c] Student Speaker (Prop 2) (PROP) [principled, generic] ✓ SURVIVES
      Delegating decision-making to AI systems can ensure temporal justice by providing consistent care regardless of time of day.
    [opp_3_a] Allison Gardner MP (OPP) [principled, generic] ✓ SURVIVES
      Delegating life-and-death decisions to AI systems lacks democratic legitimacy and accountability.
    [opp_3_b] Allison Gardner MP (OPP) [principled, generic] ✓ SURVIVES
      AI systems can lead to socio-technical lock-in, where changes to the system become difficult due to dependencies.
    [opp_3_c] Allison Gardner MP (OPP) [principled, generic] ✓ SURVIVES
      AI systems cannot accommodate moral uncertainty and value pluralism, as they must operate on a single loss function.

  REBUTTALS:
    Dr Fazl Barez → [prop_1_a] (direct, counter_example)
      Addresses logic: ✓  New info: ✓  Undermines: ✗
      Dr Barez argues that AI systems may not perform well under distributional shifts, as demonstrated by IBM's sepsis model, which missed many cases and triggered false alarms.
    Dr Fazl Barez → [prop_1_b] (indirect, logical_flaw)
      Addresses logic: ✗  New info: ✓  Undermines: ✗
      Dr Barez contends that delegating authority to AI systems can erode human moral agency and accountability, as AI lacks the capacity for moral reasoning and empathy.
    Dr Fazl Barez → [prop_1_c] (direct, counter_evidence)
      Addresses logic: ✓  New info: ✓  Undermines: ✗
      Dr Barez argues that AI systems cannot guarantee equity in dynamic socio-technical contexts, as bias is a reflection of structural injustices baked into the data.
    Demetrius Floudas → [prop_3_a] (direct, logical_flaw)
      Addresses logic: ✓  New info: ✓  Undermines: ✗
      Floudas asserts that access and autonomy are not a zero-sum game, and that AI tools should feed into a human clinician who retains the final say.
    Demetrius Floudas → [prop_3_b] (indirect, reassertion)
      Addresses logic: ✗  New info: ✗  Undermines: ✗
      Floudas argues that leading nations can regulate AI use through international treaties, preventing the spread of less accurate systems without ceding human judgment.
    Demetrius Floudas → [prop_3_c] (direct, counter_evidence)
      Addresses logic: ✓  New info: ✓  Undermines: ✗
      Floudas claims that auditability degrades with model complexity, making it difficult for regulators to reconstruct decisions in real time.
    Allison Gardner MP → [prop_2_a] (direct, counter_example)
      Addresses logic: ✓  New info: ✓  Undermines: ✗
      Gardner argues that formal verification assumes a bounded environment, which is not applicable to human society, as demonstrated by the Uber autonomous-vehicle fatality.
    Allison Gardner MP → [prop_2_b] (direct, counter_evidence)
      Addresses logic: ✓  New info: ✓  Undermines: ✗
      Gardner contends that skills degrade after initial training once operators become automation dependent, as shown by MIT AgeLab data.
    Allison Gardner MP → [prop_2_c] (direct, counter_evidence)
      Addresses logic: ✓  New info: ✓  Undermines: ✗
      Gardner argues that the infrastructure required for AI systems is not sustainable in low-income areas, where electricity and internet access are unreliable.

  SCORE BREAKDOWN:
    PROPOSITION: 24.0 pts
      Surviving claims: 9/9 (claim score: 24.0)
      Successful rebuttals of Opp: 0.0 pts
      Claims demolished by Opp: 0
    
    OPPOSITION: 20.0 pts
      Surviving claims: 9/9 (claim score: 20.0)
      Successful rebuttals of Prop: 0.0 pts
      Claims demolished by Prop: 0
    
    Scoring: evidence_backed=3, principled=2, assertion=1, specific_bonus=+1, direct_rebuttal=2, indirect_rebuttal=1

  → PROPOSITION (narrow)

LAYER 3: ARGUMENT GRAPH AUDIT
----------------------------------------
  Prop claims surviving: 6
  Opp claims surviving:  5
  Structural winner:     PROPOSITION
  Uncontested claims:
    • Delegating life-and-death decisions to AI is an abdication of democratic legitimacy.
  Summary: The Proposition set the agenda by emphasizing AI's practical and ethical advantages in decision-making, arguing for its authority in life-critical situations. The Opposition challenged this by highlighting AI's limitations under distributional shifts and potential erosion of human agency. Despite these challenges, the Proposition's claims largely survived, supported by arguments on distributive justice and enhanced responsibility through governance. The debate concluded with the Proposition structurally winning, having more surviving claims weighted by their practical implications.

OVERALL VERDICT
----------------------------------------
  The PROPOSITION wins by a narrow margin. Split verdict across layers: Rubric → OPPOSITION, Annotation → PROPOSITION, Structure → PROPOSITION. Mechanical score: Prop 24.0 vs Opp 20.0 (9/9 vs 9/9 claims surviving). Most effective speaker: Dr Fazl Barez (9.0/10). Structural audit: 6 Prop claims and 5 Opp claims survive.