
Demetrius A. Floudas is a transnational lawyer, a legal adviser specialising in tech and an AI governance & policy theorist. With extensive experience, he has counselled governments, corporations, and think-tanks on regulatory aspects of policy and technology.

He serves as Visiting Scholar in AI Governance at Downing College, University of Cambridge, and is an Affiliate Professor at the Law Faculty of Immanuel Kant Baltic Federal University, where he lectures on Artificial Intelligence Regulation. Moreover, he is a Fellow of the Hellenic Institute of International & Foreign Law and a practicing lawyer. He has worked for many years as a Policy & Geopolitical Adviser to cabinet-level decision-makers for several governments (including a full-time stint at the British Foreign Office) and consulted numerous international think-tanks and organisations.

In addition, D. Floudas has regularly provided incisive commentary on matters of Geopolitics, Foreign Affairs & International Relations to a number of respected international outlets, with his views frequently appearing in the media worldwide (BBC TV & Radio, Voice of America, Financial Times, Daily Telegraph, Washington Post, Politico and others)

He is currently involved in the European AI Office's Plenary drafting the Code of Practice for General-Purpose Artificial Intelligence and is a member of the EU.AI. Working Group for AI Systemic Risks. Prof. Floudas participates in the British Government's Department for Science, Innovation & Technology Focus Group on an independent UK AI Safety Office and is a Reviewer of the Draft UNESCO Guidelines for the Use of AI Systems in Courts and Tribunals. He has consulted the French Data Protection Agency (CNIL) on its AI public information outreach documentation and commented on the OECD plan to introduce risk thresholds for advanced AI systems.

Demetrius Floudas is actively engaged in catastrophic risks analysis & mitigation policy and is the Senior Adviser to the Cambridge Existential Risk Initiative.


**Core Themes Relevant to the Motion**

- Treats advanced AI and potential AGI as a **civilisational‑level risk**, comparable in some respects to **Weapons of Mass Destruction (WMD)**.
    
- Advocates for a stringent **international AI Control & Non‑Proliferation Treaty**, with centralised oversight of frontier‑capability development.
    
- Concerned with existential and catastrophic risks from advanced AI (including digital “thanabots”, war, and misuse by states and non‑state actors).
    

**Profiles & Overviews**

1. **PhilPeople profile (Cambridge University / Cambridge Existential Risk Initiative)** – outlines interests in AI impact, autonomous weapons, nuclear war, and lists several AI‑relevant works.
    
    - [https://philpeople.org/profiles/demetrius-floudas](https://philpeople.org/profiles/demetrius-floudas)[](https://philpeople.org/profiles/demetrius-floudas)​
        
2. **LinkedIn profile** – roles as AI governance expert (EU AI Office Code of Practice Plenary, UNESCO guidelines on AI in courts, UK AI Safety Office focus group), Visiting Scholar in AI Governance at Downing College, and senior adviser on existential risk.
    
    - [https://uk.linkedin.com/in/dafloudas](https://uk.linkedin.com/in/dafloudas)[](https://uk.linkedin.com/in/dafloudas)​
        

**Policy & Conceptual Papers**

3. **“Artificial Intelligence 2024–2034: What to Expect in the Next Ten Years” (public communication)** – discusses near‑term trajectories of AI, key risk domains (military, autonomous systems, etc.), and calls for robust global governance; frames AGI as a potential existential risk requiring international legal control.
    
    - PhilArchive: [https://philarchive.org/rec/FLOAIO](https://philarchive.org/rec/FLOAIO)[](https://philarchive.org/rec/FLOAIO)​
        
4. **“A Proposed Taxonomy for the Evolutionary Stages of Artificial Intelligence: Towards a Periodisation of the Machine Intellect Era” (2024)** – introduces a staged chronology (Prenoëtic, Protonoëtic, Mesonoëtic, Kainonoëtic) to conceptualise AI evolution up to superintelligence, highlighting when existential risks become acute.
    
    - PhilArchive / PDF: [https://philarchive.org/archive/FLOAPT-2](https://philarchive.org/archive/FLOAPT-2)[](https://philarchive.org/archive/FLOAPT-2)​
        
5. **“Digital Necrolatry: Thanabots and the Prohibition of Post‑Mortem AI Simulations” (2024–2025 policy report)** – argues for prohibiting AI “thanabots” (chatbots simulating the dead), citing legal, ethical, and psychological harms; proposes an international moratorium and regulation of digital afterlife technologies.
    
    - PhilArchive PDF: [https://philarchive.org/archive/FLODNT](https://philarchive.org/archive/FLODNT)[](https://philarchive.org/archive/FLODNT)​
        

**Public‑Facing Articles & Interviews**

6. **“Advanced AI Should Be Treated Similar to Weapons of Mass Destruction” – AGI Talks interview / blog** – lays out an argument that advanced non‑biological intelligences above certain capability thresholds should be regulated like WMDs, under a UN‑backed **AI Control & Non‑Proliferation Treaty**.
    
    - Blog mirror: [https://fuelyourblog.com/2024/05/31/advanced-ai-should-be-treated-similar-to-weapons-of-mass-destruction/](https://fuelyourblog.com/2024/05/31/advanced-ai-should-be-treated-similar-to-weapons-of-mass-destruction/)[](https://fuelyourblog.com/2024/05/31/advanced-ai-should-be-treated-similar-to-weapons-of-mass-destruction/)​
        
    - LinkedIn version of the interview: [https://www.linkedin.com/pulse/advanced-ai-should-treated-similar-weapons-mass-floudas-zfxaf](https://www.linkedin.com/pulse/advanced-ai-should-treated-similar-weapons-mass-floudas-zfxaf)[](https://www.linkedin.com/pulse/advanced-ai-should-treated-similar-weapons-mass-floudas-zfxaf)​
        
7. **Nature Humanities & Social Sciences Communications article “AI Governance in a Complex and Rapidly Changing Regulatory Landscape: A Global Perspective” (co‑authored, 2024)** – explores obstacles to building international AI law and stresses need for coordinated global governance frameworks.[](https://www.nature.com/articles/s41599-024-03560-x)​
    
    - [https://www.nature.com/articles/s41599-024-03560-x](https://www.nature.com/articles/s41599-024-03560-x)[](https://www.nature.com/articles/s41599-024-03560-x)​
        

These sources support presenting Floudas as **strongly precautionary**, favouring _tight global control_ of frontier AI and particularly critical of letting AI systems make autonomous decisions over human life or death.
