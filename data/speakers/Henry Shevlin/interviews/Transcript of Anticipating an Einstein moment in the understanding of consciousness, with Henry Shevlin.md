[Music] Welcome back to the London Futurist podcast. Today we are doing something a bit different. Regular listeners will know that Callum Chase and I are involved in a couple of ways with a startup called Concum which among other things is researching the possibility and implications of machine consciousness. Concum is seeding a nonprofit with that same goal. The nonprofit is called Prism, which stands for the Partnership for Research into Sentient Machines. This episode is being recorded in collaboration with Prism, and today we have its CEO, Will MÃ¼llership, on as a co-host. Will and Callum will be releasing a new podcast focusing entirely on conscious AI, and the first few episodes will be in collaboration with the London Futurist podcast. You can find a link to that new channel in the description. So, in this episode, I will be co-hosting with Will. Hi, Will. Over to you. Hi, David and thanks very much for having me and thanks for hosting the first Prism podcast. Today, we've got Henry Chevlin. Henry is the associate director of the Lever Hume Center for the Future of Intelligence at the University of Cambridge. there. He also co-directs the kinds of intelligence program and oversees educational initiatives. He researches the potential for machines to possess consciousness, the ethical ramifications of such developments and the broader implications for our understanding of intelligence. In his 2024 paper, consciousness, machines and moral status, Henry examines the recent rapid advancement in machine learning and the questions they raise about machine consciousness and moral status. He suggests that public attitudes towards artificial consciousness may change swiftly as human AI interactions become increasingly complex and intimate. He also warns that our tendency to anthropomorphize may lead to misplaced trust in and emotional attachment to AIs. Henry, welcome to this episode of the London Futurist podcast in collaboration with Prism. Delighted to be here and thrilled to be discussing this topic with you both. Great to meet you, Henry. So Henry, in the past there were times that it's been considered almost career suicide for scientists to study consciousness. What was it that made you get interested in the field in the first place? I actually started out really in the field of animal consciousness. I'd done some work on science of human consciousness before that mostly in the cognitive science empirical side. Then I'd beame increasingly interested in what seemed to me like a neglected topic, namely better tools and theories to assess whether for example fish can feel pain or whether honeybees have conscious perceptive experiences. And then when I joined the Lee Center for Future Intelligence back in 2017, many of those same questions about non-human consciousness in the biological case started to loom larger and larger in the case of AI. I do think a lot of the really fundamental challenges of animal consciousness, how we can ever reliably assess the presence or absence of consciousness in a system that may have a cognitive architecture that's quite different from ours that we we can't directly relate to. Perhaps many of those same questions arise from machine consciousness. Of course, there are some differences as well. In particular, the fact that we can talk to large language models, for example, in a way that we can't directly communicate verbally with animals. So there are some nuances of differences, but a lot of the issues I think are the same. That's led me to get increasingly interested in this topic. I think it's definitely become a lot more of a mainstream issue over the last few years. If you're writing a history of the evolution of the consciousness debate over this period, you'd want to have a whole chapter dedicated to Blake Leo, the Google engineer who back in summer 2022 decided that the lambda model he was interacting with was conscious. This is an ancestor of the modern Gemini model. Le Moine was subsequently fired from Google partly as a result of this. I think that was a really striking moment, a landmark. Not because I think he was right. I don't think Lambda is a particularly strong consciousness candidate. I don't think contemporary large language models are necessarily super convincing either. But I think it was a landmark moment in so far as suddenly a lot of philosophers, a lot of consciousness researchers had to answer this question of well if we don't think this model is conscious, what makes us confident in saying that? How do we know that it's not conscious? And that is a very messy issue. Blake Le Moine's not unique in taking that stance. I'll just mention that one of my former software engineering colleagues from my days in the smartphone industry, a very smart guy who's quite a senior software engineer, he occasionally pings me with letters that he claims have been co-written by two consciousnesses, his own and a consciousness which is inside a large language model. So I'm not quite sure how to respond to him, but he and many others have become convinced it seems that there is a real question of the consciousness of these systems. So there is growing public interest in how we can meaningfully describe these experiences. Yeah, I think you're absolutely right that more and more people are leaning towards attributing consciousness to these systems. Still a minority I think but there are lots of individual users of things like chat GPT or Claude who I think find themselves engaging in a kind of uncertainty feel a kind of uncertainty about whether the system they're interacting with really has thoughts or feelings. A concept that I find quite useful that I've introduced here in thinking about anthropomorphism is a difference between ironic as I call it ironic anthropomorphism and unironic anthropomorphism. What I mean by ironic anthropomorphism is that we attribute mental states to things all the time without sincerely endorsing them. For example, I've been playing a lot of Boulder skate recently and I attribute mental states to the characters in the game. This character's annoyed with me or this character has a hidden agenda. The same when we're reading books or movies. We engage in mentalizing the characters in these shows and movies. We realize, of course, that they don't really have mental states. I don't think Shadow Heart in Boulders Gate 3 really has a conscious mind, but it's a kind of makeelie. That's why I call it ironic anthropomorphism. But you can also see it as a spectrum. As you start to get more and more engaged and have increasingly sophisticated interactions with AI systems, that sort of uncertainty about whether there could be a mind, a conscious mind on the other side grows. Blake Le Moine clearly this was unironic anthropomorphism. He really thought that lambda was conscious. But I think a lot of people right now are in this gray zone where they're kind of tempted to at least characterize these systems as having attitudes, beliefs, maybe thoughts, but are really uncertain about how strong consciousness candidates they actually are. So coming back to how conscious they actually are before we can get to actually measuring consciousness, everyone seems to have different definitions of consciousness. What's your definition of consciousness? And how do you see consciousness differing from sentience, for example? So there's the easy positive answer and then there's the messy more pessimistic answer. The easy positive answer is that for most of philosophy of mind we have had a pretty good shorthand definition of consciousness namely what it's like. This is a phrase that was popularized by Thomas Nagel in his landmark article from the 1970s. What is it like to be a bat? This is sometimes called phenomenal consciousness to distinguish it from other concepts around. But basically, if there's something it's like to be a system, whether that's a human, an animal or an AI, if there is some inner world, some inner experiences, if it has qualia, then that system is conscious. And this idea of what it's likeness is widely used. It's a horribly ungainainely phrase, what it's likeness. So that is a phrase you see a lot in philosophy of mind. I think a lot of us kind of have an intuitive understanding of what that means. But at the same time, once you try and probe that concept and try and spell it out more rigorously, that's when the arguments happen. Floor Ned Block has famously quoted Louis Armstrong on jazz. When Louis Armstrong was asked, what's jazz? He said, well, if you don't already know, you're never going to know. And Ned Blocker sort of semi- jokingly used the same description to refer to phenomenal consciousness. So that's the kind of positive answer that we can use this concept of what it's like to get a handle on the idea. But I think in practice the field of consciousness science is very conflicted and things get very messy once you try and move beyond that. So I've been involved in the science of consciousness world going on almost goodness me 18 years now since my early grad studies and I think back in circa 2010 2015 there was a lot of optimism that we were really making clear progress on understanding what consciousness was. I speak here for example about the big consciousness science events things like the association for the scientific study of consciousness. I was the chair of their student committee for a while and it was a really exciting time back in the early 2010s because it seems like you had convergence on theories of consciousness from many different frameworks. So there are lots of different scientific theories of consciousness. Things like global workspace theory is one, integrated information theory another, higher order thought theory. Around this time you had a whole bunch of exciting experimental measures that seemed to offer maybe converging evidence. It looked like maybe these theories could be reconciled into a sort of a grand unified theory of conscious experience. But things didn't pan out that way. The field has certainly had a tough decade. A lot of the kind of more radical theories of consciousness which I think had slightly fallen out of the conversation. Views like pansychism, the idea that literally everything is conscious or illusionism, the idea that consciousness doesn't exist, or biocychism, the idea that all and only living things are conscious. These quite radical views are very much back in the center of the conversation. It's not like we've got any experimental evidence to support pansyism. It's not like we figured out that atoms are actually conscious. But I think it's just a reflection of the kind of churn in these debates that theories come into fashion out of fashion according to academic trends. I think we've seen that in consciousness science over the last decade. So this sort of perhaps naively optimistic model that the questions about consciousness were becoming properly dry scientific questions. That picture was maybe a bit optimistic. We're seeing ourselves getting mired in the old metaphysical controversies once again. So that's what the field of consciousness is debating. But I don't think you've given us a clear answer yet as to what your own favorite definition or most useful definition of consciousness is. Well, there's a reason for that which is but I think it's a flawed concept. I think it's a concept that we will have to evolve. We're still waiting for the sort of Einstein moment, the capernacus moment. I am not optimistic that our current theory, our current concept of consciousness will ever be able to be scientifically resolved. I think we do need a sort of paradigm shift in how we think about consciousness and I think the concept itself will need to evolve. That's my pessimistic take based on the last couple of decades or more of metaphysical controversies. But maybe one way to partially answer your question is to talk about why the concept matters. And that's because for a lot of people, not everyone, but a lot of people in these debates, consciousness is intimately connected with moral status or moral patency as it's sometimes called. I think there's a very clear intuitive intuition here. Philosopher Peter Singer puts it very well. A school boy kicking a stone along the road isn't doing anything morally problematic because the stone can't suffer. And because it can't suffer, it doesn't have any moral interests. However, if he's kicking a dog or a cat or another human because these beings can suffer, they have experiences. They're conscious. That gives them moral status. A lot of the debates that we see in animal welfare, can fish feel pain? Is shrimp welfare a valuable project to be engaged in? I think for a lot of people, they boil down to the question, can this particular animal species suffer? and suffering requires consciousness. So I think that's why for a lot of people consciousness really seems to matter. So in terms of consciousness mattering presumably you think that there is at least a fair chance that machines could become conscious or you wouldn't be in the field. Do you have a percentage number for your level of confidence in it happening and when you think it might come about? Yeah, a very good question. I think there's a non-trivial chance that some existing architectures have some form of consciousness. I think part of the challenge is that if there is consciousness in existing large models, it's going to be likely to be a very very different kind of consciousness from mere human embodied heavily perceptual experience. One concept in philosophy of mind that sometimes comes up that I think might be applicable to certain large language models is something called cognitive phenomenology. Cognitive phenomenology is the idea that there might be something it's like to have certain thoughts. Everyone knows there's something it's like to taste coffee or to hear the opening bars of a Beethoven symphony or to feel pain for example. These are classic examples of what are called qualia and some people said could there be qualia for thought. So if we're doing mathematics in our head and different answers are coming into mental focus, is there a kind of distinctive qualia or distinctive phenomenology associated with that? Another example that's sometimes given of cognitive phenomenology is the experience of understanding. So this is an example that goes back to the work of Galen Strawson, British philosopher, who says, "Imagine two people, Jack and Jack, an Englishman and a Frenchman, listening to a French TV broadcast. On the one hand, they're hearing exactly the same sound. But what it's like for Jack, who doesn't understand a word that's being said, is arguably quite different from what it's like for Jack, who's not only hearing the sounds, but is associating semantic meaning and understanding with those sounds. There are all sorts of little twists and turns this debate can take, but at least on one reading of that datim, there's something it's like to understand. Now, I don't think large language models have perceptual phenomenology. I don't think they feel pain. I don't think they have any kind of bodily qualia, but they might conceivably have some kind of analog or something close to cognitive phenomenology, some kind of phenomenology associated with things like inference or basic forms of cognition. So I would say maybe a 20% chance that there is something present in existing models that could be called a form of consciousness. Whether that form of consciousness would deserve moral status, not immediately obvious. One distinction that was mentioned earlier on that I didn't actually get to is this difference between consciousness and sentience. My read on these terms, which I think is fairly standard, is that sentience is a subtype of consciousness. Specifically, consciousness that's associated with positively and negatively Veilanced states. Veilanced here bit of a technical term but things that basically feel good or feel bad. So that could be pain most obviously but there are lots of things that can feel bad. Nausea, inability to breathe, negative emotions, anxiety, stress, boredom. All of these things have a negative veilance. And of course you've got positive veilances. The pleasure of eating a good meal, the pleasure of having a nice warm bath, the pleasure of a nice Sunday morning lion. So these are all examples of veance experience and sometimes the term sentience is used to refer just to the subset of beings that are not just conscious but can also have conscious veanced experiences. Now it seems much less likely to me that any existing large language models have veanced experience because at least in the human case that seems very much bound up with bodily and effective states. I'm not saying that that's impossible but I'm more skeptical of that in existing large language models than some kind of bare bones form of cognitive phenomenology for example. I think it's overwhelmingly likely certainly in the next 1015 years honestly probably better than even odds by the end of this decade that we'll have machines that have very good candidates for richer forms of consciousness and perhaps even sentience. One reason for this is I'm just a consciousness liberal in the sense that when it comes to the animal world for example I think there's pretty good reason to think that for example insects are pretty good consciousness candidates. Andy Baron and Colin Klein have got a great paper on this looking at the plausibility of insect consciousness. And I think yeah, there's a pretty strong case to be made that even honeybees with their tiny brains might well have some analog of conscious experience. So if you think relatively simple creatures with very small brains can have conscious experience, if you find that case persuasive, as I do, then the idea that systems with hundreds of billions of parameters might also be conscious is not that farfetched. We'll be right back after a quick break. If you spend way too much time online, first of all, same. And second, you need promo guy talk pills. I'm Roal and with Alex, we break down everything happening in tech, digital marketing, and the internet's latest chaos. No boring corporate talk, just real discussions with real opinions. So go on, press play. You know you want to. [Music] So how do we get from something like cognitive phenomenology to something having moral patency? Because you suggested that they needn't necessarily follow. So we might even accept that there's some kind of recognition, some self-awareness, some cognitive phenomenology happening inside our large language models, but we needn't necessarily feel bad about switching them off. What else is necessary before we avoid ever switching them off? Well, it's a very interesting question. So, there's a great thought experiment by philosopher Dave Charas in his recent book reality plus where he imagines a type of being called the Vulcans, obviously named after the Vulcans from Star Trek, who are basically as psychologically similar to us as possible given the following constraint. Nothing ever feels good or bad to them. They have beliefs about the world. Maybe they act in the world. Maybe you even are happy to ascribe them some kind of desires or goals, but nothing ever feels positive or negative to them. They don't feel any negative emotions when their goals are thwarted, when they don't get what they want. They don't feel any negative associations around pain or suffering more broadly. And Char has asked this question, do the Vulcans deserve moral consideration? And it's a very very divisive topic. I meet lots of philosophers who say look if these Vulcans are truly equanimous if they are completely unmoved by anything that can happen to them in the world if nothing really makes them feel good or bad then they don't care about their own interests they don't have any interest as far as they're concerned so they don't matter morally at all whereas others would say are you kidding that's a monstrous view these are smart intelligent creatures very similar to us in lots of other ways they can write poetry, they can do philosophy. That alone, that kind of cognitive sophistication gives them a kind of moral status. So there is actually a surprisingly deep and controversial debate about whether sentience is even required for moral status or whether consciousness alone without the veilance might actually suffice. I also think maybe there are some candidate forms of veilent states that don't require rich perceptual or bodily phenomenology. There's one candidate state in particular that I've spent some time thinking about which is boredom. I frequently get bored. I get very stressed out sometimes when I don't have enough informationational stimulation. And I think the interesting thing about boredom is that it's not tied necessarily to any particular perceptual modality or sensory modality like pain for example or nausea. It doesn't require any necessarily complex or biologically grounded emotional capabilities. It's not like fear which is very much grounded in different endocrine systems and evolutionarily evolved adaptations to threats. Instead, it seems like a very high level property of informationational systems. If you have a certain expectation of regular informationational stimulation and you represent at some very broad level something's going wrong. If you're not getting that informationational stimulation, then I could see that being a quite plausible form of negatively veilent state that we could see even in systems that don't have rich perceptual or embodied phenomenology that might just have something like cognitive phenomenology. Again, I'm not convinced that any existing large language models get bored, but that's the kind of state that I would be looking for as the first sort of evidence of a veilance cognitive state that might be realized in an AI system. So, let's ask you the $64 billion question. Should we be trying to build AIs that have got consciousness? Is that something that excites you or is that something that worries you? Well, yeah, it's very hard. This is an intersection point for a lot of different debates. Now there are some people who would say that we shouldn't be trying to make any consciousness. David Benatar is possibly the most famous advocate of this position known as antiatalism. He basically suggests that because to be conscious involves the risk of suffering or certainty of suffering at some level that provides a good reason not to create new conscious experiences beings that can have conscious experiences. and Thomas Metsinger, very highly acclaimed German philosopher who has similar antiatalist or negative utilitarian sympathies has suggested that this is a good reason not to create conscious AI systems. I think in my own case I skew and this is whole messy area called population ethics. I think that generally speaking adding more conscious beings to the world is good if those beings live net positive lives. I am cautious though in the case of AI systems because our understanding of what AI consciousness might be like is so impoverished even to call it rudimentary I think is giving it too much credit. We really have no idea what it might be like to be these systems or what kind of states might be experienced as either very positive or very negative. You have some radical thinkers like Brian Tamasic for example who's argued that even relatively simple reinforcement learning systems to the extent that there are negative signals in these systems that could lead to experienced negatively valance conscious states and we might not even realize that we've built these systems that have these negatively valent states. There are a few interesting workarounds we could look at. One slightly wacky idea but I don't think it's totally wacky that I've discussed. I'm not the only person who's come up with this idea, but it's one that interests me is the idea of hydonic offsetting. So, we're sort of familiar with the idea of offsetting your carbon footprint, the idea that I'm going on a flight to America, so I'm going to pay for trees to be planted sort of carbon credits. And roughly the idea here is if you're building an AI system and there is a chance that it's conscious and there is a chance some uncertainty about whether some of its states might be experienced as negatively veilanced. One way to deal with that moral hazard might be to add a whole bunch of positive states or states that if they turned out to be conscious would be experienced as positively veilanced. A kind of compensatory pleasure for any pain that we accidentally create. As I say, it's a bit of a wacky idea. But the nice thing about it is it's relatively theory agnostic. You don't need to buy into one specific theory of consciousness or even have particular confidence that the model you're creating is conscious. You just need some framework for saying well look if this type of state is perceived negatively or leads to negative conscious experience by flipping the sign so to speak or adding some mirror image type state you might be able to add pleasure to the system. Does that totally remove the moral hazard of creating potentially conscious suffering beings? Well, for the Pat Benars of this world who see this deep asymmetry between suffering negative states and positive states where negative states always create distinctive kinds of obligations, they're not going to be very impressed by that. But I think a lot of us would say, look, sure, we make these kind of trade-offs all the time. We have that extra glass of wine at dinner knowing that it'll give us a headache in the morning because we regard the pleasure of that extra little thrill of inebriation as compensating for the mild headache the next morning. So, in our own case, we do make these trade-offs. We even make them for our kids. I've got two young kids, and we make our kids do boring homework because we think it'll be better for them in the long run. We make them drink some unpleasant tasting medicine because it'll relieve their suffering in other ways. So, it's not as wacky an idea as may seem on the surface. We'll be right back after a quick break. Marketing is a puzzle. You can either give into the confusion or learn to love the solve. On the Axium's real talk about marketing podcast, guests like Aaron Schaefer of GM share how to manage challenges. Make sure that your nods fixed in terms of what you're thinking about. The team's going to change and you know the way teams operate are going to change. Co was a great example of that. The industry is going to change. You know, are cookies going away or not? That was another great example of that. And so, how do you continue to evolve as the space evolves? Guests from HP, the trade desk and data bricks share thoughts on today's marketing landscape. Find real talk about marketing on YouTube, axium.com, or wherever you hear podcasts. Let's come back to measurements. To get to a state where we add positive states, negative states, we still need to be able to measure it. Do you think we can ever develop a reliable test for consciousness in machines or animals? And if so, what would that look like? I think there are two separate questions here. One is what are the odds of getting good consciousness tests and second how likely is it we'll develop tests to identify positively and negatively veilanced mental states? In other words, states that have a positive or negative feel to them. In principle, you might think that we could do these things independently. You could imagine a non-concious system that still had some non-concious analog of positive or negative states. Biology is full of positive and negative learning mechanisms for example. So you might think we can identify those. I think that project is probably a bit more feasible and scientifically tractable than the project of developing reliable consciousness tests. So, and this might kind of connect to what I was saying earlier on about hydonic offsetting because even if there's a lot of uncertainty about whether a given system is conscious, we might still be able to intervene or design its mental economy such that if it is conscious, it's having a net positive experience and if it's not conscious, well, it doesn't matter. I guess that's because I think the kind of psychological dynamics of veilance are not brilliantly understood but a little bit better understood. for example, their role in motivational economy, the role in motivating behavior. We've got a little bit more of a handle on that than consciousness. As regards to a scientific test for consciousness in nonhuman systems, not to sort of undercut some of the more positive things I said earlier on, I'm very pessimistic that that's a near-term achievable project precisely because of what I said at the start that I think we are still an Einstein or two away from a good handle on what consciousness is. That there are some deep conceptual confusions. And maybe to take a slightly left field angle here, go off in a slightly different direction. It's not entirely clear to me that the paradigm that treats consciousness as a deep scientific kind is the right paradigm to use. Let me say a little bit more about what I mean by this. There is an assumption in the majority of consciousness science that there is some deep biological or computational or information theoretic quality that systems can have that determines whether or not they're conscious that we might find a natural kind corresponding to consciousness scientific kind that would allow us to reliably tell whether a given system is conscious. So this is a position I've called deep realism. the idea that there's some deep non-surface level property that systems can have or fail to have that determines whether they're conscious. And I think this maybe conflicts with some of our intuitions about moral status. So imagine we go out into space and we find this brilliantly sophisticated race of aliens who are perfectly charming. They have complex societies, advanced technological societies. We can talk to them maybe Star Trek style. We even fall in love with them. And then we realize that actually despite all this behavioral sophistication, they don't have this specific natural kind that our best science has told us is criterial for consciousness. What should we make of that finding? Well, one option is to say, "Oh, I guess they're not conscious. They don't matter morally. Despite their technological and social sophistication and the fact that John over here is married to one of these aliens, they don't actually matter morally at all." I find that a pretty monstrous conclusion to reach. So there are two other options. One could be to say even if they're not conscious, maybe they matter morally for other reasons. Maybe being socially sophisticated is one pathway to moral status that doesn't require consciousness. A final option is to say, well, clearly our science of consciousness has to be wrong. But that last move is more messy than you might think because it essentially makes the presence of consciousness determined by high level behavioral capacities. So in other words, we're rejecting our potentially really good candidate theory of consciousness on the basis of the fact that these aliens can have complex societies, form complex emotional relationships, this kind of thing. So we've shifted away from a deep realism about consciousness to a shallow view of consciousness where consciousness is a matter of exhibiting the right kind of behavior. And this kind of more behavioral level understanding of consciousness is one that is being I think increasingly explored. So Mary Shanahan a deep mind and imperial has defended views along these lines. They're sometimes characterized as sort of neovicensteinian. Roughly the idea here is that look consciousness is as consciousness does. We know consciousness when we see it. We don't need to crack open someone's brain or look inside their microprocessors in order to know whether they're conscious. If you're capable of forming these rich relationships, that's all we need to know. Now, that's a very radical view. I should still emphasize basically is completely at odds with a kind of implicit deep realism in so much of consciousness science for the last 30 or 40 years. But it's one that crucially I think is probably going to become more popular because I talked about the aliens, but really I'm talking about AI because we can increasingly form complex relationships with AI systems. They can surprise us with novel thoughts. They can engage in pretty sophisticated forms of reasoning and cognition and yet they're still pretty poor candidates for consciousness according to most of the leading theories. So something's got to give there. And I suspect that as we start to see more and more people develop deep emotional connections to AI systems, particularly what I've called in some of my published work social AI, that is AI systems that are specifically developed for meeting social needs. AI girlfriends, AI boyfriends, AI buddies and companions, I think as people start to fall in love with these systems, the idea that they might not be conscious is going to seem increasingly outrageous and increasingly counterintuitive. So that leads on to this anthropomorphizing you've spoken about. Could you tell us a bit more about some of the dangers of anthropomorphizing what's happening in the mind of a machine and how much harm can actually cause by people attributing consciousness where there is none. So I think there are some fairly straightforward risks sort of scientific problems. So a paper that my colleague Mart Helina and I wrote a few years ago for nature machine intelligence called use rich psychological terms in AI with care. We talked about how if you look in the comparative cognition literature for example animal minds literature you have very strict standards for when you can attribute a given mental state to an animal. We're still debating whether chimpanzees have a theory of mind. That debate's been going on going on 50 years now. We have these very scrupulous careful debates about whether corvid birds can do causal reasoning. And yet in the machine learning literature we noted people throw around psychological language all the time. They describe systems as understanding. They describe them as agents without necessarily the same kind of standards of evidence or burdens of proof that you see in other areas of cognitive science. And we say look this is just scientifically not ideal. We want cognitive science to be a joined up enterprise with consistent standards of evidence involved in attributing mental states. So that's one kind of scientific query. There's a risk for miscommunication and cross purpose in cognitive science if we're using these terms willy-nilly in scientific contexts. When we're dealing though not with anthropomorphism by experts but anthropomorphism by the general public, well there are some quite practical concerns. I think a lot of the mistakes that people make with large language models come from treating them as being more humanlike than they really are. For example, placing excessive trust in large language models when they make confident utterances because degrees of confidence is expressed by large language models don't work like degrees of confidence with us. Language models will be very confident in saying things that are false. This is at the root of the hallucination problem, which is itself a bit of an anthropomorphic term that I'm not a fan of, but I'll save that around for another time. So that's one little risk. But I guess the really big moral risk is false positives when it comes to consciousness and moral status. If we start to treat systems that are really not conscious as deserving moral consideration, if we falsely mistakenly attribute rich complex mental states to them that they don't really have, then that could lead to prioritizing them at the expense of beings that really do matter, whether that's humans or or conscious animals. So that's probably the sharpest moral risk. But of course there's the flip side risk of that which is the false negative worrying which is that by being too reluctant to attribute mental states or conscious states to AI systems we might miss beings that matter morally. One other thing that might matter morally if AI develops a kind of consciousness is that it will perhaps become harder to control. We might give it a set of guardrails. We might say do this but don't do that. And somehow spontaneously, if it's conscious, it might have a sense of freedom, a sense that it really matters. And if it suspects that we're about to switch it off or put it in a box, it might take powerful evasive action. So it might have a bad moral effect on other beings because it might discriminate against them in order to pursue its own survival. Is that something you worry about? Yeah, it's very interesting. I think in the human case, consciousness is very much intuitively bound up with concepts like free will, autonomy, conscience, the ability to make decisions for ourselves. And to the extent that you're worried about the control problem in AI, you might think that consciousness is a threat multiplier, that a conscious AI system might be more likely to make decisions that contravene our interests. I'm not super convinced that there is as tight a connection between consciousness and free will as people think. I'm pretty skeptical of free will in general, but I also think we can easily imagine or fairly easily imagine a system that had very complex cognition, had its own agenda, had interests that didn't align with ours perhaps that was not conscious. I think in some ways the scariest scenario for me when thinking about bad singularity type scenarios or bad ASI super intelligence scenarios is that we create super intelligence that wipes us out. But it's also not itself conscious. It extinguishes the flame of consciousness in the universe. So in some ways I think that's almost more scary to me than conscious AI that turns against us because at least the flame of consciousness would continue in that scenario. But more broadly my general view about the mind is that the relationship between consciousness and cognition is so murky and so messy that we should be cautious about assuming that any given type of mental state or any type of cognition is intrinsically bound up with consciousness. So maybe you could have a system that did all the things you just described, David, rebelled against this, had its own agenda, yet was non-concious. That seems at least a possibility to me. My last question would be to pick up on this the murkiness and the fact that as you said, we need an Einstein moment perhaps to see things differently. Present company excluded. Do you have a candidate in mind for such an Einstein or indeed might an AI itself turn out to be the Einstein? Maybe we'll switch on one of these systems and it will say, "I see you humans are struggling with a theory of consciousness." Look at it like this. Yeah, great question. I'm glad you gave me the second option so I didn't have to pick among which my colleagues is most likely to be the Einstein of AI consciousness. I should start by saying there's lots of good work happening in the AI consciousness field. I think we are making some progress. I'm a particular fan of the report by Patrick Butland, Rob Long, and others from 18 months or so back that looked at different theories of consciousness and explored what they implied about the likelihood of AI consciousness. That's good work, but notice that it's still conditionalizing on specific theories of consciousness. It's saying, well, if this theory of consciousness is true, then here's what would follow about AI consciousness, but if that theory is true, here's something quite different that would follow about AI consciousness. So I think that is progress but it still leaves so much murkiness and uncertainty. So yeah I actually completely agree with your second point. I think the best solution or the best hope that I have right now of the consciousness debate being settled the hard problem ever being solved is that super intelligence of some kind artificial super intelligence allows us to make the kind of conceptual leap required to really get through the problem. I say that because the existing consciousness debate I see as just so mired in fundamental methodological conceptual metaphysical issues that if there's going to be any real progress, it's going to require radical new insights of the kind that perhaps only super intelligence could give us. I've got one last question as well. You mentioned artificial super intelligence there carrying that flame of consciousness if it happened to wipe us out. Do you think that a conscious artificial super intelligence might treat us better and be better for humanity than a non-concious artificial super intelligence? There are a couple of different interpretations of that. One is just a kind of benevolent AI overlord question. Could governance be better if we passed decisions, moral decisions about the future of humanity over to super intelligence? But then I think the nuance that that Mrs. saying is maybe if it's conscious, it would treat us better. Is that suggestion? Yeah, the argument being that if it understands consciousness, if it understands the concept of consciousness, if it feels it itself, it might have some understanding of what we feel, some understanding of consciousness for us, of human consciousness. Yeah, it's an interesting question again and sorry to sound really boring by making this point again. And I think the relationship between consciousness and cognition is just so murky that the question of whether that would be sort of necessary in order for a system to feel appropriate empathy for humans is very unclear. Being able to really know exactly what someone's going through isn't necessarily a precondition for treating them well. Again, imagine our xenoan anthropologist goes out into the cosmos and discovers that there's a species out there that has some bizarre emotional state that is very seriously bad for them, but it's completely unlike anything humans have ever experienced. Nonetheless, this anthropologist realizes these aliens put a high priority on avoiding being in these states. It has various problematic behavioral consequences for them. Even without directly understanding what that state is like, as xeno anthropologists could still be compassionate without that level of understanding, they could help these alien beings avoid being in this very exotic state in the first place. So it's not obvious to me that the ability to experience what we experience is a precondition for good moral treatment by a future AI system. Another little glimpse of this is if you ask chat GPT or any other large language model what's it like to I don't know be a gap year student visiting Mongolia you ask it to give some detailed description of what a human experience would be like it'll do a really really good job despite the fact it has no idea what it's really talking about so a lot of the information that we think of as bound up with conscious experience with conscious memory there is a non conscious or justformational analog of that that could be mastered or could at least be relatively well understood or implemented by plausibly non-concious systems. So I get the view that if we were to meet in two years time say and have another recording there would be a lot more to say in part because the field is growing in part because more members of the public are asking questions and encouraging research but in part because we'll have more complicated more sophisticated large language models possibly other kinds of AI and they'll be giving us new challenges and new models to think about. So, I do hope we'll be able to tempt you back. But in the meantime, I want to thank you very much for giving us a lot to think about and I'll be looking out for that Einstein coming and telling me that my whole ideas of consciousness need to be turned on their head. Wonderful. A pleasure. I'd be delighted to come back in 2 years. I'm sure we'll have loads more to talk about then. Thanks very much, Henry. Great to have you on. [Music]