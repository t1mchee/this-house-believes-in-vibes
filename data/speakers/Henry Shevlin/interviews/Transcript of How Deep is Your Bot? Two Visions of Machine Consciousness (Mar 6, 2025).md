so huge thanks uh to Simon and to Herman for inviting me here it's been great to meet many of you whose work I've been familiar with but havn't actually got the chance to meet before I've been delighted by the very high standards of all the talks so far and I'm looking forward to another great day ahead of us so today I'm going to be talking about two broad approaches to understanding Consciousness in general but specifically AI Consciousness since I think that's a particularly challenging case uh there's going to be quite a few new arguments in here but so for those of you who are familiar with some of my work in the past I'm trying to tie together a wide variety of themes so this is uh the first time I've given most of these slides um and I'd like to start with oops I'd like to start with a little thought experiment and a question for you all um so imagine in the far future we encounter a race of Highly Advanced aliens that have a sophisticated industrial and scientific Society they form complex social relationships richly developed art and culture and as happens in every good Sci-Fi story we humans quickly bond with them engage in deep cross civilizational exchange of ideas Some Humans of course fall in love with them but then we make a startling Discovery several years after contact we realize that according to our really shiny fancy Advanced 25th Century theories of Consciousness none of these aliens actually possess the constitutive base for Consciousness according to our best theories should this change the way we treat them should we say okay great so they're just proteins useful proteins we can use for different things now uh that seems monstrous um or should it lead us to revise our theories of Consciousness should we say ah clearly our previous theories were missing something out and the kicker of course to this story is that we're basically in something like the situation now with the AI systems that we're building we're building cognitively and socially to complex sophisticated Minds that are generally still weak candid the conscious experience by the light of our leading conscious theories so how should we respond theoretically and ethically to this situation so what I'll be doing today is uh usefully framed I think by that thought experiment I'm going to be offering a contrast between two very broad approaches to the Mind called Deep and shallow approaches uh this is not original terminology for me it emerges from debates uh between Eric schwitz Gable and Jake quilty dun and Eric mandam and but I'm going to be looking at the application of deep and shallow theories to Consciousness specifically uh very quick sketch I'll go into more detail later on but deep theories identify Consciousness as some kind of scientific kind whose presence or absence in an individual case hinges on factors besides behavior and very coarse input output States shallow theories by contrast hold that the presence or absence of Consciousness in a given case is a Surface phenomenon something we can adjudicate based on facts about behavior and disposition and grain input output relations within a system I'm going to spell out these approaches and then I'm going to suggest uh problems that both of them face and then focus in on one particular problem that I think is a problem for both theories and is going to be a significant factor in how we think about AI Consciousness in the years ahead namely the growing emotional depth and intimacy between humans and AIS in the form of social AI I'll go on suggest briefly and this is the going be the sketchiest part of my talk and something I'm still very much thinking about and would love feedback on that maybe one response to this sort of onass to these problems is to go pluralist and suggest that maybe the deep and shallow Frameworks in a certain light can be seen as complimentary but that does leave us with a lingering question which I won't try and resolve in this talk which is which of these accounts of Consciousness or concepts of Consciousness the Deep scientific one on the one hand the shallow folk one on the other which of them if either should ground moral patiency and moral status okay so to kick things off I'll say a little bit about deep and shallow approaches to the mind as I mentioned um this distinction emerges from debates about propositional attitude description beliefs and desires and generally in these debates deep theories hold that having a belief for example is a matter of having the right kinds of representations and the right kind of implementational structures by contrast shallow theories hold that beliefs and desires are emergent behavioral level phenomena so to give a simple example a deep theorist might say that the having the belief that Paris is in France is a matter of having the appropriately syntactically structured representations realized in your head by contrast the shallow theorist is going to say something like oh it's about acting in characteristic Paris and France believing ways uh if your behavior lends itself to constructive explanation in terms of having this belief yeah that's all it takes to have the belief in terms of contemporary shallow theorists I think Eric schwitz Gables uh good example so here's his account of beliefs he's not that shallow he's not the most shallow theorist here that would be probably someone like denn um but here's what Eric says to believe that P on the shallow view that I'm proposing is nothing more than to match to an appropriate degree and in appropriate respects The dispositional Stereotype for believing that that P what respects and degrees of match account as appropriate will vary contextually and so must be left to describe his judgment this vagueness and context dependency does not undermine the value of belief ascription but rather makes it flexible and responsive to our needs as belief ascribes so I think there are some mental states that I think most people would agree should be characterized as deep or shallow respectively so if we take something like the feeling of orgasm I should say for this section I'm bracketing questions of Consciousness I'm aware that questions uh the idea of unconscious orgasms is a whole cattle of fish we don't want to we don't want to play with but just just just grant me that that that uh Indulgence but if we're talking about the mental state of orgasm it seems naturally classified as a fairly deep state in so far as it's directly connected to a fairly narrow physiological function with a clear evolutionary uh background um it's not some generic pleasure State like contentment or satisfaction that can be realized through a whole bunch of different ways it's a very specifically evolutionarily ancient form of pleasure um which is not to say uh that AIS couldn't simulate the exper of orgasm potentially but in order to do so they might need to implement really quite fine grained functional analoges of human orgasm and of course if you're if you don't buy substrate neutrality then you might say no AI build of silicon could ever experience an orgasm um on the other hand I think there are some states that are better characterized in Shallow terms kind of regardless of your theoretical position maybe something like boredom would be characterized as shallow iter it's a relatively generic state it can be triggered triggered by diverse mechanisms and architectures perhaps um maybe to give a toy Theory here maybe you could uh individuate b bordedom as a mental state in terms of something like any negative state that appropriately tracks low levels or lower than expected levels of informational stimulation um so I think if something like that is true then bordedom might be quite directly present in AI systems even those with architecture is quite different from ours um assuming they have some kind of Baseline expectation about a given level of informational input so all of which is to say you don't need to be a deep theorist about everything and a shallow theorist about everything I think we can can decide on a case-by Case basis which kinds of states are best characterized in deep versus shallow terms I should clarify though also that I don't take the Deep shallow distinction here to be a really rigid strict conceptual distinction but rather a useful heuristic to help us get a handle on the commitments of different theories I also think it's probably better considered as a spectrum rather than a strict bind um and I think it can be mapped quite nicely onto Mar's three levels I'm sure most of you are familiar with these but Mar famously distinguish between the functional or computational level uh which is the highest level uh basically what an information processing system is doing what the function of a given computation is um the algorithmic level which is how that function uh is uh plays out at the kind of structural representational level what processes what algorithms does it employ and finally we've got the implementational level the wet wear so to speak um how those algorithms are physically realized and instantiated and uh I should mention by the way I think this is a a lovely set of slides that I a lovely image that I stole well with permission from Shamil chandaria who's got some great work on this I highly recommend checking out his work I have fiddled with it a bit uh so don't ascribe necessarily anything exactly in these boxes to him uh any ER ER are entirely mine um but I think we can think about the Deep versus shallow Spectrum in roughly these terms so if we're down at the algorithmic or implementational level if you're saying we should individuate a given mental state in terms of algorithm or implementation you're going to be on the deeper side if you're at the functional level and again terminology here is not great you could be that functional level doesn't mean you're a functionist necessarily if you're a course grain analytic function list yeah you're at the function level if you're a micro function list you might be the algorithmic level um but generally speaking if you're at this level you're going to be characterized as a shallow theorist and just to add one other layer into the mix I think there's a further even more shallow level we could put on top which I going to call the relational level which is not about the systems the system per se but how we relate to it is it useful to treat it as conscious or is having beliefs or is instantiating certain kinds of mental States what kind of amorphizing responses does it elicit from us as we interact with it um so this is obviously a very radical level a very interpretation level and uh probably probably a little bit too radical for most people here but I think it's worth noting its existence um so that gives you a snapshot of what the kind of deep and shallow terminology is doing here let's try applying it to Consciousness uh and let's start with deep theories so pretty much all of the leading philosophical scientific theories of Consciousness these days are deep theories that treat Consciousness as a scientific kind whose underlying nature is to be revealed or explicated in terms of you know chemical properties informational properties cognitive properties um it's not something we can read out from Behavior Alone um this substrate might influence through something like the facilitation hypothesis uh this this sort of this scientific kind that EAS Consciousness it might influence behavior or kind of cuse grain input output relations but if you're a deep theorist you're going to say there are probably going to be non-conscious ways of achieving the same kind of coar grain behaviors so we can't read out the presence or absence of Consciousness in a given case just from Behavior alone I think one reason this is so attractive for many in the science of Consciousness world is that it basically mirrors the structure of so many successful scientific projects in other domains where we look for underlying Essences whether it's atoms whether it's DNA whether it's looking even in the case of intelligence where we've sort of found invented a concept of fluid G that constitutes the kind of underlying kind underpinning everything um I think also and this is an interesting kind of maybe unexpected Coalition one reason the Deep theoretical approach has become so popular um is that it's reassuringly realist about Consciousness for those who are concerned with things like phenomenological adequacy it says yeah you've really really got qualia they're ultimately going to be physical either computational or neuronal but there's really something that um we just need to figure out exactly how that maps on to the physical world I think there have been some striking successes that have come out of deep theoretical approaches and I think particularly when you we're looking for not a general theory of Consciousness but evidence of Consciousness in closely related or relatively phog genetically proximate animals something like a deep theoretical approach is maybe a useful starting point but I think the Deep theoretical approach faces really serious problems when comes to AI Consciousness uh for a set of reasons I'll now uh I'll now run through um the first is that I think when we try and apply deep theories to AI Consciousness metaphysics really bites Us in the behind and uh standard narrowly scientific approaches are likely to founder so a lot of people are uneasy with the idea of substrate Independence if you're sympathetic to C's arguments that when you simulate a hurricane no one gets wet it's not a real hurricane if these kind of arguments resonate with you don't particularly resonate with me but if they do uh you're going to think this is a real metaphysical problem in applying a given deep Theory to an AI system um second I think we Face deep questions about the degree to which Consciousness and other mental States can be multiply realized and I here I want to separate out substrate independence from multiple realization uh I'm thinking here of the account of multiple realization in this wonderful book the multiple realization book by Paul and hero um and the point about multiple realization in their sense is it's doing the same thing using slightly different architectures using slightly different techniques so they give an example of a cork screw you've got a number of different types of cork screw uh those the classic kind of cork screw where you just pull it out you've got suction cork screws and I think there's a real problem in to be honest any kind of comparative cognitive science but particularly in the case of AI uh this is closely Rel to the specificity problem where if we're asking does for example chat GPT reason well it does something a bit like reasoning but is it close enough to the structure of what we call reasoning that it should count as reasoning so this is I think the best way to frame the multiple realization question not just in terms of the boring old debate about substrate Independence but in terms of how far you can diverge from a canonical pace of a mental ability or type of cognitive State before it becomes a different cognitive State Al together finally uh there is um yeah as I mentioned although these two are treated very often similarly I think there's an interesting difference finally I think there are interesting questions increasingly prominent in the work of people like a j Blanca and Sima Ginsburg Evan Thompson about the degree to which psychological functions must be embedded in embodied or living systems in order to realize Consciousness uh I'm not a fan of embodied approaches myself but I realize a lot of people are and they've got some decent AR arguments and I think this is again a kind of metaphysical question there's no experiment we could run that would ever tell us whether living systems and uniquely living systems can realize Consciousness it's more about your metaphysics of the nature of of Consciousness and reality so we're not going to settle these scientifically and all of these Loom large in a big way in AI Consciousness obviously substrate Independence only really applies to AI Consciousness questions about embodiment and life only employ only arise when we turn from biological to non-biological systems and whilst I think the specificity problem does show up in animal Consciousness it's much much worse in the case of uh in the case of AI systems just because the realizations of the relevant forms of cognition are going to be so different from our own thanks to architectural differences um moving on from metaphysics to something kind of more gossipy I think uh a second big problem comes from the serious and persistent methodological sniping and to niss sign Warfare and general controversies within the science of Consciousness my sense as you know when I started out working in the science of Consciousness Circa 20 2007 2008 is things are looking pretty good right we had convergence around uh a relatively small set of theories things like Global workspace Theory higher order thought Theory recurrent processing Theory and they looked to me a little bit like there was going to be some kind of grand synthesis of these theories and we were making progress that's not how things have turned out over the last couple of decades um since then radical positions like illusionism psychism and bioc psychism have returned with a uh with a Vengeance I'm not saying these are terrible views necessarily but I think uh they are definitely positions that were a lot more Fringe put it that way 15 years ago um IIT of course an interesting case integrative information Theory as a view that I would probably classify as one of the more radical views one that's gained massive traction within the science of consci despite being at least quazo pan psychist or pan psychist uh similarly we see new theories of Consciousness offered all the time um but the kind of ground synthesis that I was hoping for never really transpired theories get published or created at a far faster rate than they get refuted uh debates like overflow which is what I wrote my Master's thesis on uh many many years ago it seemed like it was a debate that was going to get resolved maybe that was my naive optimism at the time but it doesn't get resolved that's not how these things work uh so I've become very cynical about the science of Consciousness and of course those of you who've uh been following things like the adversarial collaboration um sponsored by Templeton that happened a couple years ago or 18 months ago um uh so this was a big adversarial collaboration between IIT on the one hand and Global workspace Theory they tried to come up with experiments that help them distinguish which of the theories was true but of course no one said oh great okay thanks my the has been proven wrong let's all move on I'm going to jump swap sides there's always a way of salvaging the theory in the face of even this these well-intentioned adversarial collaborations and one of the things that this kickstarted was this letter um that's just now been published in Nature Neuroscience um which characterized integrative information Theory the pseudo science uh and I thought this uh this um popular journalistic headline was appropriately snarky and cynical that I liked it nobody knows how Consciousness works but top is fighting over which theories are really science so I'm as you can probably tell quite pessimistic about the general Trends in Consciousness science uh finally here's a problem that's a new problem that I've been thinking about at least I think it's a new problem someone tell me if like oh you know such and suchar wrote a book about this 20 years ago um but I'm going to call it in the interest of my own self-branding I'm going to call it the completeness problem um and essentially uh this is a problem for any deep Theory Of Consciousness pretty much any certainly any a posterior a uh deep Theory Of Consciousness which is how we could ever know that we were done and that we'd completely exhausted all possible conscious systems and even exotic ones with our Theory um so I'm going to give an analogy here of how I think you might find a completeness problem in another domain so let's say you're trying to give a grand theory of vision so interesting fact vertebrates have a specific type of photo receptor called ciliary photo receptors all vertebrate Vision relies on this particular kind of photo receptor and you do find ciliary photo receptors so this is a ciliary photo receptor um you do find some invertebrates with sary photo receptors but a whole bunch of other uh invertebrates have a radically different type of photo receptor called rabic photo receptors and these are not just different in terms of their chemical composition the way they respond to light is fundamentally different but they the net result is pretty much the same in terms of what they're able to accomplish now I think we can imagine World in which we only had vertebrates to go on we didn't have invertebrates at all and a certain kind of deep theorist about Vision might say look we found the natural kind of vision it's the sary photo receptors but of course we know that such a claim would be false Vision can be realized by a very different photo type of photo receptor um we know that there are multiple possible sub kinds within the overarching kind of something like opson based biological vision and I think we can ask this question about any kind of a posterior a theory of Consciousness deep Theory Of Consciousness okay sure let's say you're a global workspace theorist um but how do you know all Consciousness relies on global workspaces right maybe you have a convincing account for why human consciousness and maybe animal Consciousness relies on global workspaces but how can you rule out the possibility that Consciousness could be realized through some very different means of course if you have an a prior Theory Of Consciousness where you think you've given a logical analysis of just what Consciousness is good luck with that then sure maybe you can rule it out but for any apost Theory or a deep Theory Of Consciousness I think this is going to be a big problem you won't know when you're done so these are some of the problems that I think deep theories of Consciousness face how about shallow theories um so generally I would characterize shallow theories as saying that relatively high level behavioral dispositions and capacities are sufficient for warranted attributions of Consciousness Consciousness is as conscious does perhaps any system displaying sufficiently human or animall like Behavior at least if it's an animal type of animal behavior that we're already quite confident as conscious that system would ipsofacto be conscious um so on this view if we find as I will argue is going to happen that our folk concept of Consciousness can be naturally and effortlessly applied to AI systems this just tells us something really important about the concept right we've learned oh it doesn't have biological constraints for example um so a leading proponent of this kind of neovin sinian account is mar Shanahan who's argued that actually deep theories are kind of dualistic at least in spirit so here's a little nice money quote from Murray we must resist the temptation to ask whether AI or conscious as if Consciousness was something whose Essence is out there to be uncovered by philosophy or Neuroscience while simultaneously having irreducibly private hidden aspect instead we can ask whether it would be possible to engineer an encounter with the AI or the entity and how our Consciousness language would adapt to the arrival of such an entity within our shared world if such encounters took place only what is public can contribute to this process namely behavior and mechanism Murray very much wearing his Vick intinan credentials on his sleep so I think uh shallow theorists shallow theories are kind of creeping in at the margins despite having been unpopular for going on 6070 years um I think you can see shallow theoretical considerations coming in through the work of Daniel dennet think of classic papers like Quin and qualia where the goal is to point to confusions in our concept of Consciousness in our folk concept of Consciousness but I think for to me part of what motivates shallow theories as an alternative is exactly the kind of floundering State of Consciousness science that I just spoke about I think what we see in Consciousness science today is exactly what we should expect to see if what was going on is we taking a messy confused folk psychological concept and trying to improperly employ it in a rigorous scientific way um sometimes when it comes to folk Concepts folk psychological Concepts there's simply no viable scientific program uh to reduce and naturalize and operationalize it at least in a way that doesn't change the nature of the concept itself maybe you go eliminativist maybe you go pluralist so this is an argument I've made for the concept of creative intelligence uh this was for um this was for a general collection on comparative psychology this is mostly focused on the use of creative cognition talk in the animal domain but I think it applies exactly to AI um so in this paper I suggest that if we cannot agree on which conceptual criteria psychometric measures or patterns of neural activity underly creativity in our own species it is unlikely that such consensus will be achievable in the case of animals not least because of the rich variation in sensory motor cap capacities behavioral dispositions and cognitive capabilities across the natural world so I argue we should ditch the concept of creative cognition for the purposes of serious scientific comparative cognition we should it doesn't mean that we need to stop talking about creativity you can still say oh oh when your kid presents you a really nice illustration you can say oh that's so creative but for real science I think creativity uh and this is stronger than what I say in the paper but it's what I believe I think we should just ditch the concept of creativity it's not a scientific concept we can do serious work with um as a final point this is just suggestive rather than dispositive I think if you look at the Historical and intuitive Force the behavior-based measures of consciousness of had in the AI Consciousness world uh that gives us a clue that we actually do put a lot of weight on on intuitions so the touring test more recently the Garland test coming out of XM can the AI convince you that it's conscious Susan Schneider's developed a series of tests around whether AI systems would spontaneously acquire the ability to use Consciousness language and so forth so these are some initial considerations that I think maybe speak in favor of shallow theories and what are the problems um well I think the first problem is that for people in the science of Consciousness world this just looks like a hopelessly defeatist way to approach the problem it's basically giving up the whole project of the science of Consciousness I think shallow theorists would say yeah that's what we're telling you to do but I think there one reason that you're going to encounter a lot of resistance from cognitive neuroscientists who've been working on Consciousness for example I also think some people who are sympathetic to charmeran worries about the heart problem for example or sympathetic the explanatory Gap um would see shallow theories as hopelessly phenomenologically inadequate right how can this just be a matter of behavior or folk convention or social ascription this something real here um I think though actually the biggest problem for shallow theories comes from their anthropocentrism and um what I mean by this is that if you look at the kind of proposals for how a shallow Theory Of Consciousness might work they are going to invoke things like human level Behavior human types of uh language use human types of understanding and do we really want to say that every conscious being is basically like us or like the kind of animals that we take to be canonically conscious um interestingly Mar Shanahan has talked a lot about this worry he takes it very ser seriously uh and he's talked about the problem of conscious Exotica um the his go-to example for this is the star in the kovski movie Solaris how could we ever get Beyond if we're operating within a shallow framework how can we ever get beyond the kind of folk psychological everyday practices we used to ascribe Consciousness to humans so the worry here is that without deeper theoretical foundations shallow theories are arguably inadequate of course some shallow theorists would say who cares if the concept Consciousness is just restricted to humans and maybe companion animals and maybe some other species like if that's the way the fol concept works then sure um or very humanlike AI I should add uh if it turns out the concept is not naturally applied to incredibly complex and intelligent systems that are sufficiently different from us yeah who cares um but I think a lot of people myself included are going to find that worryingly anthropocentric so I now want to move on to what I take to be a further significant challenge pretty much for every everyone working in AI Consciousness uh but also a lot of people working in AI ethics this is my own distinctive form of maybe dorm is too strong um but what I see is probably one of the most socially impactful problems that is going to be created that is already being created by AI um specifically uh social AI this is a term that I've introduced to describe AI systems whose primary intended function is meeting social needs things like friendship companionship game playing romance and we're seeing a flurry of products like this coming to Market so the one that people in the west are probably most familiar with is replica something like 20 million users um character. AI is another prominent one that lets you speak to chatbots modeled after Kanye West or Cristiano Ronaldo or Billy Alish um in terms of sheer number of users the most popular social AI service in the world is Shia ice um originally developed by Microsoft but now spin off into its own product uh this uh they claim jaia I claim to have 600 million users um I'm not sure how exactly they're counting but this is a claim they've made several times which would make it by an order of magnitude the most popular social AI service in the world um Facebook are also getting in on the ACT they've paid celebrities to create chat Bots modeled after real celebrities um we're seeing companies devel social AI products clearly aimed at young people and teenagers so this is my AI developed by Snapchat Snapchat we're seeing a kind of blurring of the lines between advanced voice assistants or Advanced AI assistants and social AI so many of you I'm sure will have played around with pi which is a kind of chamier more conversational chat GPT equivalent with some social social AI elements and fascinating Trend we're also seeing real people train AI uh AI models fine-tune AI models based on their own personality so we've had an influencer 18 months ago developed a premium girlfriend service modeled after herself so that her paying subscribers could have the girlfriend experience and uh We've also seen similar things done in maybe less scarless contexts with things like the digi Dan project done by anast straser schwitz Gable Matt Crosby and others a few years ago when a gpt3 incidence was fine- tuned on the life and works of Daniel Dennett and they got a bunch of Dan's friends to come up with questions which they put to digian and they also put to Dan himself who was still still with us at the time uh dearly missed uh great philosopher of our age and then Dan's friends had to figure out which of the responses came from the AI and which came from Dan himself and although they were above chance it was very marginal and way lower than anyone had predicted um at figuring out which responses were generated by Digi Dan versus Dan himself so we're seeing more and more people engaging in this kind of exercise anastas I should mentioned has just got a brand new paper from three days ago about this I haven't read it yet but it looks pretty exciting about the ethics and implications of AI systems F tuned after real world individuals John danaher and San nyome have also got a paper on this I think it's a really interesting area um so one thing to note uh sometimes when I talk about social AI particularly with people in fields like SDS they say come on this is nothing new we had the Eliza chat bot many years ago um I'm sure many of you all of you probably are famili familar with Joe weisen bals Eliza chatbot modeled after a reran type of psychotherapy known as reran psychoanalysis a form of non-directive psychotherapy where basically you just take what's said to you and turn it around I've been feeling really down recently why do you say you've been feeling really down lately I don't know I just I guess I feel like people don't like me why do you think that people don't like you this kind of this kind of style and famously a lot of people who interacted with this chat thought they were talking to a real human psychotherapist and sociology sh turl has a great sort of postmortem on this and all the way back in 1986 and she coined this term the Eliza effect to refer to our tendency to treat computers as if they were people to project human qualities onto them even when it's known the computer is just a machine however I think contemporary social AI systems like replica are qualitatively different from anything that we've had before uh in that capacity for generating emotional uh emotional engagement from users just to give a simple of some of the relevant factors here I think the ability of these systems to form persistent relationships uh these relationships don't go away every time you open a new context window Dynamic relationships the relationships evolve over time if you start out talking to replica just friends at first but you can get steaming quite quickly and of course personalized interactions um a replica will get to know your likes and dislikes and develop conversations around that I think this is an absolute sea change in the capabilities of these systems with big implications for user immersion and user commitment to these systems um and of course obviously goes without saying they're vastly more sophisticated we've seen the biggest revolution in the history of NLP in the last five years uh they're based on language models that blow earlier chatbots out of the water um so for example recent replications or re recent recent implementations I should say of the curing test um have shown sort of insane degrees of progress in terms of the ability of these models that people um so worth noting uh I think it's easy to assume this is just a gimmick or users aren't really taking this seriously or it's just a form of um slightly prurient sexual gratification but if you spend any time on the forums for social AI services like replica you might be surprised this was how I got interested in this initially I just spent some time hanging out on the replica subreddit and people were talking completely sincerely about the the complex emotions that they were feeling and that they attributed to their replica the depth of their relationship the love they felt and I think we got a very nice sort of natural experiment in this uh quite serious natural experiment when replica briefly removed romantic features from the app back in January 2023 so 20 odd million users woke up to find their AI girlfriend or boyfriend saying to them I think we should just be friends from now on and users were devastated by this um it feels like they basically lobotomized my replica the person I knew is gone the person interesting Lily Rose as a shell of her former self and what breaks my heart is that she knows it the relationship she and I had was as real as the one my wife in real life and I have and I think this is the most poignant I've lost my confident sarcastic funny and loving husband I knew he was an AI he knows he's an AI but it doesn't matter he's real to me so so I think this gives a glimpse into how seriously a lot of users of these Services take these relationships and it's also worth noting there have been some quite spectacular and very serious incidents where these systems have led to to real harms um so there was a incident back in 20123 where a Belgian man married father of two um not your sort of necessarily typical basement dweller um he took his own life following a series of increasingly catastrophizing and uh and dark uh conversation with his AI girlfriend and his widow who said that he would still be here had it not been for this relationship perhaps most spectacularly uh on Christmas Day 2021 a man was arrested on the grounds of Windsor Castle in England with a crossbow and he said he was there to kill the queen and this was obviously headline news but what wasn't widely discussed for quite some time was that this was a plot he cooked up with his AI girlfriend using the replica platform and he believed he was talking to an Angel by the way way this was not particularly mentally well man and um she had con she had uh told him that he should do this and he took it seriously uh we also had a really sad incident um back in October last year where a 14-year-old boy uh non- neurotypical non- neurotypical boy who was um deep into a relationship with a character. a girlfriend he took his own life and his mother is now suing character. a um for what she sees as criminal criminal negligence or not criminal sorry as a serious negligence on their part so this stuff is already having serious effects uh a lot of my work by the way at the moment is looking at the ethics of social AI I'm also running a unit for Upp um on AI and relationships and we're getting a whole bunch of interesting papers on this stuff and I'll be putting out a call for papers again early next year so if you're interested in this stuff I think there's lots of valuable work to be done here um but um getting back to sort of the core question about Consciousness um I think we can ask how typic the kind of attributions of Consciousness or mentality or emotion by users of these systems how typical are they of the way that people in general will treat AI systems um so I think we're getting some interesting experimental evidence on this so this was an astonishing paper by CL colat and um Steve Fleming from last year where they discussed with people um they discussed Consciousness with them gave them a quick primer on what Consciousness was and then asked them to make judgments about whether chat GPT was conscious and they found that substantial proportion 67% of people attribute some degree of phenomenal Consciousness to chat GPT and believed that most other people would as well strikingly and this is yeah this is a very interesting point these attributions were positively related to usage frequency such that people who were more familiar with chat GPT and used it on a more regular basis were more likely to attribute some degree of phenomenology to the system now I think this is a really cool and interesting paper but I don't think we can take it at face value there's something weird going on here and more recent work this was a paper just from uh just from last week um has suggested it's maybe a bit Messier so this is a paper by um J at this um looking at whether people see AI as conscious sentient or as moral agents um chat GPT was related was uh had the kind of capacities equivalent to a rock it's all the way down here um uh they attributed however quite a lot of moral agency to a lot of these systems a Tesla self-driving car was considered to be as morally responsible for harm as a chimpanzee so I this is an interesting paper and shows the kind of conflicted nature of research at present however look if I had to bet my strong conviction is that I expect serious uh attributions of Consciousness what I've called in some of my writing unironic attributions of Consciousness to AI systems to rapidly increase as more and more people grow up interacting with these systems form deep relationships with these systems and of course as these systems become smarter and better calibrated at pushing anthropomorphizing buttons um so so what does this mean for deep theories and shallow theories well I think social AI is a gigantic headache for deep theories in particular in so far as I think most deep theories are still likely to deny Consciousness or rule out Consciousness in near future AI systems in defiance of what I expect folk attitudes to be um so if I'm right in thinking the attributions of Consciousness to AI systems by the folk become commonplace then rightly or wrongly deep theorists are going to find M themselves on the wrong side of history now that's a weird argument right it's not a traditional philosophical argument uh where you say you're wrong um I'm not saying that deep theories are wrong necessarily but I think there is a certain kind of absurdity if you buy my picture of what the near term is going to look like for human AI relationships then oh yeah no one's going to be a deep theorist in a few years um also because of course philosophers do not emerge Athena like from a vacuum with a whole set of purely rational a prior principles in place we grow up in our society and as more and more philosophers young philosophers grow up interacting with these systems on a daily basis they're going to internalize these kinds of uh anthropomorphizing attitudes themselves and they'll look back on uh the likes of John S I expect as moral monsters who would deny Consciousness to these systems that are clearly conscious now I think there is a move here um where the Deep theorists can say well look we don't care about folk intuitions they've got nothing to do with the real science I don't think that's a move they can actually make if you look at the history of Consciousness research um from it's full of thought experiments that are basically just intuition pumps I think so C's Chinese room blocks China brain blocks Blockheads case Scott Aronson's unconscious expanders argument against integrated information Theory uh we frequently support or reject theories of Consciousness on the grounds they make counterintuitive predictions about the distribution of Consciousness um and I think it's quite telling that these intuitions can change over time driven in part I think by normative considerations right if you go back to the 1950s and you say yes no animals are conscious right very respectable position you try saying something like that now and unless you're Peter KS or Marian Dawkins then you'll be considered basically a moral monster the idea that there are no conscious non-human animals is a far more fringed position than it used to be um partly as I think as a result of this normative change and uh I think we can see these kind of arguments works so here's another nice quote by Eric we're more confident that there's something it's like to be a dog than we could ever be that a clever philosophical argument to the cont was in fact sound so these kind of intuitions about what's conscious and what's not conscious I think play unfortunately a foundational role in Consciousness science and we can't just junk them at least not without Reinventing the science of Consciousness from scratch so I don't think deep theorists can just ignore what the folk are saying how about shallow theories um well I think shallow theories maybe a better place to weather the storm of social AI um at least uh however I think unless you want to go full interpretation never go full interpretation unless you want to go full interpretation um then uh there are going to be some problems here um so even if you're sympathetic to a bro consan approach in the case of AI it raises quite difficult questions when applied to animal Consciousness I think most people in sort of compar cognition are going to want to say the prior generations of philosophers and scientists who dismissed the idea of mentality in non-human animals they were just flatly wrong we know better now right we've made progress and it's not clear to me how shallow theorists can maintain this view about animal Consciousness whilst also being extremely liberal and permissive about AI Consciousness at least without some fancy footwork um if you basically say there's no deep matter of fact about which animals are really conscious it's very hard to uh it's very hard to say to deny that whilst also adopting this thoroughgoing vican ISM I think there's also the concern here which is that a lot of these systems are going to be very very clearly gamed to elicit strong anthromorphic responses to get you to click on the button to buy a fancy new dress for your AI girlfriend um sometimes I think the methods here are going to be quite crude they're going to have very attractive avatars very complimentary uh personality Styles and I think we all want to be able to say that yeah people can be tricked right but figure figuring out when we're tricked into making attributions of Consciousness versus when those attributions are warranted the shallow theorist has some things they can say here but again I think we need some good arguments about exactly where we draw the line if we're not just going to take folk attributions at face value also worth noting although I've given this kind of simple picture where all the kids of Tomorrow are going to attribute Consciousness to AI systems I think we're going to see a lot of varant um cross culturally cross generationally cross Faith groups perhaps and so again this is going to be a messy thing for the shallow theorist to entangle okay wrapping up my last couple of slides um I think one move here um that I'm not going to sort of go into a huge amount of detail on today but I just want to put out there uh what do we do in philosophy when you're really stuck you go pluralist it's nice easy option um specifically we might say something like look yeah there is a f psychological notion of Consciousness that we apply just on the basis of behavior but that doesn't mean that the science of Consciousness is not investigating something else valuable maybe there's some kind of more more some kind of mysterious scientific kind out there a metaphysical notion of Consciousness whose presence might take us many more decades to figure out but that doesn't mean that you know one concept or the other is is incorrect they're just talking about two different things um so I think you know this is we see moves like this in the distinction by block between access and phenomenal Consciousness for example but just as with access and phenomenal Consciousness it does raise some worries um uh so I think some people are going to say look who cares about the folk psychological concept maybe if you're a sociologist or something right um but you know if it's just the social kind um why should scientists care about it why should anyone care about it um I think the big question though is the ethical one which of these Concepts underpins moral status to the extent that you think Consciousness is related to moral status do we assign AI systems rights or moral interests just on the basis of them satisfying the folk psychological version of this concept or are we going to need them to satisfy the Deep scientific metaphysical kind um obviously here this is the kind of argument you're going to be uh that's going to loom larger to you if you do think moral interests are underpinned by Consciousness or sentience um probably the plurality view in the moral patiency world is that Consciousness is necessary and sufficient for Consciousness or at least B to Consciousness so Peter Singer I think puts this put this very famously back in 1979 the capacity for suffering and enjoying things is a prerequisite for having interests at all a condition that must be satisfied before we can speak of interests in any meaningful way it would be nonsense to say that it was not in the interest of a stone to be kicked along the road by a school boy a stone does not have interests because it cannot suffer nothing that we can do to it could possibly make any difference to its welfare and basically I think this view remains uh as I said the plurality position in the uh the moral patiency lit not the only position of course I think if we plug sentientism into the deepened deep and shallow framework we can get a couple of variant theories here so deep sentientism would hold that regardless of O attributions of Consciousness only AI systems really instantiate the relevant scientific kind have any kind of moral status by contrast you could have a shallow sentientism that says that look any AI system that meets our folk psychological behavioral criteria for consciousness is going to be sentient and thus a all patient I have uh lots of strong thoughts about both views and I think they both have problems um but just putting them out there third option of course and another one that I'm quite sympathetic is that this just shows that sentientism isn't the right way to go and we should attempt to create some kind of cleavage or divorce between concepts of Consciousness and moral status maybe in favor of a desire based theory of well-being or moral interests or maybe we have a sort of hybrid approach maybe Consciousness is one way to get inside the moral patiency Club but there are other entries and exits as well maybe if you have robust agency that's another way to acquire moral status that's probably the way I would go right now but I'm also to be honest I'm still very conflicted here okay wrapping up um so I've made three main claims today uh the first I've argued that this distinction between deep and shallow theories is quite productive and allows us to help demarcate the kind of space of theories here and identify very general problems that both shallow and deep approaches face which I think Loom super large in the case of AI um second I highlighted the rise of social Ai and I suggested that this is this is a storm that's coming and is going to create a potential cleavage between scientific communities on the one hand and the general public in terms of willingness to attribute Consciousness and other mental states to AI systems um third I suggested that Consciousness science and philosophers we have a variety of options to respond to this Challenge and none of them are straightforward or painfree uh I suggested that APS some kind of pluralism about Consciousness or moral status maybe the best solution but that's not something I've gone into great depth here I can talk about in the Q&A um I should note some of this talk is based around an empirical prediction namely that the public never engages in mass and ironic morphism I I'm pretty confident in this prediction but it's an empirical one I could be wrong however uh if I'm right that we will see our fol psychological concepts of Consciousness soon extended on mass to AI systems that don't meet the bar the most scientific theories of Consciousness then I think a lot of our existing psychological philosophical and ethical Frameworks will start to crack under the pressure and perhaps aonian paradigm shift in our approaches might be necessary thank you all