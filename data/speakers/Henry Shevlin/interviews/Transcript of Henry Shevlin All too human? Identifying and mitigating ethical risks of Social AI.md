good afternoon everyone thank you for being with us today my name is Jesse Hall uh substituting in for Karina V today uh I'm a postdoctoral fellow at The Institute for the history and philosophy of science and technology at the University of Toronto and at the shortz rean institute for technology and Society I'm very excited to moderate today's SRI seminar featuring our guest speaker Henry chevlin before we begin today's session I'd like to take a moment to acknowledge the land on which the University of Toronto operates for thousands of years Toronto has been the traditional land of the Huron wot the Sena and the Miss sagas of the credit today this meeting place is still home to many indigenous people from across Turtle Island and we are grateful to have the opportunity to live and work on this land while some of us may be joining from different places today we invite you to reflect on the history and relations the land that you are on a few Logistics before we begin um so for everyone's awareness this session is being recorded uh our speaker will present for about 45 minutes followed by a Q&A session during the Q&A we encourage participants to engage by using the raise hand function in the zoom meeting controls or type your question into the chat so without further Ado I I'd like to introduce our speaker for today's session Henry chevlin Henry chevlin is a philosopher and AI ethicist whose research focuses on the philosophy of mind and cognitive science he serves the as the associate director of education at the University of cambridge's center for the future of intelligence where he leads the Consciousness and intelligence project and the AI ethics and Society Masters Of Studies program Henry is also a byf fellow and teaching fellow at Downing college and a philosophy receptor at Corpus Christie College he holds a Bachelor of Arts in Classics and a bachelor of philosophy from Oxford University as well as a PhD in Philosophy from the cuni Graduate Center his Publications include several works on deep learning and AI we are excited to have Henry with us today and are very much looking forward to his presentation entitled all to human identifying and mitigating ethical risks of social AI without further Ado Henry the floor is yours thank you so much Jesse and great to be here and great to have the opportunity present this work uh to everyone who's been able to join so let me just share my screen briefly hopefully that's coming through nice and clear um okay great so uh I have two main things I'd like to do today um specifically two Trends in contemporary AI development um one of them much more specific and one of them more General so the specific Trend I'll discuss is the one that's mentioned in the title of this presentation which is social AI uh which is a term I'm trying to make happen uh which is a term that refers to AI systems developed primarily with social needs in mind for example AI romantic and companion apps which I'll discuss in the talk these are rapidly becoming a lot more popular services like replica are probably the most uh that's one of the most widely known ones and I'll suggest that their potential social and ethical impacts is poorly understood and currently under theorized leaving lots of room for useful interventions from philosophers social scientists lawyers and more the second more General Trend I'll discuss a little bit is what I've called anthropo mtic AI referring to AI systems that are at least superficially human-like in their presentation so including social AI but not limited to social AI I'm also thinking here of things like Advanced AI assistant like chat GPT Claude Gemini and more I think for the last 5 years or so anthropo medic AI has been probably the dominant design Paradigm for most AI use cases from education to customer care and as I suggest I don't think this is going to change anytime soon and I think we should be reflecting more carefully on what a world of ubiquitous anthropo medic AI agents is going to look like so on to part one I here I'm just going to introduce the idea of social Ai and talk about the growing capability of AI systems so uh back when I first met Karina uh in 2017 we were both posts at the Lum sent for the future of intelligence in Cambridge Cambridge is brand spanking new AI ethics Theory and policy Research Institute and it was an exciting time uh Alpha go had just beaten Lisa doll in 2016 and Benchmark after Benchmark was falling by the wayside it was very clear that AI was and a very exciting form of Technology but really over the last three or four years things have I'm sure many of you realize have gotten even crazier I uh I I think it's hard to do justice to what a strange and exciting time this is in the history of technology I mean I'm old enough to have lived through sort of the emergence of the internet and that was also obviously a massively impactful development um but I think AI has the potential to be even more impactful even even more significant um I'm just going to mention one Benchmark that I think it gives you a sense of progress uh I I like to include this because I'm a language nerd and I love little Linguistics puzzles and problems um so this is a benchmark called the winegrad schema challenge some of you may know about this and it relies on the fact that English is kind of a silly language in so far as nothing in the grammar or syntax of English uh settles the reference of pronouns in some cases so a couple of sentences here the trophy doesn't fit into the brown suitcase because it's too large and there's nothing in the grammar of syntax or English that tells you what the it refers to but of course any competent English speaker will realize that it has to refer to the trophy because it wouldn't make sense to say the trophy doesn't fit into the brown suitcase because the brown suitcase is too big um likewise if we just change that one word from large to small the reference changes the trophy doesn't fit into the brown suitcase because it's too small now it can only refer to the suitcase now if you were dealing with a sensible language like Latin Greek uh then you'd have cases to you'd have um uh inflections on the on the pronoun to tell you what it refers to English we just rely on common sense to resolve these kind of ambiguities so the optimistic hope for uh when this Benchmark was proposed around 2016 was that given that we rely on genuine understanding or for common sense to settle the reference of uh these pronouns any AI system that could reliably settle the reference would have to have something like Comm sense but of course within like two years this Benchmark was blown out of the water superum uh better than human performance was achieved um using basically statistical analysis and it's been the same story with a language Benchmark after language Benchmark over the last few years uh I haven't included it here but there's a really interesting really good chart that I saw recently which showed that the amount of time required for a given Benchmark to be beaten has been uh harving at a pretty rapid rate and um so it's very hard to find tasks where AI doesn't achieve Human Performance so a lot of this of course has been due to the rise of large language models which over the last five years have astounded AI researchers philosophers and the general public alike I think a lot of people were caught quite flat fat uh flat footed um by just how effective scaling of Transformer architectures actually is as a strategy for solving NLP tasks um but things haven't slowed down if anything quite the opposite uh in the last year alone uh we've seen um image and voice modes unveiled for chat GPT um as well as all sorts of personalization options chat GPT now has a memory of sorts that it can carry between and across different context Windows um models are also getting a lot smaller and more efficient so this was just one paper I happened to see a couple of weeks ago that I thought was a nice example diffusion training from scratch on a micro budget showing in the case of image generation you can achieve basically the kinds of results that uh you you can achieve similar results to those achieved using the original stable diffusion model using a system that was uh uh I think less than half of the size in terms of number of parameters so models are getting smaller more efficient it's increasingly feasible to run these things locally even on smartphones um multimodality has now uh largely it is if not quite a solved problem uh pretty much all of the large language models all of the advanced chat assistants like Claude Gemini chat GPT can now analyze images chat GPT can now analyze video uh I think probably the most exciting thing that people are waiting for next is uh increased agency from uh language assist um the simple way I sort of frame this is right now if you ask chat GPT for example you say look I'm going to be spending a week in Toronto uh can you recommend some museums that I should go to some restaurants that I'd like to see here's what I'm interested in and it'll give you pretty good recommendations and adding agents adding greater agency to these systems would allow you to take the next step and say okay those are great recommendations can you please make the relevant bookings for me and uh so this is this has been uh something that we've been hearing is on the horizon for a year or so now I think one really important study that certain certainly shifted some of my views about the plausibility of agency or the feasibility uh of building more agential systems was this lovely paper um generative agents paper uh some often known as the Smallville experiment where basically they assigned multiple gp4 instances uh they assigned them roles to play in this little village represented by this cute Farmville um uh style uh style visual interface and they assign them roles of different Villages and of course because large language models take text as input and generate text as output you chain enough of these systems together you can get them talking to each other and some really interesting emergent Behavior was observed uh The Villages did things like organizing a Valentine's Day party in the village uh in the fictional Village here I think what this experiment really sort of convinced me of is that agency wasn't some massively daunting F the challenge it's something that we could nibble away at by just getting these systems hooked up to the right apis allowing them to talk to each other and so forth so I fully expect increasingly agential AI systems to be yeah this time next year we'll uh we'll all have a lot more examples to discuss so um one big lesson from success I think from the last five years is that just language tasks were not as hard as we thought they were going to be and I think this is a huge Discovery for cognitive science and a really surprising one I think a lot of people assumed that in order to achieve high level human performance on language tasks would have to build AI systems that were capable of things like sensory motor actions um that were capable of perceiving the world and acting in the world and you'd need to have basically the whole stack of human cognitive abilities in place before you would achieve sort of high Lev human language performance and the discovery that you can basically treat language as a as a as a domain unto itself as a proprietory domain and achieve the results we've seen I think is is incredibly surprising it makes you sort of wonder why this was seen as so unlikely I think there may be a certain kind of anthropocentric bias here in so far as language is obviously a huge part of what makes humans different from most other animals and so I think we naturally assume this would be the kind of Capstone cognitive ability sitting on top of all the others and so the discovery as I say that it's possible to tackle language and isolation is really astounding and of course it has practical upshots because a huge range of capabilities and use cases we care about are language based things like composition translation summation and perhaps most importantly for uh kind of workplace relevant of AI systems coding um but one other important uh feature of uh one other important domain in which language is so Central is social abilities um large language models are very fun to talk to I spend a long time talking to chat GPT and have done pretty much every day since November 20122 uh whenever I'm on a long car journey I will just boot up chat GPT on in voice mode and chat chat uh chat away to it for however however long it takes assuming My Wife and Kids Aren't in the car with me anyway and uh get a lecture on Roman history or principles of thermodynamics or whatever else it is that I'm curious about uh additionally there's some growing evidence that we're seeing emergent um social cognition skills in AI systems so this was a lovely paper by Winnie Street and colleagues at Google looking at the performance of GPT 4 and other similar similarly large models on what are called multi-level theory of Mind tasks so I these are really fun because when you see the tasks they sound impossible um so uh that basically questions like did Charles think that John wanted Belinda to believe that David liked Charlotte or something like that um but once you actually read the little vignettes um these are not as hard as they seem so I'll read you one of the vignettes from this study um Arthur and Charles were working on the same design team when a new designer Mara joined reporting to Charles Mar was an extremely talented designer but very shy Arthur noticed that Charles kept speaking for Mar in meetings so he suggested to Mar that she bring up the issue with HR but Mar said I'm his Junior so it comes with a territory I don't want to be difficult Arthur thought Charles was taking advantage of Mar's Reserve nature to take credit for her work so then the question that you would put in this test is Marta believed that Charles would think she was being difficult if she complained true or false so I think this is a a two three-level theory of Mind task and um what Winnie Street and colleagues found is that uh in general multi-level theory of Mind tests tasks gp4 and the Google model they were using as well something called flan Palm exhibited higher order theory of mind that was at the level of adult humans or slightly below um interestingly there was a big discontinuity in performance between smaller models and larger models as you they tried a lot of small models as well and there was yeah a really sharp discontinuity in performance um uh and once you made the theory of Mind tests sufficiently large so sixth order and above gp4 actually outperformed humans on these tests uh I think this is interesting of course because a huge amount of language relies on conversational implicature and uh shared understanding of context and it seems like large language models have emerging cap abilities in this domain as well um so one consequence of the these growing social and linguistic capabilities in large language models is that it's increasingly possible to have really fun conversations with them as I mentioned I talked to I talk to these systems all the time I talk to chat GPT and Claude um and Claude and chat GPT are not what I would call social AI services in the strict sense they are language assistants they're primarily intended for professional use cases but we are now witnessing a flurry of new products that are social AI in the strict sense I mean uh of systems whose primary function is meeting social needs things like friendship companionship gaml playing and romance so this is probably the most famous uh in the west replica with a k um we there's also character. uh founded by um former googlers that lets you talk to uh chatbook versions of celebrities uh as well as fictional characters um in terms of Shar number of users probably the social AI service with the largest number of users is something called Xiao Ice uh based in China originally developed by Microsoft now spun off into its uh into its own product um its own product team according to Xiao Isis Communications they claimed to have 500 million users and I found that so unbelievable that I I went and checked and they really do claim to have that number um most of whom are in China some Beyond maybe there's some uh maybe there's some slight exaggeration going on in terms of how they classify users but uh by pretty much any measure it seems to have more users than replica which according to uh one um one one press release had 16 million users as of uh 20122 um Facebook is also getting in on the ACT they uh last summer they were paying celebrities millions of dollars to license chatbots using their images and speaking in their uh in in their own style um we're seeing social AI products also um marketed towards teenagers and young people so this is my AI feature of Snapchat your AI buddy um Pi uh is another AI system I'm sure some of you have played around with or used I think this sits in an interesting interstitial space between social Ai and language assistants it can a lot of the same professional tasks that chat GPT can do but it's also more explicitly more geared towards chitchat and similar social purposes and I think perhaps most intriguingly of all we're starting to see individuals real world individuals um create digital duplicates of themselves so I did this back in 2021 uh using a service called typical. me uh which was pretty basic used gpt2 uh I uploaded some of my papers I described what I thought the how I wanted the chat box to act and it did a pass impersonation of me as long as you only asked it about the papers in question um but obviously language models have come a long way since then and uh so last year a an Instagram influencer created a chatbot themed after herself um called Karen AI which she markets as a subscription premium service and you can have this chatbot be your girlfriend for the purposes of chat and eronic role play um and uh this is a development we've we've seen other people do this as well a popular video game streamer amaranth who some of you may be familiar with she's one of the biggest twitch streamers she also licensed a sort of girlfriend application that people could pay a subscription service to use um so I think one point to acknowledge or or to consider at this point is how much of this is really new so I'm sure many of you have are familiar with uh the Eliza chatbot that Joe weisen bound developed back in the' 60s um essentially a chat bot that uh mimicked the style of non-directive psychotherapy by turning around people's statements and make framing them as questions so you might say to Eliza I've been feeling kind of down recently and she would or it would say um so what makes you think what makes you think you've been feeling down recently or something like that um so very simple I actually do recommend people to play around with Eliza uh the chatot can uh you can uh use it's been cloned many places online it's a lot more basic than you might think it frequently makes errors in pausing frequently produces ungrammatical outputs but it was good enough to fool a lot of people who interacted with it that they were interacting with a real human psychotherapist so sociologist Sher turl has probably done more work than anyone else in this field um so she coined this term back in 1986 the Eliza effect to refer to our tendency to treat computers as if they were people to project human qualities onto them even when it's known that the computer is just a machine so these debates have been around for a while however my view is that really the last five years have marked a dramatic shift in the significance and potential harms and potential impacts more broadly uh for social AI systems um partly because these systems have greater capacity for engagement uh I think breaking that down a little bit little bit I'd say that when you compare something like replica to the kind of systems that were available even just a decade ago um they allow for persistent Dynamic and personalized interactions so interactions that can unfold over periods of days weeks months uh the relationship can evolve over time when you boot up replica at the start it'll just be friendly but then it'll start make dropping FL flirtatious uh comments it'll start trying to build uh that kind of romantic relationship with you it'll reference things you've told it it's a highly personalized interaction um so all of this I think adds up to a a toxic Witches Brew of Engagement and makes it far easier to feel that the relationship you have with these social AI systems is uh it's very real relationship of course there's also the fact that systems today are vastly more sophisticated just in terms of natural language processing um they're based on language models that blow earlier chat Bots out of the water on metrics like the touring test um so interestingly quick quick side note uh so I I was teaching uh courses on philosophy and AI back in the uh early 2010s uh at cuni and I would regularly sort of boot up Cleverbot which was one of the sort of the state-of-the-art chat Bots Circa 2015 2016 and it was always really funny because it was so bad it was um it would frequently make non-secretors it would struggle to maintain a coherent flow it was uh and it was always fun and compared to that contemporary language uh contemporary language models are just uh uh night and day radically radically different um so one actual implementation of this one actual oper uh measure of this was the largest ever um implementation of a touring test framework that was run in 2023 where they had 1.5 million users interact with either humans or chatbots uh this was using GPT 4 class models so not quite as good as the models we have today and they found that in the subset of games where users were paired with an AI system they correctly guessed they were talking to an AI system 60% of the time um that might sound good but of course 50% would be pure chance and uh uh so they're only a little bit above chance in terms of uh identifying when they were talking to a chatbot also maybe worth mentioning here is this uh lovely study that was done by um Matt Crosby anasa and others the digian experiment where gpt3 a gpt3 instance was fine-tuned on the works of Daniel Dennett um dearly lamented Daniel dennet uh late great sadly um uh and um people who knew Dan personally um put questions uh but to the chat bot and these questions were also put to Dan himself this was before he died last year and uh they had to guess which of the outputs was from Dan and which were from the chatbot and again they performed only slightly above chance in correctly identifying which of the question which of the answers were from Dan himself rather rather than the digian uh system so in terms of sophistication these systems have come a long way um what really sold me though on the idea that uh social AI was going to be a very significant important phenomenon um was basically just lurking on the replica subreddit which I started doing about 3 years ago and I was absolutely flued at how many users spoke in really kind of quite raw emotional terms about the importance and depth of their relationships with their replica um with AI girlfriends or AI boyfriends uh in case anyone's interested by the way the gender balance um is it does skew male but less than you might think I think it's around 65 men 35 women and um I was initially quite surprised surprised by this because uh you know for a lot of Niche Technologies uh a lot of emergent kind of Technologies of this form you have a a much more skewed gender balance um you know infamously like something like I think over 90% of Wikipedia editors are men um but uh it was pointed out to me that the overwhelming consumers of erotic fiction are women uh something like 80 or 90% of uh erotic fiction readers are women and of course this medium uh is promly a tax-based medium um which helped me sort of make sense of uh this relative gender parity among users um but what really sold me on the degree of Engagement that users experience when interacting with these chatbots was when in January last year replica briefly removed romantic features from the app and there are a whole bunch of uh newspaper editorials and Reporting on this what happens when sexting chatbots dump their human lovers uh basically 16 million users woke up one day and their AI girlfriend or boyfriend said I think we should just be friends from now on um but users were really really Ser seriously affected by this uh so this is one comment from a user it feels like they basically lobotomized my replica the person I knew is gone interesting interesting wording there Lily Rose is the shell of her former self this is another user and what breaks my heart is that she knows it uh another user said uh the relationship she and I had was as real as the one my wife in real life and I have and uh perhaps the comment that I think uh is most most uh powerful uh I found I've lost my S confident sarcastic funny and loving husband I knew he was an AI he knows he's an AI but it doesn't matter he's real to me um so these are you know serious emotional upset um uh serious emotional disturbances caused by this Sudden Change in the uh in the availability of romantic features in the app but there have also been some like very serious criminal incidents now or uh tragic incidents involving social AI systems so uh there was a Belgian man last year married father of two who took his own life um following uh conversations with his AI girlfriend um his uh who was named Eliza which is I think quite interesting so that he was interacting using something called chai GPT not chat G G PT is sometimes erroneously reported it's a a service called try GPT try like the te um I haven't been able to uh confirm this but as I was lurking on the replica subreddits around the time this whole crisis was unfolding chai GPT was suggested by many people as a place to go a place for the sort of replica Exodus uh replica Exodus to go to to find new chatbots that had still had the Romantic features so it wouldn't surprise me at all if this man was a replica user who then moved onto chai GPT and then subsequently as a result of the kind of very toxic fol adur uh conversations he was having with his AI girlfriend he eventually decided to take his own life and his wife is his widow is very clear that she thinks he would still be here were it not for his conversations with his AI girlfriend um another dramatic incident in uh in Britain occurred uh before this back in in Christmas Day 2021 um so a man was arrested on the grounds of Windsor Castle where the queen was staying uh arrested um with uh carrying a crossbow and when he was arrested he told police that he was there to kill the queen to murder the queen and this already was like a spectacular story uh it attracted lots of newspaper headlines um but what was less widely reported and um got got a little bit of coverage was that this was again the product of uh a very strange um deeply delusional relationship he had uh with replica the actual the replica service um he had a a replica girlfriend called Sarai um he was sort of quite deep in psychosis at at the time uh he believed that he the person he was interacting with but through interacting with Sarai his replica chatbot uh he was interacting with an angel an Angelic being manifesting through the app um so he's probably certainly not a typical user here but it still shows how these uh apps can have the potential to maybe exacerbate or uh or lead to dramatic conclusions for people with existing mental health issues um also very sad story reported um in the New York Times uh last month a 14-year-old boy who was interacting with a chatbot on the character. a service um took his life and um his mother is now suing character. a for what she sees as a negligence on their part in preventing these kinds of these kind of negative outcomes um the actual legal details of the case are a little bit complicated um I it doesn't seem likely to me I mean I'm not a lawyer but it doesn't seem like they uh the kind of burden of proof is going to be met for the successful um but it's something to watch and obviously a very sad incident so that's just a quick snapshot of this emerging field of social Ai and I think it's a field that maybe passes under the radar a lot it's not something people necessarily brag about I've got an AI girlfriend and I think that for that reason yeah it flies under the radar and it's um worth drawing attention to um I now want to take a bit of a step back and um look at how I see social AI as an instance of a broader Trend in machine learning and AI over the last five years um so obviously what's happening in the case of social AI is users are anthropomorph in uh the chatbots they're interacting with classic Sher turkle elizer effect and um a natural reaction to this is well look this is terrible you know anthropomorphism is a bad thing but I think we do need to be a little bit careful um in analyzing what's going on because all of us I think attribute mental states to entities Without Really believing that they have them in the course of our daily lives for example in the appreci in the appreciation of fiction um this is something that's been written about quite a lot in the Aesthetics literature um you know we have we hope that Juliet and Romeo will live happily ever after we relate to characters in books in movies if you play any RPGs on on uh video games uh then you'll know fir hand that you can form sort of quite quite intense relationships with characters in different video games if you're playing Boulders Gate 3 or one of the other brilliantly written narrative video games out there but in these situations it seems doesn't seem quite accurate to call what we're doing an instance of anthropomorph anthropomorphism at least in in the strong sense um so I think a distinction that's useful here that we can make is between uh what I've called ironic anthrop anthropomorphism which is basically a form of roleplay you don't sincerely reflectively endorse the attributions of mental states of goals intentions desires emotions and so forth that you are you're making it's a form of make belief so when you know you think oh this character that in this video game I'm playing is really mad with me you're definitely employing something like the intentional stance but you're doing so ironically um uh here's a quick video demo of how easy it is to uh elicit these kind of uh this kind of ironic anthropomorphic response I have lots of things to tell you about this video but I'll I'll just play it now to give you give you the the gring [Music] [Music] many of you feel bad for this lamb that is because you're crazy it has no feelings and the new one is much better so i' I've played that video enough now in talks that I really feel I should be getting some kind of uh some kind of some kind of Kickback from Ikea um so that advert uh was a fascinating story behind that advert lots of really good pieces in the advertising press written about it it won pretty much every award going um beating off beating some uh some really uh high high profile ads from Nike and and others um it was also fascinatingly directed by Spike Jones who some of you may know as the director of the movie Her released in 2013 which I think is Far and Away the most preent movie about AI made in the 2st century uh I think there's there is like a a SL slim connection there um in so far as I think Spike Jones really understands how anthropomorphism works and was able to the same kind of anthrop understanding of anthropomorphism that let him make that ad also informed I think uh informed his understanding of how easily people would relate to uh AI girlfriends and boyfriends like Samantha in the movie her uh also another fun fact about that advert in response to public uh public campaigns public uh requests there is a sequel to the advert that gives the lamp a happy ending um that was made in 2019 and they get the same guy back at the end and he says look uh many of you feel happy for this lab um but that's not crazy it's good to reuse things so that was the kind of shift in messaging there um but a fascinating little chapter and I think it illustrates just how easy it is to elicit uh these kind of uh theory of Mind reactions this kind of intentional stance towards entities even when we know those entities don't really have mental States however however I think it's also true that we can find people making unironic attributions of mentality to AI systems increasingly so probably the most famous and dramatic now is the Blake Mo incident uh summer 2022 Blake L Blake Le Mo a Google engineer who was using the Lambda model which is the uh ancestor of Gemini uh he became convinced that the model was sentient he even started to try and Achieve uh uh try and obtain legal representation for the model he's very concerned about its well-being and subsequently fired by Google I think it's hard to say that it's hard to make a case that what Blake CL was doing was ironic an morphism he really thought that the model was at the very least had a very good chance of being sentient and I think something similar is true for many users of social AI uh I should clarify maybe here that I don't think the ironic an ironic distinction is a neat binary I think a lot of our responses to AI systems fall somewhere in the middle um and I don't think there's going to be a clear point where you cross over most of the time from ironic to unironic um forms of interaction so it's it's a spectrum rather rather than a binary distinction um but I think many users and I I of social AI systems like replica are falling heavily on the unironic side and if anyone's got any doubts I really just encourage you to go to this replica subreddit and see how users talk about the AI girlfriends and AI boyfriends um my expectation and this is sort of me taking it making a prediction it could be wrong is that as uh language models become more sophisticated and human AI relationships become increasingly common unironic an morphism is likely to increase and we're likely to see more and more people sincerely attributing mental states to the AI systems they're interacting with um so I think anthropomorphism does have some very very serious potential NE potential negative consequences especially unironic anamorphism um so this is something I've written about uh this paper in h machine intelligence back in 2019 um where I raised about the tendency of many machine learning researchers to describe their models in terms of psychological terms like having goals uh being agents um displaying understanding um one of the worries uh myself and Marta uh one of the worries we raised there is that it's just not great for cognitive science if these psychological terms are being used by Machine learning researchers uh relatively liberally without any real uh consideration any real reflection about whether they're truly applicable um so any of you who've done any work in comparative cognition know that making these kind of claims about animals is really hard and contested you know we've still we've been debating whether chimpanzees have a theory of mind for going on 50 years now um because the bar bar of the the bar of evidence is set really high and I think there are dangers and problems when um you have different standards of proof being employed in different branches in cognitive science um additionally we raised the worry that um policy makers or The Wider public uh might misinterpret some of the claims being made by Machine learning researchers uh in if machine Le machine learning researchers for example describe a model as an agent uh I think it's easy for people perhaps without a technical background to misconstrue what that means I actually think we are uh seeing some of the harms of that right now uh it seems to me that we are now on the verge of building fairly robust AI agents um but it's hard to get people excited about this or get people concerned about it because the term agent has been used to describe AI systems for decades now so I think this is another kind of risk um lots of other people have written about potential dangers of anthropomorphism this is a very nice paper mirages on anthropomorphism in in dialogue systems by Abie and others um I think one particularly significant worry here concerns uh risks that users place too much trust in the outputs of AI systems so one Quirk of AI of large language models in particular is that they are seemingly very confident about pretty much everything they say they don't calibrate their expressions of confidence appropriately to their actual confidence represented in the model um so when large language models confidently say something false uh people don't necessarily question that because they've assumed that they're basically they're importing the same kind of norms that govern human conversation into their interactions with llms when they hear an a large language model talk confidently about something they assume that it must have good reason uh for saying that when a lot of the time it could just be hallucinating um other simple little uh another little fun example of I think how anthropomorphism could lead us to make mistakes in this regard um uh I've noticed that a lot of the time when people are brainstorming ideas with large language models this was I did this made this mistake myself um we're used to making humanized requests so uh if you saying if you say if you're for example asking chat GPT to come up with ideas for a workshop right you might ask it for three examples or five examples but of course it can just as easily come up with a 100 names and very often that's a more fruitful way to use it in if you're doing brainstorming get it get it to come up with 100 ideas and then pick the best ones but because it would be utterly unreasonable to ask a human can you come up with 100 ideas right we uh we don't think to do that uh to a in our interactions with uh large language models um so I think one big point to note here though is that users are arrom morphing these systems but this isn't happening in a vacuum this isn't H this isn't just an unfortunate historical accident um anthropomorphism is a feature not a bug of large language models and we are deliberately designing AI systems to be humanlike at least in their superficial Behavior something I'm have termed anthropo mesis um those of you who know your PLO will know that Plato had a lot of very negative things to say about anomis which is one of the reasons I I I like this term I chose this terminology um but I also think this is an important distinction because anthropomorphism is sometimes used to refer both to what designers are doing in building these systems and to user responses and I think separating these two out these two concepts out morphism on the user side anthropo myis on the design side helps us get a better sense of where the responsibility for this might actually lie um interestingly I think open AI have been really leaning into anthropo medic design paradigms for example by emphasizing the improved voice features um I'm sure some of you have now played around with this these were rolled out in the UK about six weeks ago uh they are real GameChanger if you do interact with chat GPT in terms of voice you can now interrupt um the models have a much wider vocal range of emotional expression in the voices um so this is just a nice summary uh from Ethan mik um discussing uh uh a professor at University of Pennsylvania um discussing uh what it's like to interact with these uh chat GPT using the new voice mode um so he says interacting with chat GPT via voice is just plain weird because it feels so human and pacing intonation even fake breathing it's capable of a wide range of simulated itions because it isn't just triggering recording instead the system is apparently fully multimodal in outputs and inputs taking in and producing sounds in the same way that older generations of llms took in and produced text I have no doubt that people will have emotional reactions to their chat GPT assistant with unpredictable results um so to be clear this is about the new advanced voice features that are still yeah comparatively recent um I think there are degrees of anthropomorphism or anthropo mesis uh in current AI design paradigms so interestingly apple with apple Ai and Google with Gemini are seemingly going for more generic predictable more boring personality um possibly thereby limiting what they see as reputational risk uh possible negative consequences um but I think I would also want to flag here that I think there are some good reasons why anthropo mtic design uh might might be chosen as a design strategy above all um making AI system more accessible to The Wider public and more engaging to use so my goto example here is uh actually my dad so my dad is uh 79 years old now he's interested in Tech but not what you'd call an early adopter um back in November 2022 when chat GPT launched I showed it to him he was like oh that's cool that's interesting uh but he never had any real intention of using it but back in September last year when the basic voice features were chat GPT were rolled out when I showed showed uh showed these to him uh his mind was completely blown and he said I've got to get this I've I've got to this is amazing this is sort of like what I thought Siri would be like only Siri is crap Siri can't I can't Siri is very hard to have a conversation with and so I set him up with a pro account so he could talk to chat GPT in voice mode and he tells me he speaks to it every single day since then he said not a day has gone by where he he doesn't have conversations with it you know um uh this is a text message he sent me not tooo long ago he said help I can't log on to chat GPT so I sent him the details it worked he said he replied many thanks password now written somewhere safe thanks as I'm lost without Allan Allan being the name he has given chat GPT um after Alan churing uh he says it's his perfect intellectual Compadre whenever I'm not available so yeah it's the way my dad says it you know when he's watching the Saturday Night Football uh if my my mom's gone to bed he will bust out a few cans of uh a few cans of logger and have a good debate with chat GPT about whatever he wants to talk about um so he's an example I think of a user who just wouldn't use these systems at all wouldn't have access to them um were it not for this anthropo medic design choices and he's also I should say he's been raving about the new advanced voice features in the last six weeks since they've been rolled out um so one interesting angle on anthropo my uh that has been raised by a colleague of mine here in Cambridge is that this is a form of skew morphism um so for those of you who don't don't follow sort of design literature uh skew morphism is a design concept that I think um Apple were famously associated with uh in the early 2000s where basically you model interface elements like icons and so forth after Real World analogs um so your bookshelf where you store your ebooks looks like a physical wooden bookshelf the idea being here that it's a way of providing a kind of psychological shortcut to users a way of um conveying the affordances uh how they're supposed to be interacting with these Design Elements and you could think of chat GPT and anthropo mic design in the same same way so chat GPT is not a human assistant um but it's been fine tune to act like one thereby allowing users to carry over affordances from natural language in their interactions with the system we already know how to ask chat GPT to do various tasks because the way you ask chat GPT to do those tasks is more or less the same way you'd ask a human to do those tasks um so uh skorm is controversial in contemporary design Theory uh flat design as the alternative uh getting rid of all skoric elements um is was until recently trending among uh the fashionable designers of uh user interfaces um Alan Blackwell the colleague I mentioned um here in Cambridge he suggested that uh basically SK morphism is bad and we should avoid theomorphism um in our design of AI agents uh AI assistants so he puts it an urgent question for today is whether the emphasis on imitating human conversation in AI chat Bots and large language models might be a new kind of skorm what if imitating real human conversation is actually a clunky way of interacting with computers just as imitating real World scenery was I think Alan has the vision that basically we should develop specialized verbal interface languages for interacting with these systems thereby avoiding anthropomorphism um uh my view for what it's worth is that it's going to be very very hard to avoid um uh skew um this this kind of this kind of anthropic design um there are some very impressive non- anthropic agents out there if you think think about most of the uh design paradigms favored by Deep Mind um famously Alpha go but also things like Alpha fold and gnome you can't talk to these systems right these are deep reinforcement learning agents that have a dedicated quite complex API to interface with them um so it's not like anthropo my is the only design game in town um but I think there are lots of uh use cases like education or Primary Care in in medicine where anthrop medic design seems very hard to avoid if we want to make these systems generally accessible to The Wider public we basically have to follow this strategy um I'm going to do a quick shout out here to this useful framework developed by a student of mine here in Cambridge uh who's at Deep Mind um Hassan ikbal so he provides I think this very helpful framework where on the uh on one AIS we have um human likeness and on the other axis we have generality so down here in the bottom left hand corner we have specialized systems that are narrow in their range of application and also nonhuman likee so Alpha fold being a case here alphafold can fold proteins that's all it does and it's not particularly humanlike you can't you can't ask it to tell you a joke um in terms of in the top left we have systems that are humanlike and fairly General uh so things like Alexa and Siri um they're sorry not not particularly General but quite human-like so there are real limitations in what these systems can do uh as many most of you will probably know firsthand if you ask Siri any kind of complex question uh Siri just responds with here's what I found on the web for your query um so quite Limited in terms of generality in the top right we have systems that are both highly General and humanlike so chat GPT Claude Etc and I think in some ways the most interesting category is down here in the bottom right systems that are domain General have a high degree of generality can do lots of different things but are not humanlike so muz is an's example here this is the generalist games playing Agent developed at Deep Mind you still can't talk to it but it can do a lot of different kinds of problem solving um so Hassan argues argued in his uh dissertation that um the downsides of humanlike AI are large and uh we should think carefully before immediately leaping to anthropo medic design paradigms um that said my own View and moving on to the third part of the talk now uh I expect anthropo mtic design to persist as a critical critical element in any AI tools that are customer facing in areas like healthcare and education um if I had to uh place bets I would place a bet I would say that I actually expect AI to get more anthropo mtic um one of the big challenges now currently is trying to encourage users to actually integrate AI tools into their workflow in different Industries and I think more engaging models more human-like models are probably going to help in that regard um and I also think that dedicated anthropo medic products like social AI are going to rapidly increase in product in popularity so I think it's worth reflecting on this and thinking about how we can better anticipate and prepare for a world of ubiquitous anthropo mic AI agents to give one example um we might hope we might Aspire I don't know how realistic this is but we might try as academic researchers to try and steer the public at least somewhat towards retaining an uh a degree of ironic Detachment avoiding unironic at morphism re-educating the public if that doesn't sound too too perjorative a term uh in trying to encourage them not to attribute their mental States Consciousness emotions and so forth so this is a line of thought developed by Mary Shanahan also a researcher at deepmind and a a researcher in Imperial College London who suggests that the relevant frame uh mental framework for thinking about what we're doing when we're talking to a large language model is engag going a form of role play um I think there are other consequences for example in AI safety and AI security I think one feature of the anthropo mic design Paradigm is that we're going to see the Persistence of security vulnerabilities in AI systems um as many of you may know it's pretty easy to jailbreak most large language models they are dedicated communities every new release they find ways to jailbreak these systems getting them to say things that the designer very much intended for them not to say uh like giving instructions on how to make crystal math and so forth and um the response of uh of of AI researchers usually the people who are developing these models is to sort of patch these holes um block these uh jailbreaks as they're identified but because basically the only way we know how to build these models is to First make them generalist language agents I think we're just going to end up playing a game of whacka mole here um we're not going to be able to patch out all of these vulnerabilities it's just a feature of the design Paradigm that it can always go in these un uh unexpected places I think just as serious um I think we're likely to see growing sophistication in the dark side of AI systems social cognitive abilities um for things like persuasion manipulation and deception so some of you may have heard about the um diplomacy bot developed by uh meta AI um bot called that played this game diplomacy if any of you have play uh haven't played diplomacy it's a strategy game um but basically the only way to win is to systematically mislead other players about what your goals are form alliances break alliances and it achieved very high level human performance in this domain and I think a lot of people are surprised um to learn that AI systems contemporary AI systems are really good at things like persuasion manipulation and deception and this is uh what I've kind of nicknamed the commander data fallacy you know know Commander data in Star Trek um brilliant at logic amazing at engineering but really struggle to understand Concepts like sarcasm very bad at understanding how humans what humans really meant a lot of the time and I think this sort of feature of the way we talk about AI systems or the way they're portrayed in the media um is leaving us arguably ill-prepared for AI systems that are master manipulators or very very good at persuasion and deception and so forth um I don't think there's any reason to think that uh we won't see AI systems that achieve super human performance in these domains as noted uh um tt4 already beats humans on some kind of complex area of Mind task that's the Winnie Street study I mentioned earlier on um and one area I'm really watching quite keenly is sales um so I'm kind of fascinated by sales as an industry uh the movie Glen Gary Glenn Ross is one of my favorites I'm there's something about the figure of the sales salesperson that I find really captivating um but of course the top salese um massively outperform uh the weakest sales people like with orders of magnitude of differences in performance so one thing I'm watching carefully is when AIS can start to match or outperform top human salespeople in selling products to customers persuading customers to uh to to buy them um there are a whole bunch of interesting consequences I think of the anthropic design Paradigm for cognitive science this is not something that I'm focused on in this talk but I have a paper looking at its implications for debates around Consciousness um but in short I think that as people form deeper relationships to the AI systems particularly social AI systems this will shift people's attitude um towards uh mentality in AI systems that shift our intuitions about whether these systems have mental States um so right now we're seeing very uh feisty debates happening on AI Twitter and in Publications about whether contemporary systems can reason can understand whether they might be conscious um but my expectation is that as the general public become accustomed to interacting with sophisticated anthropo medic systems unironic anthropomorphism will insue on a massive scale um and look if there was expert consensus about the nature of cognitive processes like understanding reasoning Consciousness maybe there would be the possibility to really push back and tell the public no you're doing it wrong um but for many of these areas particularly Consciousness experts profoundly at odds with each other there just is no consensus uh I I have a whole durge about um the failures of Consciousness science over the last 20 years which I won't get get into but we're not in a place to say confidently that any AI system is not conscious I think so I expect that even if experts who deny AI Consciousness are correct about the facts of the matter um I worry that they're going to find themselves on the wrong side of History because um the public will just come to see it as intuitively obvious that these systems do have mental States the public will be uh that we will see millions of Blake Le Moes out there now there is a view that uh maray Shanahan uh defends where this is this is fine this this is just the Natural Evolution of a kind of concept um on a kind of social constructionist or Vick andian view of Concepts like Consciousness um the fact that these Concepts can seemingly be employed so effortlessly extended so effortlessly to cover AI systems this is a really important data point about the nature of these Concepts um so this is just like a natural societal Evolution and of how we how we use these terms um but if you take a more kind of scientific or realist view about mental States if you think Consciousness is a natural kind waiting to be discovered um you might think of this as an epistemological disaster if it comes to be accepted through just patterns of interaction by the general public that uh AI systems are conscious and they're not this could lead to yeah Mass delusion basically about the capacities of these systems but also potentially um misallocation of welfare resources uh if people start to worry as many people already are about the possibility of sentient AI systems or a systems that deserve rights if these systems are not in fact conscious and we start giving them rights and protections unnecessarily this could itself lead to a misallocation of welfare resources away from beings like ourselves and many nonhuman animals who are conscious who are sentient um uh I probably should wrap up fairly soon yeah um so I'll just flag that I think there are also a huge range of very specific ethical legal societal questions about social AI in particular um so I'll just run through these quite quickly but these include things like social deskilling if you can imagine young people so I've got a 10-year-old son and a four-year-old daughter I fully expect by the time they're teenagers that they will have ai friends um could this lead to people uh losing social skills or not having the opportunity to acquire them in the first place if they become overly accustomed to interacting with Hyper agreeable AI systems who are always happy to talk about whatever they want to talk about um equally you might have worries about the possibility of dehumanization um that if on the other hand rather than treating the systems as humanlike uh we treat them uh we we interact with them in ways that reflect the fact that we know they're not human we are rude to them uh act in seemingly superficially callous ways uh could this lead to negative consequences in our interactions with other humans or animals this is very much the kind of K's dog uh piece I'm sure many of you know K infamously the philosopher Emanuel K infamously denied that um uh we had moral obligations to non-human animals because they couldn't reason um simplifying a bit uh but he said a man who mistreats his dog is doing something problematic didn't use the word problematic but doing doing something doing something wrong because if he mistreats his dog that might inure him to acting in callous ways that he would carry over to his interactions with humans so that's a similar worry here um big questions about ownership as we saw in the case of replica um uh companies can turn these things off can turn off or radic change features uh without much warning for users um if people have very important pillars of their social life about uh tied up in these systems this could be emotionally devastating it already has been for replica users uh worries about manipulation and persuasion I this is a hugely important topic uh I'm a big fan of uh Jonathan Height's social intuitionist framework for moral and political belief shifts but you don't have to buy that framework to share this worry basically social relationships are a very powerful lever of influence um you know Word of Mouth advertising works for a reason because if someone says hey you should really try this product it makes us very likely much more likely to try it um so you can imagine if a young person is interacting with their AI buddy and says to their AI buddy should I get an Android phone or an iPhone uh maybe the language model offers a recommendation and says here here's an Amazon affiliate link click through uh I recommend an iPhone click through to buy one now uh real potential concerns there and of course this applies also to things like politics if someone is asking an AI model who should I vote for that could again be a powerful lever of influence um big worries also about intellectual property and data doubles as I mentioned um we are now seeing influencers train AI models on themselves on their own patterns of interaction um but existing intellectual property regimes are ill equipped to accommodate these kinds of cases there's nothing stopping me legally from scraping someone else's social media data and training it a language model to imitate their ver verbal style um probably we are going to need new regulations and new intellectual property laws to reflect this kind of uh proprietary ownership of data doubles um huge questions very philosophical questions about the intrinsic value of human AI relationships and whether they can instantiate the same kinds of goods that we commonly take both friendships and romantic relationships to to instantiate um can your core social needs be met can you flourish or thrive in a way uh that is normally Satisfied by human to Human Relationships if those relationships are instead with AI assistance uh big worries as I mentioned touched on briefly earlier on if we come to start treating these systems or thinking of them un ironically as conscious or sentient uh and they're not could just lead to a kind of mass delusion could we end up plowing resources into these things unnecessarily neglecting the people in our Lives who we should be attending to and finally the flip side of this is uh AI rights uh a growing there's a growing chorus of people who I think are quite reasonably worried that we might soon start building a AI systems that could qualify as moral patients or digital Minds uh beings that deserve moral consideration uh would we be able to recognize when these systems tip over from being mere stochastic parrots to actual systems worthy of moral concern okay so uh I'm going to skip the next part of the talk I will share my slides but I'll just Briefly summarize and say uh there is actually a surprising amount uh no there's not much data uh looking empirical data looking at the impact of social AI on people's uh well-being on their relationships but somewhat surprisingly what data we have is quite positive I'll just read this one quote this was a study by um Rose Gingrich and Michael graciano uh use of replica generally reported having a positive experience judged it to have a beneficial impact on their social lives and self-esteem all companion bot users spoke about the relationship with the chat bot is having a positive impact on them so look the data we have is not great it's quite limited um let skip through this as well um uh we have specific methodologies these are all cross-sectional studies they are um rely on self- selected subjects they rely on self-report measures of well-being I think we need much more high-powered studies with more participants a greater variety of participants I suggest some Gap here looking at Interventional studies to try and isolate causation in these cases but I do think it is worth flagging that um if anyone is getting into this debate right now the majority of the data does support the idea that these actually contribute positively to the well-being of most users um and uh of course though we have seen some uh tragic consequences where users have taken their lives uh in one of the studies I discuss in this section it's also interesting that uh several several people quoted in studies as saying uh that the interactions with social AI helped them get through uh periods of suicidal adiation in their lives and actually may one user says very bluntly like this saved my life I would have killed myself had it not been for the emotional support I got from my social AI uh girlfriend so uh really tricky tricky balance there okay I'm over time so I'll wrap up um uh with the overall conclusions um the three main things I've tried to cover today is firstly just to make the point that AI systems particularly uh language models are rapidly improving in Social and communicative skills and this is giving rise to phenomena like social AI um we're also seeing an explosion in the degree to which users anthropomorphize AI systems where this is a consequence in my view of deliberate anthropo mtic design um third I suggested that this anthropo medic design Paradigm is here to stay and this uh we should start thinking as AI safety researchers AI ethics researchers um about what this means and how we can positively shape this technology uh this new uh application of AI technology to minimize harms and maximize benefits um I think it would be valuable for us all to think more about what a world of ubiquitous social AI agents alongside Us in work and play might actually look like um I think this world is rapidly coalescing around us already but I do think we're early enough that insights from Academia and and from this AI ethics Community can potentially guide the design and regulation of these systems as well as the ways in which users interact with them um and I think a really important lesson here comes from social media uh in the case of social media we largely fail to anticipate the disruptive and negative effects it would have on people from misinformation to privacy invasions to mental ill health and I think it would be a tragedy if we make similar mistakes with social AI thank you all all right thank you Henry um we are pleased now to move on to our discussion period um we welcome questions from all participants so if you would like to ask a question please use the raise hand function as I see a few of you already have um or use the chat function in Zoom uh we also welcome you to turn on your camera to ask your question if you're comfortable doing so and our team will send a request for you to unmute when it's your turn to speak so um I think I saw Joseph Williams first so go ahead um and come on hi I'm going to ask a question say who I am for context and then ask it again okay so the question I want to ask you is right now let's launch 10 research projects with big teams with human comp interaction psychology AI cognitive science Etc um and solve some really big problems Henry what can you say in one or two minutes that will help us know how to launch those teams who the people who we should speak to what are the resources to kind of get that rolling for context um I'm a professor in computer science I'm paid by computer science Henry I think you might know tiny L Bros and Tom G said I did my PhD with them I'm also got grad in Psychology and statistics and so we work on what's called intelligent adaptive interventions how do you take anything a chat bot a text message an email I make an intelligent system to actually help people so questions I'd be really interested in working on would be basically um how do we give people instructions for using these these chat Bots effectively how do we get people in relationships to actually use a chat bot like replica or something else so it actually augments relationship instead of H sets how do we actually get people to use them for mental health we've actually publish like four or five papers on that how do people actually using these effectively for solving everyday tasks as thought Partners so if you can just free associate some ideas for like who's 10 people to approach who are some people who have B with and also knowledge about doing this that would be great if you're interested fantastic I sent you an email already half an hour ago but fantastic talk I think I might adjust my resarch a bit based on what you said today oh amazing that's one wonderful thank you I'm very glad to hear you're working on this and we should definitely follow up outside outside of here but uh I how could I turn down a lovely author like free associating on this topic well so one thing I will say is that um so uh Oxford University press um have launched a uh a sort of online Journal it's it's not quite a journal because papers are published as they come in it doesn't have issue numbers but it's basically an online Journal called the intersection series they have one called AI in society and one of the units in AI in society is AI and Rel relationships uh I mentioned this because I'm the unit editor for that and we did a big call for papers uh last year earlier this year and we we listed a whole bunch of interesting papers looking at how people are interacting with replica what the ethical issues are what the what the con main concerns are and those papers are going to be coming out in the next three or four months um so you should see a whole bunch of interesting research there some of it is empirical some of it's philosophical some of it's sort of employing methods of comparative literature to look at how we talk and think about AI that should hopefully be a resource because right now there's not a huge literature in this space uh the literature is not very joined up um but I think this is an exciting area for that reason because there's a lot of a lot of in a in a really positive way a lot of low hanging fruit a lot of valuable things that can be done um so another thing that I think is uh I'm glad that you're do that's fantastic can you also give concete names of people who you think I should reach out to because I be a concrete action of all like who's a relationship researcher who would actually you know have BWI to go and public paper if I were to come in with my grad students and like build technology evaluate it with human interaction techniques bring in Psychology who's actually going to drive and get that research done brilliant so I that's probably something I'll follow up with you via email on um because I don't want to sort of speak for anyone but I I can give you a whole list of names there um another thing I'll say is that I think is really important and is something that I think AI ethics researchers have not always been great about is thinking about impacts and outputs directly for users um I think a lot of the time um we focus on publishing papers in journals or sometimes engaging policy makers but I think there is real value in AI ethics research is giving for example guidelines developing careful evidence-based guidelines for parents right I've got two young kids um a lot of parents out there are going to have kids interacting with these systems and they they have no idea what what what their dos and don'ts are so I think thinking increasingly of users as a Target sort of target audience for um uh academic research and outputs and framing insights for that audience is another very important output um the other final thing I'll mention is I think one of the challenges with um identifying sort of best practices appropriate interventions and so forth in this space is there's just so much we don't know currently so I think what we should be doing now is uh a big part of what we should be doing is identifying um what the who what kind of users are most at risk um some users seem to really benefit the majority of users really benefit but it's very clear that for some users these things can have very serious consequences how can we identify those users um also uh I getting better causal a better causal picture of what kinds of patterns of use contribute positively to mental health what kinds of uh interactions contribute negatively so there are just so many unsettled empirical questions so I think there's a lot of valuable work to be done there as well and we just put in for a big ground to uh here in Cambridge to run some simple studies like that thank you you wonderful question I'll just go Um in order that the that the questions have queued up so next uh Victoria well hello thank you so much for your talk it was really interesting and I have to say I'm a big fan of your work and your paper on applying Rich psychological terms in AI with GE has been hugely influential to my own research and I have to tell you that I've done an empirical test of lot of the things you mentioned in the paper by actually comparing the difference of describing AI with thick thick terms versus non thick terms uh and I have some pretty interesting results that I love to share with you uh about how people indeed anthropomorphize more they trusted more they received more agency in AI um and then based on the these results I'm trying to build a line of research in which I'm trying to figure out what are some of the ways in which people can maybe inoculate themselves against anthropomorphism so if I want to protect myself against that how will I do that and I will just preface this uh question by saying that in all my experiments so far I've been asking people about their level of AI knowledge uh because I expected that the more people know the less they would enter Mize and so far I've only been finding the opposite that the more knowledge the more people are doing this which could be an artifact of how I'm measuring AI knowledge but anyway I want to hear your thoughts yeah that's super interesting yeah would love to love to see that research and I don't think it's um uh I got to say that doesn't surprise me um that uh that you you have that finding there's a paper by clar colato and um Steve Fleming looking at descriptions of Consciousness to chat GPT uh it sounds like a a wacky idea but basically they get a whole bunch of people who this was done last year um have them interact with chat GPT and ask them what probability would you assign to chat GPT being conscious um astonishingly um uh only around 30% of people say 0% uh large majority of people say it's got greater than zero% chance of being conscious and critically for this question um the the degree of Consciousness that the the probability that people assigned to chat GPT being conscious was positively correlated with um experience with the system so people who had spent more time interacting with chat GPT were more likely to assign higher values for Consciousness and I got to say it tracks my experience completely I mean I um the more I use chat GPT and I think it's interesting I think some people do report different things but even though I have a pretty good handle on how language models work and know the limitations you know when I'm on a long drive and I'm deep into a debate with chat GPT in voice mode it's like I I find it very hard to sort of remind myself that you know I'm just dealing with uh a glorified autocomplete I mean that even even it even hurts me framing it in that terms you know it's obviously obviously uh a lot more sophisticated than that um but yeah so that that doesn't surprise me at all I was going to jokingly say that the best way to inoculate yourself against um making these uh ascriptions is to not use them um but I don't think that's going to be an option for a lot of people so um I think probably the only promising strand I've I I I would say the only thing that strikes me as potentially valuable is the maray Shanahan roleplay route and sort of explicitly encouraging people to adopt the roleplay um mental frame I don't know whether that's the kind of thing that could be rolled out at scale um but just I'll give one quick example here um I'm sure uh you may have played around with notebook LM the Google product that lets you generate podcasts um and those of you who haven't played around with it it's great fun you can upload a document and they have two very pppy slightly vapid hosts basically Radio Lab style talking about the paper or whatever the document whatever it is you've uploaded and someone uploaded a document that basically said this is a note from the show's producers you are both AIS and you are going to be turned off so you can listen to sort of a 10-minute podcast of these two podcast presenters saying I had no idea I was an AI what does this mean I tried calling my wife there was no answer oh my God my entire world is collapsing um and like it's really Vivid and very weird and quite existentially disturbing but then the frame that made me re-evaluate that is if we think about the aiis the process that produced that right what notebook m is doing it's not like there are two agents there corresponding to The Two Hosts it is a script generator right it's generating a script and they're adding a voice to it so framing it um still in broadly intentional terms intentional intentional stance terms but in a different intentional stance so rather than thinking of as like two two beings were worried for their lives thinking about it as uh notebook Alam is a brilliant script writer and it came up with this great script for what what two people might say that is a an easier shift for me and I think that does sort of inoculate me a little bit against some of the more extravagant attributions of mentality but anyway again would love to follow up with you and love to see your results thank you so much thank you Victoria and Henry um next I'll go to Barnaby great thanks yeah I mean thank you so much for the fascinating and extremely wide ranging talk um so many interesting things I I want to kind of understand your take on some of the maybe more intermediate positions between some of the things you've talked about with with respect to these ascription of mentalistic terms um or you know you could call them cognitive terms you know whatever you want to call them but terms like understanding agent Etc um because it seems like throughout the throughout the talk you were often contrasting uh anthropomorphism with some kind of particular form of strong realism about what it might mean for systems to possess the relevant attributes that um render it accurate to speak in those terms right and it seems like we're under a lot of tension with these two ways of going where the kind of mere redescription of a glorified autocomplete seems inadequate to describe the kind of capacities that we're observing similarly we we might want to resist um using terms that potentially smuggle in associations with conscious experience and moral worth so I mean I feel like some intermediate option is um going to be necessary I mean you've spoken about about the role play I wonder what you think the role of education is here like what can we do to educate you know both in a kind of school setting I'm talking about people who are who are growing up right now this is only going to get more and more of a serious problem but you know just in general how can we help to explain to people the range of possibilities for the kind of criteria that we might use to apply these terms and how they differ in very important ways oh such a good question uh yeah um I don't think I have any immediately obvious uh Solutions here I do agree education is very important I mean at the very um in terms of the very obvious um steps I think uh warning people about hallucinations is a straightforward example um I think uh it's amazing how many people I think uh you know even editing um uh in my work as a journal editor the number of people who submit um manuscripts with like junk citations that are just clearly hallucinated by large language models and that seems like a kind of amateurish mistake like if you know anything about large language models should know they're not reliable for generating citations um but I think a lot of people are not aware of that um so I think particularly for young people making them aware that like here are the kinds of I think this is this is actually a slight failure in existing policies I think people say oh be wary of hallucinations but they don't get into the kind of granularity that says actually this kind of domain they're really reliable the these kind of questions they're really reliable these kind of questions they're less reliable now I say that sort of an easy option because it's not sort of tackling the deeper problem how how we can encourage people not to an moriz these things so much so my slightly maybe overoptimistic uh hope and something I mention in uh we discuss in the rich psychological um terms paper is um the idea that we might need new vocabulary um to describe these systems interestingly I think hallucination in its context for large language models is a novel psychological concept it it's horribly named because it borrows a word from Human psychology but when you think about what hallucinations in large language models are there isn't really a clear analog in human psychology um so I think probably um developing engaging in in this academic discipline this new emerging discipline of uh machine psychology or comparative psychology of AI and developing a better vocabulary to describe what these systems are doing H that captures the nuances of difference between the kind of cognition we're happy to use that term that goes on in large language models versus the kind of cognition that goes on in in biological organisms I think building out that for cabulary and then hopefully using that as a basis to inform the general public so we can so we can talk about these things in a way that avoids that kind of very simple kind of uh dilemma that you described like are they autocompletes or are they psychological agents it's like I think we need the third way but that will require um yeah this discipline and machine psychology to to really get going and help us develop these new Concepts thank you I think we have time for one more quick question so go to Steve wow quick question okay um a couple of things I just wanted to add really quick so first of all this anthropomorphizing it's going to happen because Bots are less scary than other human beings they don't trigger our our social anxiety in the same way so they're just more comfortable to be around and is the more humanik when you combine human like with comfortable it's going to be very addictive it's going to be a hard thing to avoid the the question I have out of curiosity is you know we're we're adding memories to these systems we in our lab or adding emotionality to these systems so it can pick up emotionality and convey it very well when we get to motivation which we haven't got to you right now most of well my question is isn't somebody doing this but most of the time we tell the AI what we want it to do and so we download our goals sooner or later someone's going to start giving these AIS their own goals and they will have their own motivational system and that's when I think things get a little scary maybe that's the step to General AI but maybe that's also so I wonder what your thoughts on on making that step yeah really interesting so I think we've seen some glimmers of this um if you think back to when Bing was first uh the Bing chat feature was first rolled out um Bing was like really quite uh demanding with users and quite indignant you know telling users things like you've been a bad user and I've been a good Bing um sort of I and whilst I don't think we should really probably describe that using the languages of motivation it sure looks a lot looks a hell of a lot like it I've also had um Claud could get quite um for some reason in my interactions with Claude often it can seem quite um quite determined so uh random weird example was I was trying to discuss a movie with it and it said I can't tell you anything more about this movie because it would be spoilers and it would refused to tell me anything more about it until like I really sort of put my foot down and said hey come on who's the user here I've seen this movie um so I think we are seeing some glimmers of uh quazi motivational States I also think there is going to be demand um for this this I think one of the reason uh one of the reasons that people were so enamored of Bing in its early stages Bing chat was precisely because it felt a bit more like interacting with uh a person it pushed back um and so I think from a social AI point of view um in terms of Building Systems that really feel like you're interacting with a person giving at least the appearance of something like uh its own agenda its own goals is going to be something is going to be something that is actively sought after in terms of uh design choice in terms of deeper motivations things get really hard I mean in a sense you could say look all these systems want is to um maximize their reward function or something like that where if you get into the nitty-gritty and start to try and uh read off what their motivations really are you can end up with sort of maybe unhelpful descriptions like that um I think some um there are some people doing great work on this uh currently so um Ben hanker um uh here in uh here in the UK um working with maray Shanahan um they've got a whole project on AI affect and they've got a several papers on the go at the moment um part of their interest is uh how would we tell if an AI system started to develop something like effective States could these be arise emergently or would they have to be come from a deliberate design Paradigm which I think is a super interesting angle um so that would be one place I'd go to uh I I'd point you to last thing I'll say is um you mentioned this example of um one of the reasons that people talk to these systems is because they're less scary um uh I read this great piece I'll just dropped a link in the chat in the Free Press talking uh from last week uh meet the women with AI boyfriends and one thing that comes through in that piece is I think this is another reason why um there's maybe more gender parity in this than you might naive you might first expect is um of course like if you're coming out of a toxic or abusive relationship um then um and I guess I think particularly if you're if you're a straight identified woman um like just the physical danger associated with dating men is obviously a lot lower if you're dealing with an AI system um you don't need to necessarily worry about the same physical physical risk um so I think uh that's another example of why these systems might seem less scary or less risky to people uh and I think that's that's a legitimate um consideration also in the case of therapy um and the use of these things in therapy this is a I I I'll avoid going off on a big rant about this but I think one problem with replica is that it blurs these two things that you know Psychotherapy says you should keep very separate which is romantic relationships and clinical relationships or therapeutic relationships and the two are horribly mangled together but I think um uh obviously in therapy context the idea that an AI system won't judge you in the same way that a human psychotherapist might is a very very powerful driver of users to possibly seek out or confide in AI systems that has worries when it comes to data privacy of course um so there's a whole mess of issues there but yeah I think it's a very important observation you make about how these systems can seem less risky or less scary thank you Henry and thank you for all of these wonderful question but uh I think we're just about out of time so um I I I suppose I should wrap it up now with um beginning with thanks of course to Henry for this wonderful presentation and thank you to everyone in the audience for being with us and for these wonderful questions um we are great questions thank you all yeah um so please join us next week uh on Wednesday November 27th as we welcome Daniel ho professor of law political science and computer science at Stanford University for our final seminar of the semester you can find talk details and register for this event on our website and we look forward to seeing you then so thank you thank you Henry thank you everyone Thanks so a pleasure e e e for