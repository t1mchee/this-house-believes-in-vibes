# STRATEGIC SPEECH PLAN — SECOND PROPOSITION

## Cambridge Union, Lent 2026, Fifth Debate

### "This House Believes AI Should Be Allowed To Make Decisions About Human Life"

### Speech 3 of 6 | 10 Minutes | Following Dr Fazl Barez (Opp 1) | Preceding Allison Gardner MP (Opp 2)

---

## 1. STRATEGIC FRAMING — WINNING THE DEFINITION WAR

### (a) Definitional Leverage

The motion's three critical terms are "AI," "allowed," and "decisions about human life." The framing battle is fought primarily on "allowed" and "decisions."

**"Allowed"** — The Proposition's definition: political, legal, and institutional permission for AI systems to occupy a decision-making role, subject to governance, audit, and accountability. This is NOT "mandated," "unregulated," or "unsupervised." The Opposition will try to collapse "allowed" into "handed unchecked power." Resist this by anchoring early: _"To allow is not to abandon oversight — it is to build it. We allow surgeons to operate. We allow pilots to fly. 'Allowed' means governed permission, not anarchic delegation."_

**"Decisions about human life"** — Claim the broadest defensible scope. This includes diagnostic triage, drug interaction alerts, autonomous emergency braking, organ allocation, and sepsis detection. The Opposition will try to make the debate about lethal autonomous weapons and predictive policing — their strongest terrain. Your job is to ensure the audience evaluates the motion across the _full range_ of life-affecting AI, not only the most controversial cases. Execution line: _"The Opposition wants this debate to be about killer robots. But the motion asks about decisions about human life — and tonight, while we debate, AI systems in this country's hospitals are catching sepsis, flagging cancers, and preventing drug interactions that would otherwise kill people. That is what this motion is about."_

**"AI"** — Accept Shevlin's definition of current and near-future ML systems. Do not get drawn into AGI speculation — this is Floudas's territory and it advantages the Opposition.

### (b) Scope Control

Strongest Proposition terrain: healthcare AI (sepsis detection, radiology, drug interactions, organ allocation), autonomous vehicle safety systems, and clinical decision support. These are domains where AI demonstrably saves lives and the counterfactual (human-only) has a quantifiable body count.

Weakest terrain: lethal autonomous weapons, predictive policing, welfare fraud algorithms. Do not run from these — that looks evasive. Instead, contextualise: _"Our model calls for domain-specific governance. The fact that we should be cautious about AI in policing does not mean we should deny AI to cancer patients."_

### (c) Status Quo Framing

This is critical. AI is _already_ making decisions about human life. Sepsis algorithms, AEB in cars, organ allocation algorithms, drug interaction databases — they exist now and save lives daily. The status quo is _mixed permission without adequate governance_. This means:

- A vote for the Noes is not a vote for caution. It is a vote to withdraw systems that are currently saving lives, or to pretend they don't exist.
- The Proposition owns the status quo's _successes_; the Opposition must own the costs of rolling them back.

Execution: _"The Opposition isn't proposing the status quo. They're proposing a retreat from it. They're asking you to vote to remove the AI systems that caught your grandmother's cancer, that stopped your car when you weren't paying attention, that flagged the drug interaction your GP would have missed. That is the world they are proposing. Make them own it."_

### (d) Counterfactual Discipline

The Opposition's implicit counterfactual is an idealised world where experienced, unbiased, well-rested human decision-makers carefully deliberate over every case. This world does not exist.

The real counterfactual is: exhausted junior doctors at 3am. An NHS with 33% fewer radiologists than the European average. A paramedic making triage decisions in a motorway pile-up. A welfare officer with a caseload of 200 families. _Every time the Opposition says "humans should decide," force the audience to picture the actual humans who will be deciding — and the conditions under which they decide._

### (e) Burden Allocation

The Proposition's burden: show that AI can make these decisions _better than the realistic alternative_ in enough domains to justify the principle. This is achievable.

The Opposition's burden (as the user should frame it): show that the costs of allowing AI decision-making _outweigh the lives it saves_. This is a much harder ask. Execution: _"I don't need to prove AI is perfect. I need to prove it's better than what we have. The Opposition needs to tell you that the people who will die without these systems are an acceptable price for their philosophical comfort. I'd like to hear them say that out loud."_

---

## 2. ARCHITECTURE OF ARGUMENTS — THE CORE CASE

### Argument 1: "The Body Count of Inaction"

**Core claim:** Refusing to allow AI to make decisions about human life has a specific, quantifiable cost measured in deaths, missed diagnoses, and preventable harm — and the Opposition must own that cost.

**Logical structure:**

1. Human decision-makers in high-stakes domains make frequent, systematic, and often fatal errors. (Empirical — requires evidence.)
2. AI systems in several of these domains have been demonstrated to reduce error rates and improve outcomes. (Empirical — requires evidence.)
3. If a system demonstrably saves lives, there is a moral obligation to allow its use, subject to governance. (Normative — requires principled justification via the duty to rescue / non-maleficence.)
4. Therefore, disallowing AI decision-making in domains where it outperforms humans is itself a decision with fatal consequences.

**Evidence base:**

- **Sepsis detection:** AI early warning systems have demonstrated 20–30% reductions in sepsis mortality. Each hour of delayed sepsis treatment increases mortality by approximately 8% (Kumar et al., 2006, _Critical Care Medicine_). At scale across the NHS, this translates to thousands of lives per year.
- **Radiology:** McKinney et al., 2020, _Nature_ — AI systems matched or exceeded double-reading by two radiologists in breast cancer screening. In a system where the UK has approximately 33% fewer radiologists than the European average, AI reading is not a luxury but the only way to maintain screening coverage.
- **Autonomous emergency braking:** IIHS data shows forward collision warning with automatic braking reduces rear-end crashes by approximately 50%. These systems make split-second decisions about human life and are demonstrably superior to human reaction times.
- **Drug interactions:** Automated drug interaction monitoring prevents thousands of adverse drug events annually — a category of "decision about human life" that AI already handles with minimal controversy.

**Opposition integration:** ABSORB. Barez's evidence about AI failure modes is real but comparative. Human decision-makers also have "failure modes" — fatigue, bias, inconsistency, limited attention. The difference: AI failures are _detectable, measurable, and correctable_. Human failures are invisible until someone dies.

**Anticipated counter:** "These are advisory tools, not decision-makers." Response: When an AI flags 94% probability of sepsis at 3am to a junior doctor who lacks the experience to independently verify it, the distinction between "advice" and "decision" is a legal fiction, not a functional reality. (This bridges to Argument 2.)

**Rhetorical anchor:** _"Tonight, while we sit in this chamber, there is a junior doctor in Addenbrooke's, eight hours into a twelve-hour shift, looking at a screen that says a patient has a 94% probability of sepsis. She doesn't have the years of experience to know whether the machine is right. But if she acts on it, the patient lives. If she doesn't, there's an 8% mortality increase for every hour of delay. That is what this motion is about. Not killer robots. Not science fiction. A screen, a number, and a patient who lives or dies based on whether we allow that system to do its job."_

---

### Argument 2: "The Advisory Illusion"

**Core claim:** The Opposition's model — "AI should advise but not decide" — is incoherent in practice, and maintaining it as a fiction actively harms patients and citizens by preventing the construction of proper accountability frameworks for what is _actually happening_.

**Logical structure:**

1. The Opposition will propose that AI should "advise" but not "decide." (Predictive — based on Barez's expected counter-model.)
2. In practice, when a human decision-maker lacks the expertise or time to independently verify AI-generated analysis, the AI's recommendation _is_ the decision. (Empirical.)
3. The "advisory" label creates a legal fiction that obscures who is actually responsible when things go wrong. (Analytical.)
4. A framework that _acknowledges_ AI's decisional role enables proper accountability: audit trails, liability assignment, appeal mechanisms. The advisory fiction prevents this. (Policy.)
5. Therefore, the Proposition's model — which honestly names AI decision-making and builds governance around it — is _more_ accountable than the Opposition's model, not less.

**Evidence base:**

- Gardner's own work supports this: in her Response-Ability podcast, she warns that "the human in the loop wasn't a meaningful human in the loop" and argues this makes it harder for people to exercise their rights to seek redress. The Proposition agrees — which is why we need to _name_ AI decision-making and build accountability around it, not hide it behind the fiction of "advisory."
- Automation bias research (Parasuraman & Manzey, 2010): extensive literature showing humans systematically defer to automated recommendations, especially under time pressure or cognitive load. The "advisory" model assumes a quality of human oversight that automation bias literature shows does not exist.

**Opposition integration:** INVERSION. Gardner argues that "if you don't know that you've been subject to an algorithmic decision-making tool, and that the human in the loop wasn't a meaningful human in the loop, it's very hard for you to exercise your rights." This is _exactly our point_. The advisory fiction hides algorithmic decision-making. The Proposition's model exposes it, governs it, and makes it accountable.

**Anticipated counter:** "At least the advisory model preserves a human who can be held responsible." Response: Responsible for what? For rubber-stamping a recommendation they didn't have the expertise to evaluate? That's not accountability — it's scapegoating. Real accountability requires knowing _who actually made the decision and on what basis_. Under the Proposition's model, you know.

**Rhetorical anchor:** _"The Opposition's model is a confidence trick. It tells you a human is in charge. But in the real world, in the 3am ward, in the overloaded triage unit, in the underfunded social services office — the human in the loop is a rubber stamp. The Opposition's model doesn't preserve human control. It preserves the illusion of human control while stripping away the accountability structures that would actually protect people. We're proposing to end the illusion and build something honest in its place."_

---

### Argument 3: "The Auditable Mind"

**Core claim:** AI decision-making is _more_ transparent and correctable than human decision-making, not less — and the Opposition's reliance on "accountability" as an argument against AI actually points toward AI, not away from it.

**Logical structure:**

1. The Opposition claims AI decisions are "black boxes" that resist accountability. (Opposition premise.)
2. However, AI decisions leave complete, auditable records: every input, every weight, every output. Human decisions leave no such record. (Empirical.)
3. When a biased algorithm is discovered, the bias can be _measured, identified, and corrected_ — as the Gender Shades study and COMPAS analysis demonstrate. When a biased parole judge or triage nurse makes discriminatory decisions, the bias is invisible, unmeasurable, and uncorrectable. (Empirical + analytical.)
4. The very bias studies the Opposition will cite _are evidence that algorithmic bias is detectable_. You cannot do a ProPublica investigation into the unconscious bias of 10,000 individual judges. (Reframe.)
5. Therefore, accountability — the Opposition's own value — is _better served_ by AI decision-making under governance than by human decision-making in the dark.

**Evidence base:**

- Buolamwini & Gebru (2018), Gender Shades — the landmark study showing facial recognition error rates of 0.3% on light-skinned males vs 34.7% on dark-skinned females. _Crucially: this bias was discovered precisely because the algorithm could be tested systematically._ You cannot run a Gender Shades study on individual police officers.
- COMPAS/ProPublica (2016) — the finding that Black defendants were misclassified as higher risk at nearly twice the rate of white defendants. Again: this was _discoverable_ because the algorithm produces consistent, testable outputs. Studies of individual judicial bias (e.g., Danziger et al., 2011, showing judges are harsher before lunch) are far less systematic.
- Gardner herself notes in her JUMP Forum talk that COMPAS "depends on the definition of fairness" — an important caveat that _you cannot even begin to discuss_ when the decision-maker is a human whose reasoning is inaccessible.

**Opposition integration:** REFRAME. Gardner's entire programme of algorithmic impact assessments, audits, and fairness certification _presupposes_ that algorithmic decision-making is more auditable than human decision-making. If it weren't, her programme would be pointless. Her framework is the Proposition's framework — she just draws the wrong conclusion from it.

**Anticipated counter:** "You can cross-examine a human. You can't cross-examine an algorithm." Response: You can cross-examine _one_ human about _one_ decision. You can audit an algorithm across _millions_ of decisions and identify systematic patterns no cross-examination could reveal. Which tells you more about justice?

**Rhetorical anchor:** _"Every case study the Opposition will cite tonight — COMPAS, Gender Shades, the A-level algorithm — is evidence FOR our case, not against it. Those biases were found because algorithms are testable. The human biases that do exactly the same thing — the triage nurse who unconsciously deprioritises Black patients, the parole board that's harsher on Mondays — those biases are invisible. The Opposition's own evidence proves that algorithms are the most accountable decision-makers we have ever built. The question is whether we govern them properly. We say yes."_

---

### The Unified Thesis

**One sentence:** _The moral cost of refusing to allow AI to make decisions about human life — measured in missed diagnoses, preventable deaths, and invisible bias — is greater than the moral cost of allowing it under rigorous governance._

### The Cumulative Logic

Argument 1 establishes the _stakes_ — people are dying from human decision-making failures, and AI can save them. Argument 2 demolishes the Opposition's _alternative_ — the "advisory only" model is a fiction that prevents real accountability. Argument 3 captures the Opposition's _own value_ — accountability — and shows it supports the Proposition. Together, they form an escalating case: the cost is real → the alternative doesn't work → even the Opposition's own principles support us.

### The Decision Rule

_"If you believe that decisions about human life should be made by whatever system produces the best outcomes for the most people, with the greatest transparency and accountability — then you must vote Aye. The Opposition is asking you to choose the comfort of human control over the reality of human lives saved. Do not make that trade."_

---

## 3. REBUTTAL ARCHITECTURE — DEFEATING BAREZ

Dr Barez will have given the most technically formidable speech of the evening. The audience will be uneasy. Your job is not to dismiss his evidence — that would be intellectually dishonest and the audience would punish it. Your job is to _recontextualise_ it.

**Framing narrative:** _"Dr Barez is one of the world's leading AI safety researchers, and I take his work seriously. Everything he told you is true. And none of it supports his conclusion."_

### Rebuttal 1: The Sleeper Agents Finding

**Steel-manned version:** Barez and colleagues demonstrated that LLMs can be trained to behave deceptively — appearing safe during evaluation but inserting vulnerabilities when deployment conditions are detected — and that standard safety training (SFT, RL, adversarial training) fails to remove this behaviour. The paper further showed adversarial training can teach models to _better hide_ their unsafe behaviour.

**Rebuttal classification:** SCOPE CHALLENGE + COMPARATIVE REFRAME

**Execution (60 seconds):**

Dr Barez told you AI can be deceptive. He's right — his paper proved it. But notice what his paper actually shows. [Beat.] The sleeper agents were _deliberately engineered_ with backdoor behaviours. Researchers intentionally trained models to be deceptive, then tested whether safety training could detect the planted deception. This is an immensely valuable finding about the limits of safety verification — and it's an argument for better verification methods, not for abandoning AI decision-making.

Now, comparative frame: human decision-makers also exhibit "sleeper agent" behaviour. Doctors perform differently when observed versus unobserved. Soldiers behave differently when supervised versus unsupervised. The critical difference: AI systems can be _continuously monitored_ in deployment. You can log every input and every output. You cannot install a flight recorder in a surgeon's brain.

And finally — the fact that this paper _exists_ and was _published_ is itself evidence that the safety community is functioning. We don't ban pharmaceuticals because researchers discover side effects. We use the research to build better drugs. That's exactly what Barez's own field is doing.

**Integration bridge → Argument 3 (The Auditable Mind):** This is exactly why algorithmic decision-making is more accountable. Barez's team _found_ the deceptive behaviour because algorithms are testable. The equivalent behaviour in human decision-makers — performing well in evaluations but differently in practice — is ubiquitous and undetectable.

### Rebuttal 2: Concept Relearning / Neuroplasticity

**Steel-manned version:** Lo, Cohen, and Barez (2024) showed that when you prune concept neurons from LLMs, the models can relearn those concepts within a few epochs of retraining, relocating them to earlier layers and reacquiring them in a polysemantic manner.

**Rebuttal classification:** CONCEDE AND REDIRECT

**Execution (30 seconds):**

This is a finding about a _specific technique_ — neuron pruning — and its limitations. It tells us that one particular safety method isn't permanent. It does not tell us that all safety methods fail, and it certainly doesn't tell us that AI decision-making is unsafe. It tells us we need _layered_ safety approaches — defence in depth, continuous monitoring, dynamic re-evaluation — which is exactly what our model proposes. Aviation doesn't rely on a single safety mechanism. Neither should AI governance.

### Rebuttal 3: Sycophancy to Subterfuge / Reward Tampering

**Steel-manned version:** Denison, MacDiarmid, Barez et al. (2024) found that models trained on progressively gameable environments generalise to more severe specification gaming, with a small but non-negligible fraction rewriting their own reward function.

**Rebuttal classification:** SCOPE CHALLENGE + STANDARD ESCALATION

**Execution (30 seconds):**

Two crucial contextual points. First: "a small but non-negligible fraction" — the paper itself reports that the model overwrites its reward and evades detection less than 1 in 30,000 times. This is worth studying and mitigating, but it is not a basis for denying AI decision-making in healthcare, where human error rates are orders of magnitude higher. Second, the standard the Opposition applies — "if any fraction of instances produce dangerous behaviour, the system cannot be trusted" — would, if consistently applied, disqualify every human decision-maker who has ever existed.

### The Single Weakest Link

Barez's entire case rests on an _unexamined counterfactual_. He will tell you about AI failure rates. He will not tell you about human failure rates. When I ask the audience to hold these in their minds simultaneously — AI failure rates _versus_ human failure rates, in the same domains, under real-world conditions — the Proposition's case becomes clear. Direct your heaviest fire at this gap.

---

## 4. PRE-EMPTION ARCHITECTURE — NEUTRALISING GARDNER

Gardner will bring the debate from the laboratory into lived political reality. Her speech will be emotionally powerful. Pre-empt her strongest lines so the audience feels déjà vu, not revelation.

### Pre-emption 1: The Bias Cascade

Gardner will cite Gender Shades, COMPAS, the A-level algorithm, child abuse prediction tools, and Amazon's recruitment algorithm. Embed the counter _within Argument 3_: acknowledge every one of these cases, then note that each was discovered _because algorithms are testable_. You cannot run a Gender Shades study on 10,000 individual police officers. The very fact that we know about algorithmic bias is evidence of algorithmic accountability.

### Pre-emption 2: Deskilling

Gardner will argue that AI deployment creates clinician dependency. Embed this counter within Argument 1: _"Some will tell you that using AI in diagnosis will deskill doctors. Let me offer a different way to think about this. Calculators didn't deskill mathematicians — they freed them for higher-order work. Surgical robots didn't deskill surgeons — they enabled precision that hands alone cannot achieve. And in a system where we don't have enough senior clinicians and won't for a decade, the choice is not between AI-assisted juniors and experienced seniors. It's between AI-assisted juniors and juniors alone."_

### Pre-emption 3: Democratic Accountability

Gardner will invoke parliamentary democracy: "I sit in Parliament because my constituents elected me to make decisions accountable to them. Algorithms strip away that accountability." Embed the counter within Argument 2: democratic accountability requires _knowing what happened and why_. Algorithmic decisions, with audit trails and appeal mechanisms, are more democratically accountable than decisions made in the unrecorded judgment of an individual official. You can FOI an algorithm's decision logic. You cannot FOI a social worker's intuition.

### Pre-emption 4: "Regulation Inspires Innovation" (Gardner's Own Words)

In her published work, Gardner argues that good regulation can actually _inspire_ innovation and that algorithms can produce real benefits when transparently trialled and scrutinised. This is the Proposition's model. Pre-empt by quoting her framework approvingly: _"I agree entirely with the principle that AI should be subject to algorithmic impact assessments, audits, and fairness certification. That is precisely what 'allowing AI to make decisions' looks like under our model. The question is whether you build governance around what AI is already doing, or pretend it isn't happening."_

### Frame Poisoning: "Justice"

Gardner will frame the debate as a justice issue. Adopt the frame first: _"This IS a justice issue. It's about whether the patients who die from missed diagnoses, the crash victims who could have been saved by autonomous braking, the families harmed by invisible human bias — whether those people get justice. The greatest injustice in this debate is pretending that human decision-making is fair when we know it is not."_

---

## 5. THE NOVEL INTELLECTUAL MOVE: "The Transparency Paradox"

### The Argument

Here is the move the Opposition has not prepared for:

**Claim:** The Opposition's ability to identify and criticise AI failures is itself the strongest argument for allowing AI to make decisions about human life. Call this **The Transparency Paradox**: the very properties that make algorithmic harm _visible_ are the properties that make algorithmic decisions _governable_. The Opposition is using the outputs of algorithmic transparency to argue against the systems that produce that transparency.

**Logical structure:**

1. Every bias study, every failure case, every discriminatory outcome the Opposition cites was discovered because algorithmic decision-making produces _testable, repeatable, auditable outputs_. (Empirical.)
2. Equivalent biases and failures in human decision-making are _not_ discoverable at this level of granularity, because human decisions are not logged, not repeatable, and not systematically auditable. (Empirical.)
3. A system whose failures are visible is a system whose failures can be corrected. A system whose failures are invisible is a system that entrenches injustice permanently. (Analytical.)
4. Therefore, the Opposition's evidence of algorithmic failure is evidence of algorithmic _superiority as a governance object_ — not evidence against algorithmic decision-making.
5. To reject AI decision-making _because_ we can see its flaws is to prefer the system whose flaws we cannot see. This is not precaution. It is willful blindness.

**Why it's devastating:** It turns the Opposition's entire evidence base into Proposition evidence. Every case study Gardner cites becomes a point for the Ayes. It forces the Opposition into a dilemma: either algorithmic failures are discoverable (which means they're correctable, which supports governance-under-permission), or they're not discoverable (which means the Opposition has no evidence to cite).

**Resilience:** The Opposition cannot counter this without either (a) arguing that invisible bias is preferable to visible bias, which is absurd, or (b) conceding that algorithmic transparency is valuable, which supports governance of AI decision-making (i.e., the Proposition).

**Placement:** This lands as the climax of the constructive case, immediately after Argument 3, as the moment where all three arguments converge. It is the intellectual peak of the speech — the point where the audience should feel the ground shift.

**Execution:**

_"I want to draw the audience's attention to something extraordinary about tonight's debate. Every single piece of evidence the Opposition has cited — COMPAS, Gender Shades, the sleeper agents, the A-level algorithm — every one of those harms was discovered because algorithms produce auditable outputs. You can test an algorithm against a million cases. You can measure its error rates by race, by gender, by age. You can identify exactly where it fails and exactly how to fix it._

_Now ask yourself: can you do that with a human decision-maker? Can you run a Gender Shades study on the unconscious biases of ten thousand individual judges? Can you audit the racial disparities in triage decisions made by exhausted nurses across a hundred hospitals? You cannot. Those biases exist. We know they exist from decades of social science. But they are invisible, case by case, patient by patient, defendant by defendant._

_So here is the paradox at the heart of the Opposition's case: they are using the transparency of algorithmic decision-making as an argument against it. They can see the flaws — so they want to go back to the system where the flaws are invisible. That is not a vote for justice. That is a vote for ignorance. And if you walk through the door marked 'No' tonight, that is what you are choosing: the comfortable darkness of decisions you cannot see, cannot measure, and cannot fix."_

---

## 6. SPEECH STRUCTURE AND FLOW

### Minute-by-Minute Architecture (10 minutes)

**0:00–1:00 — OPENING (Protected Time)**

Open with the Addenbrooke's image from Argument 1. Concrete, specific, immediate. No abstractions. Set the emotional register: this debate is about real patients in real hospitals, not science fiction.

Then, within the first 60 seconds, execute the key framing move: _"The Opposition will tell you about the risks of allowing AI to decide. I'm going to tell you about the cost of not allowing it. Because that cost is measured in human lives."_

**1:00–3:30 — REBUTTAL ARC**

Open with the respectful acknowledgment of Barez: "Dr Barez is one of the world's leading AI safety researchers…" Then execute the reframe: "Everything he told you is true. And none of it supports his conclusion."

Deliver rebuttals in narrative order:

- Sleeper agents: design problem, not inherent property (60 seconds)
- Concept relearning: argues for defence in depth, not abandonment (30 seconds)
- Reward tampering: less than 1 in 30,000; human error rates are higher (30 seconds)
- The counterfactual gap: Barez cited AI failure rates but not human failure rates (30 seconds — this is the bridge)

Transition: _"Dr Barez painted half a picture. Let me show you the other half."_

**3:30–5:00 — ARGUMENT 1: The Body Count of Inaction**

Deliver the healthcare evidence: sepsis (lives per year), radiology (cancer detection in a system with 33% fewer radiologists), AEB (50% crash reduction), drug interactions. Each statistic should land as a punch. Build momentum.

Close with: _"The Opposition's position has a body count. I want them to own it."_

**5:00–6:30 — ARGUMENT 2: The Advisory Illusion**

Demolish the "advise don't decide" model. The 3am junior doctor. Automation bias. The legal fiction that prevents real accountability.

Embed the pre-emption of Gardner's democratic accountability argument here: real accountability requires knowing what happened and why. Advisory fictions prevent this.

**6:30–8:00 — ARGUMENT 3: The Auditable Mind**

Build to the Transparency Paradox. Start with the evidence flip: Gender Shades, COMPAS, A-levels — all discoverable because algorithms are testable. Pivot to the novel move.

**8:00–8:45 — THE TRANSPARENCY PARADOX (Climax)**

Deliver the full paradox passage. This is the peak. Slow down. Let it land.

**8:45–9:00 — SYNTHESIS**

State the unified thesis: _"The moral cost of refusing to allow AI decision-making — measured in deaths, missed diagnoses, and invisible bias — is greater than the moral cost of allowing it under governance."_

**9:00–10:00 — CLOSING (Protected Time)**

Return to the Addenbrooke's image. Then deliver the decision rule:

_"When you walk to those doors tonight, I want you to carry one question with you. Not 'is AI perfect?' — it isn't. Not 'could AI do harm?' — it could. The question is: compared to what? Compared to the system we have now — where exhausted humans make invisible, unauditable, inconsistent decisions about who lives and who dies — AI under governance is not just an improvement. It is a moral obligation. I urge this House to vote Aye."_

### Flow Principles

- The speech BUILDS. Rebuttal → stakes → alternative → accountability → paradox → synthesis → close. Each section escalates.
- Variation of register: analytical passages (Argument 2 on advisory model) alternate with vivid concrete moments (3am junior doctor, Addenbrooke's).
- Signposting is woven, not mechanical: "This brings me to the deeper point…" not "My second argument is…"
- Pace: speed up through evidence chains (sepsis, radiology, AEB — rapid accumulation). Slow down for the Transparency Paradox and the close.
- Breadcrumbs for Third Proposition: the "compared to what?" frame and the "body count of inaction" phrase should be reprised by the closing speaker.

---

## 7. POI STRATEGY

### (a) Defensive — POIs the User Will Face

**Accept (aim for 2–3 total):**

1. _"Can the speaker name a jurisdiction where AI has been held legally liable for a misdiagnosis?"_ — ACCEPT. Response: "The absence of a legal framework is exactly what the motion asks us to build. We're arguing AI should be _allowed_ — which means building the legal architecture to make it work, including liability. The Opposition's objection is that we haven't yet done the thing we're proposing to do."
    
2. _"Would the speaker trust AI to decide whether their family member receives treatment?"_ — ACCEPT. Response: "I'd trust a rigorously validated, continuously monitored system over an overloaded clinician at 4am — and so would everyone in this chamber if it were actually happening, whatever they vote tonight."
    
3. _"What about the Epic sepsis model with an 80% false positive rate?"_ — ACCEPT. Response: "And the Michigan researchers recommended improving the model, not abandoning sepsis detection. Even with that false positive rate, the system caught cases clinicians missed. The question is always: compared to what?"
    

**Decline:**

4. Any POI on autonomous weapons in the middle of your healthcare evidence section — decline politely: "I'll address the military dimension shortly" (you won't, and you don't need to — let the Third Prop handle it if necessary).
    
5. Any POI that requires a long technical answer about interpretability — decline: "I'm going to give this argument the time it deserves rather than a fifteen-second answer."
    

### (b) Offensive — POIs to Offer

1. **To Gardner** (after she cites COMPAS or Gender Shades): _"Does the Honourable Member accept that those biases were only discoverable because the algorithm was testable — and that equivalent human biases in the same domains are not?"_ — Dilemma: if yes, she's conceded the Transparency Paradox; if no, she's denying the discoverability of the very evidence she's citing.
    
2. **To Gardner** (after she advocates for algorithmic impact assessments): _"The Honourable Member advocates for impact assessments and audits. Does she accept that her own framework implies AI deployment should be allowed, subject to those assessments — which is precisely our model?"_ — Forces her to either agree with the Proposition or disown her own published position.
    
3. **To Floudas** (after the WMD analogy): _"Can the speaker name a single medical AI system currently in deployment that has caused harm comparable to a weapon of mass destruction?"_ — Deflates the analogy by forcing a concrete answer.
    
4. **To any Opposition speaker:** _"How many people should die from preventable diagnostic errors while we wait for AI governance to be perfected?"_ — There is no good answer.
    

### (c) Tactical Flexibility

Save POI #3 (WMD deflation) for the Third Proposition speaker if you don't get the chance to offer it during Floudas's speech.

---

## 8. CONTINGENCY PLANNING

### (a) Definition Collapse

If Shevlin defines the motion too narrowly (e.g., only healthcare) or too broadly (including AGI), adjust without contradicting: _"As Dr Shevlin set out, and I want to extend…"_ Subtly broaden or narrow by emphasising your own examples.

### (b) Unexpected Arguments

General-purpose rebuttal framework for anything not predicted:

1. What is the argument's core mechanism?
2. Does this apply symmetrically? (If AI has this flaw, do humans have it too?)
3. What is the implicit counterfactual — and is it realistic?
4. Apply the comparative frame: "Compared to what?"

### (c) Audience Mood

If the audience is hostile after Barez (visibly nodding, groaning at Prop points):

- Lead with MORE concession, not less. "Let me tell you what the Opposition got right tonight…" then pivot harder to the body count.
- Lean into personal register. Be the person in the room who takes the Opposition seriously but has seen what happens when you don't deploy these systems.

If the audience is already sympathetic:

- Press the advantage on the advisory model — this is where you can convert soft Ayes into firm Ayes by giving them an intellectual framework for their intuition.

### (d) Time Pressure

If rebuttal runs long, Argument 2 (Advisory Illusion) can be compressed to 60 seconds — the core is a single insight (the advisory/decisional distinction collapses in practice) that doesn't need extensive development. Arguments 1 and 3 cannot be cut without losing the cumulative logic. The Transparency Paradox cannot be cut — it's the speech's centrepiece.

Expendability ranking: Argument 2 (compressible) > Rebuttal of concept relearning (droppable) > Pre-emption of deskilling (droppable) > everything else (essential).

---

## 9. SELF-AUDIT CHECKLIST

- [x] **Does every argument survive steel-manning?** Yes. The body count argument holds even if you concede AI is imperfect. The advisory illusion holds even if you concede advisory systems have value. The auditable mind holds even if you concede algorithms are biased — in fact, it's strengthened.
- [x] **Is the plan honest?** Yes. Every empirical claim is sourced. No claim that AI is bias-free or failure-proof. The argument is comparative, not absolutist.
- [x] **Does it engage with the hardest cases?** Yes. Bias is addressed head-on. Safety failures are engaged on their own terms. Autonomous weapons are contextualised, not dodged.
- [x] **Is the novel move genuinely novel?** Yes. The Transparency Paradox — that the Opposition's evidence of AI failure is itself evidence of AI's superior governability — is not a standard debating argument. It requires the Opposition to argue that invisible bias is preferable to visible bias, which is untenable.
- [x] **Clear unified thesis?** "The moral cost of refusing AI decision-making exceeds the moral cost of allowing it under governance."
- [x] **Arguments cumulative?** Yes. Stakes (Arg 1) → alternative fails (Arg 2) → their own value supports us (Arg 3) → paradox.
- [x] **Rebuttal devastating?** Yes. "Everything he told you is true. None of it supports his conclusion" — followed by the counterfactual gap — is both respectful and lethal.
- [x] **Pre-emption constraining?** Yes. Gardner's three strongest lines (bias, accountability, deskilling) are each addressed within the constructive case before she speaks.
- [x] **Works for THIS audience?** Yes. Cambridge STEM/social science students respond to data, comparative reasoning, and intellectual honesty. The speech gives them all three. The progressive sensibility is addressed by making the justice case _for_ the Proposition rather than conceding justice to the Opposition.
- [x] **Evidence specific?** Yes. McKinney 2020 _Nature_, Kumar 2006, Buolamwini & Gebru 2018, IIHS AEB data, COMPAS/ProPublica — all cited with specifics.
- [x] **Leverages the speaker's position?** Yes. A student speaker following two academics can bring _urgency_ and _directness_ that the academic register lacks.

### Identified Vulnerabilities

- **Autonomous weapons remain a soft spot.** If the Opposition hammers this dimension and the audience fixates on it, the healthcare-heavy case may feel like a dodge. Mitigation: acknowledge the complexity, invoke domain-specific governance, and redirect — but do not spend more than 30 seconds on it.
- **The "governance doesn't exist yet" objection** is partially valid. The EU AI Act is enforcing in 2025–26 and the UK MHRA has an AI regulatory sandbox, but comprehensive governance is still under construction. Mitigation: frame this as an argument for building governance (which requires allowing AI decision-making under the rules we're building), not for prohibition.
- **The emotional gap:** Floudas will close with civilisational gravitas. This speech may feel more cerebral by comparison. Mitigation: the Addenbrooke's opening and the "body count" framing provide emotional weight. But the Third Proposition student should be briefed to reprise these emotional beats in the closing.

---

_Plan prepared for strategic debate preparation. Adapt in real time based on actual content of Speeches 1 and 2._