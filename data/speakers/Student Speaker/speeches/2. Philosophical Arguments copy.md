# Philosophical Foundations for Second Proposition

## A Supplement to the Speech Plan

This document provides rigorous philosophical argumentation to undergird the three main arguments and the Transparency Paradox. Each section identifies the philosophical framework, the key moves, and how to deploy them in the speech. These are tools for the speaker to draw on — not all of them need to appear in a ten-minute speech, but knowing them allows you to argue from depth and respond to philosophical challenges with precision.

---

## I. THE ETHICS OF DELEGATION: Why Democracies Can and Do Legitimately Transfer Decisions to Non-Human Systems

### The Problem the Opposition Implicitly Relies On

The Opposition's case rests on an unstated premise: that decisions about human life carry a special property — call it **moral gravity** — that requires a _human_ decision-maker. This sounds intuitive. But it is philosophically undertheorised, and once you interrogate it, it collapses into either a tautology or a claim the Opposition cannot sustain.

### 1.1 The Principal-Agent Framework and Democratic Delegation

Democratic theory has always involved delegation. Citizens do not personally decide whether to go to war: they delegate to representatives, who delegate to generals, who delegate to targeting officers. The philosophical question is never _whether_ to delegate, but **under what conditions delegation is legitimate**.

The standard account (drawing on Locke's _Second Treatise_, §§134–142, and its modern developments in Pettit's _republicanism_) holds that delegation is legitimate when:

1. **The principal retains the power to set the mandate** (the scope and rules of the delegated authority).
2. **The principal can revoke the delegation** (the agent serves at the pleasure of the principal).
3. **The agent is subject to accountability mechanisms** (its performance can be evaluated and corrected).
4. **The delegation improves outcomes relative to the principal acting alone** (there must be a reason to delegate).

Notice what is _not_ on this list: that the agent must be human, that the agent must be conscious, or that the agent must have moral feelings about its decisions. We delegate to _institutions_ — courts, central banks, triage protocols, sentencing guidelines — all the time. The Bank of England's Monetary Policy Committee makes decisions that affect whether pensioners can heat their homes. We do not require each committee member to feel anguish about this; we require them to follow a mandate, be transparent, and be subject to democratic override.

**The move in the speech:** "The Opposition treats 'human decision-maker' as a self-evident requirement. But democratic theory has never required this. It requires a mandate, accountability, and the possibility of override. AI systems can satisfy all three. The question is not _who_ decides, but _under what governance structure_ the decision is made."

### 1.2 The Institutional Turn: Decisions Are Already Non-Personal

Philip Pettit's work on _group agency_ (_Group Agency_, 2011, with Christian List) demonstrates that institutional decisions are irreducible to individual human judgments. When a hospital triage protocol determines that Patient A receives the ventilator and Patient B does not, the _protocol_ is the decision-maker — even if a nurse physically flips the switch. The nurse's role is ministerial: they execute the protocol's logic.

This is not a degradation of the nurse's humanity. It is the _point_ of protocols — to ensure that decisions are consistent, fair, and not subject to the cognitive limitations or biases of the individual executing them.

The Opposition's model — "AI advises, human decides" — fails to engage with the institutional reality that many life-and-death decisions are _already_ made by protocols, guidelines, and rules rather than by individual human judgment. Adding AI to this picture is not a categorical shift; it is an improvement in the precision of the protocol.

**The move:** "The Opposition speaks as if there is currently a thoughtful human being weighing each case on its merits. There is not. There is a protocol, a guideline, a scoring system — and an exhausted human executing it. We are not replacing Aristotelian _phronesis_ with a machine. We are replacing a bad protocol with a better one."

### 1.3 Hobbes, Sovereignty, and the Authorisation Problem

There is a deeper philosophical move available here from Hobbes (_Leviathan_, Ch. XVI). Hobbes distinguishes between the **author** of an action and the **actor** who performs it. The sovereign is _authorised_ by the social contract to act on behalf of the citizens; the citizens are the authors, the sovereign is the actor. Accountability flows not from the actor's internal states, but from the **authorisation relationship**.

When a democratic polity authorises an AI system to make triage decisions — through legislation, through institutional governance, through clinical protocols — the _polity_ is the author of those decisions. The AI is merely the actor. Accountability attaches to the authorising institution, not to the executing system.

This dissolves the Opposition's "accountability gap" argument. They will say: "If an AI makes a lethal decision, who is responsible?" The answer, philosophically, is: the institution that authorised and deployed the system, exactly as we hold hospitals responsible for triage protocols and militaries responsible for rules of engagement. Responsibility does not require that the proximate decision-maker have a soul.

**The move:** "Who is responsible when a triage protocol lets someone die? The hospital. Who is responsible when rules of engagement cause civilian casualties? The military command. Who is responsible when an AI system makes a wrong call? The institution that deployed it. The accountability question has a perfectly clear answer. The Opposition is manufacturing confusion where none exists."

---

## II. ACTS, OMISSIONS, AND THE MORAL WEIGHT OF INACTION

### Grounding Argument 1 (The Body Count of Inaction)

### 2.1 The Acts/Omissions Distinction and Its Limits

The Opposition's case has a structural asymmetry: they treat every AI error as a _positive harm_ (an act), while treating every death from the absence of AI as merely _unfortunate_ (an omission). This leverages the folk-moral intuition that causing harm is worse than failing to prevent it.

But this distinction has been under sustained philosophical attack since at least Rachels's "Active and Passive Euthanasia" (1975, _NEJM_). Rachels argued that there is no _intrinsic_ moral difference between killing and letting die; what matters is the intention, the outcome, and the justification. If a doctor withholds a treatment _they know would save the patient_, the moral weight of that omission is equivalent to actively causing the death.

Peter Singer (_Practical Ethics_, 1979) extends this through the **drowning child** analogy: if you walk past a drowning child and do nothing, you bear moral responsibility for the death, even though you did not push the child into the pond. The cost of intervention is relevant — but when the cost is low (deploying a validated AI system) and the benefit is high (lives saved), the failure to act becomes morally culpable.

**The move:** "The Opposition asks you to worry about what happens if we allow AI to decide. I ask you to worry about what happens if we don't. When a validated system could save lives and we refuse to deploy it — not because it doesn't work, but because we are philosophically uncomfortable — that refusal has a body count. And that body count is morally equivalent to the harm the Opposition fears."

### 2.2 Scanlon's Contractualism and the Reasonable Rejection Test

T.M. Scanlon's contractualism (_What We Owe to Each Other_, 1998) provides a more rigorous framework. Scanlon argues that an action (or policy) is wrong if it could be reasonably rejected by anyone affected by it. The test is: could a person in the worst-off position under this policy _reasonably reject_ it, compared to the alternatives?

Apply this to the motion. Consider the patient who dies of sepsis because the hospital did not deploy an early-warning AI system. Could that patient reasonably reject the policy of allowing AI to make (or contribute to) triage decisions? No — because the alternative (no AI) is _worse for them_. They die.

Now consider the patient who is misclassified by an AI system. Could they reasonably reject the deployment of AI? Only if the alternative — human-only decision-making — would have produced a better outcome for _them specifically_. But in aggregate, human error rates in the relevant domains (radiology, sepsis detection, crash avoidance) are _higher_ than AI error rates. The patient harmed by AI cannot reasonably reject a system that, in expectation, saves more lives — any more than a patient harmed by a drug side-effect can reasonably reject the drug if it saves ten thousand others.

**The move:** "Ask yourself: which policy could not be reasonably rejected by the people it affects? A world where AI systems save thousands but occasionally err? Or a world where those thousands die so that we can preserve the comforting fiction of human control? Scanlon's test gives us a clear answer."

### 2.3 The Duty of Rescue as Institutional Obligation

Many legal systems recognise a _duty of rescue_ — an obligation to assist someone in peril when the cost of doing so is low. In moral philosophy, this has been formalised by Peter Unger (_Living High and Letting Die_, 1996) as: you are morally required to prevent serious harm when you can do so at modest cost to yourself.

The deployment of validated AI systems in healthcare and safety is exactly this kind of case. The systems exist. They work. The cost of deployment is manageable. The harm prevented is measured in human lives. A refusal to deploy is not neutral caution — it is a _failure of the institutional duty of rescue_.

This framework converts the Opposition's precautionary framing into a moral liability. They cannot simply say "we should be cautious." They must justify the deaths that caution produces.

---

## III. EPISTEMIC JUSTICE, TRANSPARENCY, AND THE RIGHT TO REASONS

### Grounding Argument 3 (The Auditable Mind) and the Transparency Paradox

### 3.1 Miranda Fricker's Epistemic Injustice Framework

Miranda Fricker's _Epistemic Injustice_ (2007) identifies two forms of injustice in the domain of knowledge: **testimonial injustice** (when a speaker's credibility is unfairly deflated due to prejudice) and **hermeneutical injustice** (when a person lacks the conceptual resources to understand or articulate their own experience of harm).

Both forms of epistemic injustice are _endemic_ to human decision-making in high-stakes domains and _reducible_ through algorithmic systems:

**Testimonial injustice in human decisions:** When a Black patient presents with chest pain, implicit bias may cause clinicians to discount their self-reported symptoms. Studies (e.g., Hoffman et al., 2016, _PNAS_) show that medical students and residents hold false beliefs about biological differences between Black and white patients, leading to systematic under-treatment of pain. This is testimonial injustice operating at scale, invisibly, with no mechanism for detection or correction.

**Hermeneutical injustice in opaque systems:** When a parole board denies release based on "gut feeling," the affected person cannot challenge the reasoning because there is no articulable reasoning to challenge. They lack not only access to the decision-maker's reasons but the conceptual framework to identify what went wrong.

Algorithmic systems _can_ (when properly designed) mitigate both: they do not discount testimony based on the speaker's social identity, and they produce articulable decision-paths that affected persons can examine and contest. This is not to say algorithms are bias-free — they inherit biases from training data. But inherited bias is _visible_ and _correctable_ in a way that implicit human bias is not.

**The move:** "The Opposition talks about algorithmic bias as if it were a new problem. It is an old problem made newly _visible_. When Joy Buolamwini showed that facial recognition failed on darker-skinned women, that was not a discovery that technology is biased. It was a discovery that _bias can be measured_. You cannot run the Gender Shades study on ten thousand individual police officers. Algorithmic bias is epistemic injustice made tractable."

### 3.2 The Right to Reasons and Algorithmic Explainability

There is a well-established philosophical principle — articulated in administrative law as the _duty to give reasons_ and in moral philosophy as respect for persons (Kant, _Groundwork_, 4:429) — that decisions significantly affecting a person's life must be accompanied by _reasons that the affected person can understand and contest_.

Human decision-makers in high-stakes domains routinely violate this principle. A social worker's "professional judgment," a clinician's "clinical intuition," a judge's "experience" — these are not reasons in any meaningful sense. They are labels for processes that are opaque to the decision-maker themselves, let alone to the person affected.

An algorithmic decision, by contrast, is in principle decomposable. You can identify which features weighted the outcome. You can ask counterfactual questions: "Would the decision have been different if this input were changed?" The field of _explainable AI_ (XAI) exists precisely to make this decomposition practical.

The Opposition will cite cases where AI systems are "black boxes." This is true of some current systems. But the philosophical point cuts the other way: the existence of _partially_ explainable AI is already an advance over _wholly_ unexplainable human intuition. And the direction of travel is toward _greater_ explainability, not less. The human decision-maker is the permanent black box.

**The move:** "Kant held that respect for persons requires giving them reasons for decisions that affect their lives. When was the last time a parole board gave reasons that a prisoner could meaningfully contest? When was the last time a triage nurse articulated — in examinable terms — why Patient A was seen before Patient B? The algorithmic system is not perfect. But it is the first decision-maker in history that can, in principle, show its working."

### 3.3 The Transparency Paradox as Philosophical Argument

The Transparency Paradox can be stated formally:

**Premise 1:** The Opposition's evidence of AI failure (COMPAS bias, Gender Shades disparities, A-level algorithm errors) is _only available_ because algorithmic outputs are testable, auditable, and reproducible.

**Premise 2:** Equivalent failures in human decision-making (judicial bias, clinical bias, grading bias) exist at comparable or greater rates but are _not discoverable_ at scale because human decisions are not testable, auditable, or reproducible.

**Premise 3:** A system whose failures are discoverable is _epistemically superior_ to a system whose failures are undiscoverable, because only the former can be corrected.

**Conclusion:** The Opposition's evidence of algorithmic failure is itself evidence for the epistemic superiority of algorithmic decision-making.

This is structurally similar to Karl Popper's argument for the superiority of scientific over non-scientific claims: what makes a theory _scientific_ is not that it is true, but that it is _falsifiable_. A claim that cannot be tested cannot be improved. Similarly, a decision-making system that cannot be audited cannot be made more just.

**The move:** "The Opposition is making a Popperian error. They are treating the falsifiability of algorithmic decisions — the fact that we _can_ find the flaws — as evidence against the system. But falsifiability is a _virtue_, not a vice. The system whose flaws you can find is the system you can fix. The system whose flaws are invisible is the system that perpetuates injustice forever."

---

## IV. THE PRECAUTIONARY PRINCIPLE AND ITS PHILOSOPHICAL LIMITS

### Rebutting the Opposition's General Stance

### 4.1 The Asymmetric Precautionary Principle

The Opposition — particularly Floudas — will invoke some version of the precautionary principle: when an action risks catastrophic harm and the science is uncertain, we should refrain from acting.

This principle, as standardly formulated (e.g., the 1992 Rio Declaration, Principle 15), has a well-known philosophical problem: it is _self-defeating when applied symmetrically_. This was demonstrated decisively by Cass Sunstein in _Laws of Fear_ (2005).

The argument runs as follows: The precautionary principle says "do not allow AI decision-making because it _might_ cause catastrophic harm." But the _status quo_ — human decision-making in high-stakes domains — _demonstrably_ causes catastrophic harm (medical error is the third leading cause of death in the US; human drivers kill 1.35 million people annually worldwide). The precautionary principle, applied consistently, also forbids the status quo.

The principle therefore cannot tell us what to do. It can only tell us to be afraid — of _both_ options. To break the symmetry, you need a _comparative_ analysis of risks: which system, under which governance regime, produces fewer catastrophic outcomes? And that comparative analysis is precisely what the Proposition is offering.

**The move:** "Floudas will invoke precaution. But precaution applied honestly points in our direction. The precautionary principle says: do not accept catastrophic risk when an alternative exists. The alternative to AI decision-making is human decision-making. Human decision-making kills. The precautionary principle, taken seriously, demands that we deploy the system with the lower catastrophic risk — and that system, in domain after domain, is AI under governance."

### 4.2 The WMD Analogy and the Fallacy of Misplaced Concreteness

Floudas's signature argument — that advanced AI should be treated like Weapons of Mass Destruction — commits what Whitehead called the _fallacy of misplaced concreteness_: it treats an analogy as an identity.

WMDs are defined by a specific set of properties: they are _designed to kill_, they produce _indiscriminate_ harm, their destructive potential is _inherent to their function_, and they cannot be "used well." A nuclear weapon used precisely as designed kills hundreds of thousands of people. That is what it is _for_.

AI decision-making systems have none of these properties. A sepsis detection algorithm is designed to _save lives_. Its destructive potential arises only from _misuse_ or _malfunction_, not from its intended function. The correct analogy is not to a weapon but to a pharmaceutical: a drug that saves millions but has side effects, requires regulation, and demands ongoing monitoring.

If Floudas's analogy were taken seriously, it would prove too much. Medical devices can malfunction and kill. Shall we treat MRI machines like WMDs? Aviation autopilot can crash planes. Shall we ban autopilot under a non-proliferation treaty? The WMD frame makes for dramatic rhetoric, but it is analytically useless for the question before the House, which is about _domain-specific decision-making under governance_ — not about existential risk from superintelligence.

**The move:** "A nuclear weapon used exactly as designed kills a city. A sepsis algorithm used exactly as designed saves a life. These are not the same kind of thing. Comparing them is not serious policy analysis — it is catastrophism dressed up as caution."

### 4.3 The Reversal Test (Bostrom & Ord)

Nick Bostrom and Toby Ord's "Reversal Test" (2006, _Ethics_) provides a useful tool here. When people resist a proposed change (e.g., allowing AI to make decisions), the Reversal Test asks: would you also resist a change in the _opposite_ direction? If not, your resistance to the proposed change is likely driven by _status quo bias_ rather than by principled reasoning.

Applied here: The Opposition resists allowing AI decision-making. Would they also resist _removing_ AI systems that are already making life-affecting decisions (e.g., ABS in cars, automated drug-interaction warnings, sepsis alerts)? If not — if they would not advocate ripping these systems out — then their objection is not principled opposition to AI decision-making. It is status quo bias, dressed up as philosophy.

**The move:** "I want to apply the Reversal Test. If the Opposition truly believes AI should not make decisions about human life, they should advocate removing every automated emergency braking system on every car in this country. They should demand that hospitals disable sepsis early-warning systems. They should call for banning automated drug-interaction alerts. If they would not do those things — and they would not — then they do not actually believe their own motion. They are defending the status quo because it is the status quo."

---

## V. ACCOUNTABILITY WITHOUT CONSCIOUSNESS

### Dissolving the "Responsibility Gap"

### 5.1 The Responsibility Gap (Matthias 2004) and Its Resolution

Andreas Matthias ("The Responsibility Gap," 2004, _Ethics and Information Technology_) argued that autonomous learning systems create a "gap" in responsibility because no human can predict or fully control the system's behaviour, yet the system itself is not a moral agent and cannot bear responsibility.

This is the Opposition's strongest philosophical card. But it relies on a **Romantic conception of responsibility** — responsibility as requiring an individual moral agent who _could have done otherwise_. This conception has been under pressure in philosophy for decades.

The resolution comes from recognising that **responsibility is a distributed, institutional property**, not solely an individual one. Dennis Thompson's concept of **"many hands"** problems in political philosophy (_Political Ethics and Public Office_, 1987) shows that in complex institutional settings, responsibility is distributed across designers, operators, regulators, and oversight bodies. No single individual bears full responsibility for any institutional decision — yet the institution as a whole is accountable.

This is not a _new_ problem created by AI. It is the _same_ problem that arises with any complex institutional decision: who is responsible for a pharmaceutical's side effects? The chemist who designed the molecule? The clinical trial director? The regulator who approved it? The physician who prescribed it? The answer is: all of them, in proportion to their roles. We do not refuse to deploy pharmaceuticals because the responsibility is distributed. We create _governance structures_ to manage the distribution.

AI decision-making fits exactly the same model. The developer is responsible for validation. The deploying institution is responsible for contextual fitness. The regulator is responsible for standards. The political authority is responsible for the mandate. Responsibility is distributed — but it is not absent.

**The move:** "The Opposition asks: who is responsible? Everyone who should be. The developer, the hospital, the regulator, the legislature. This is how institutional accountability works for every complex system in a modern society. We do not refuse to use anaesthesia because no single person is responsible for every outcome of surgery. We build governance. That is what the motion asks us to do."

### 5.2 Moral Agency vs. Moral Accountability

The Opposition conflates two distinct philosophical concepts:

- **Moral agency**: the capacity to be a _subject_ of moral evaluation (i.e., to be praised or blamed for one's choices). This plausibly requires consciousness, intention, and the capacity for moral reasoning.
- **Moral accountability**: the capacity to be _held to account_ — to have one's outputs evaluated, corrected, and governed by external standards.

AI systems lack moral agency. They cannot be blamed. But they can be held accountable in every functional sense: their outputs can be evaluated, their errors can be identified, their parameters can be adjusted, and they can be decommissioned. This functional accountability is, in many ways, _more robust_ than the accountability we extract from human agents, who can lie about their reasoning, refuse to cooperate with investigations, and whose decision-processes are permanently inaccessible.

The philosophical error is to assume that because AI lacks _moral agency_, decisions involving AI lack _accountability_. The two properties are separable. A thermostat has no moral agency but is accountable (we can check whether it maintained the correct temperature). A sentencing algorithm has no moral agency but is accountable (we can audit its outputs for racial bias). A human judge has moral agency — but their reasoning is, in practice, far _less_ accountable than the algorithm's.

**The move:** "Dr Barez studies whether AI systems can be deceptive. Here is what is remarkable about that research: he can study it. He can test it. He can measure it. Can you test whether a judge is being deceptive about their reasoning? Can you measure whether a clinician's 'gut feeling' is actually implicit racial bias? You cannot. The AI is the _more_ accountable decision-maker, not because it has better intentions, but because it has _transparent mechanisms_."

---

## VI. THE COMPARATIVE FRAME AS PHILOSOPHICAL METHOD

### Integrating Everything

### 6.1 The Methodological Point

Every argument above converges on a single methodological principle that should structure the entire speech: **you cannot evaluate a system in isolation; you must evaluate it against the realistic alternative**.

This is not a rhetorical trick. It is a requirement of practical reasoning, formalised in decision theory as the principle that _choices are between options, not between an option and an ideal_. When the Opposition says "AI might err," they are evaluating AI against an idealised human decision-maker who always gets it right. But that decision-maker does not exist.

The correct comparison is:

- AI system with known error rate X, under governance regime G, vs.
- Human system with known error rate Y, under governance regime H.

If X < Y, and G ≥ H (AI governance at least as robust as human governance), then allowing AI decision-making is not merely _permissible_ — it is _required_ by any consequentialist, contractualist, or virtue-ethical framework that takes the prevention of harm seriously.

### 6.2 The Burden of Proof

The Opposition may argue that the burden of proof lies with the Proposition: we must demonstrate that AI is _safe enough_ before it should be allowed. This seems reasonable but embeds a philosophical error.

The correct analysis of burden of proof in comparative contexts was given by John Stuart Mill (_On Liberty_, Ch. I): the burden of proof lies with whoever proposes to _restrict_ liberty or prevent an action that would otherwise be permissible. Since deploying AI systems is a _positive act of policy_, the Opposition might seem to bear no special burden.

But the motion is not asking whether to _introduce_ AI decision-making from nowhere. AI systems are _already_ making life-affecting decisions across medicine, transport, and safety. The Opposition's position is, in practical terms, a call to _restrict or prohibit_ these deployments. That is a restriction on institutional freedom, and it is the restriction that must be justified — especially when the restriction has demonstrable costs in human life.

**The move:** "The Opposition speaks as if the burden is on us to prove AI is safe. But AI is already saving lives in hospitals across this country. The burden is on _them_ to justify removing those systems. And they cannot justify it — because the cost of removal is measured in deaths."

---

## VII. QUICK-REFERENCE: PHILOSOPHICAL MOVES MAPPED TO SPEECH SEGMENTS

|Speech Segment|Philosophical Framework|Key Thinker|One-Line Deployment|
|---|---|---|---|
|Rebuttal of Barez|Popper on falsifiability; Sunstein on symmetric precaution|Popper, Sunstein|"Finding the flaw is the precondition for fixing the flaw."|
|Argument 1 (Body Count)|Acts/omissions equivalence; duty of rescue; Scanlon's contractualism|Rachels, Singer, Scanlon, Unger|"The patient who dies from our refusal to deploy cannot reasonably accept your caution."|
|Argument 2 (Advisory Illusion)|Delegation theory; Hobbesian authorisation; institutional accountability|Locke, Hobbes, Pettit/List, Thompson|"Accountability attaches to the authorising institution, not to the executing system."|
|Argument 3 (Auditable Mind)|Epistemic justice; right to reasons; Kantian respect for persons|Fricker, Kant, Popper|"The algorithmic system is the first decision-maker in history that can show its working."|
|Transparency Paradox|Popperian falsificationism applied to institutional design|Popper|"Falsifiability is a virtue. The system whose flaws you can find is the system you can fix."|
|Pre-emption of Floudas|WMD analogy as fallacy of misplaced concreteness; Reversal Test|Whitehead, Bostrom & Ord|"Would you rip out every ABS and sepsis alert? No? Then you agree with us."|
|Pre-emption of Gardner|Fricker's epistemic injustice; bias as tractable vs. invisible|Fricker, Buolamwini|"Algorithmic bias is not a new problem. It is an old problem made newly visible."|
|Closing comparative frame|Decision-theoretic comparison; Mill on burden of proof|Mill, Scanlon|"The burden is on them to justify the deaths their caution produces."|

---

## VIII. NOTES ON DEPLOYMENT

These philosophical frameworks are sharp tools. In a ten-minute competitive speech, you cannot develop each one fully. The recommended approach is:

1. **Pick two or three** that feel most natural to your speaking style and that respond most directly to what the first two Opposition speakers actually said.
    
2. **State them crisply** — one or two sentences of philosophical setup, then immediately apply to a concrete case. The Cambridge Union audience is smart but wants rhetoric grounded in argument, not a lecture.
    
3. **Name the philosophers sparingly.** One or two name-drops (Scanlon, Popper, Fricker) signal seriousness. More than that sounds like a supervision essay. The strongest move is to state a philosophical principle as if it is obviously true and then show why it supports your case.
    
4. **The Transparency Paradox is the place to go deepest.** This is your original intellectual contribution to the debate. Spending 60–90 seconds on its formal structure (Premises 1–3, Conclusion) is warranted. The rest of the speech should feel more like _applied_ philosophy than _academic_ philosophy.
    
5. **Keep the comparative frame omnipresent.** Every philosophical argument above has the structure: "the AI system, under governance, is better than the realistic human alternative." This is the through-line that gives the audience a clear decision rule.