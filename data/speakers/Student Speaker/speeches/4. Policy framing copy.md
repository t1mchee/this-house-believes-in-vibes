# Policy & Geopolitics Arsenal — Second Proposition

## The Governance Is Already Happening

---

## I. THE STRATEGIC FRAME

The Opposition's implicit model is: "AI is ungovernable, therefore it should not be allowed to make these decisions." The Proposition's counter: **AI is already being governed, across every major jurisdiction, by every major company, and through multiple overlapping mechanisms — and this governance infrastructure is deepening faster than at any point in the history of technology regulation.**

The question is not whether governance is possible. The governance already exists. The question is whether it is _good enough_ — and whether refusing to allow AI to make decisions about human life makes it better or worse.

Our answer: prohibition makes governance _worse_, because it drives development underground, removes incentives for transparency, and hands the field to the least safety-conscious actors.

---

## II. THE GLOBAL REGULATORY LANDSCAPE (As of February 2026)

### 2.1 The European Union: The World's Most Comprehensive AI Law

The EU AI Act (Regulation 2024/1689) is the first binding, comprehensive AI-specific legislation in the world. It entered into force on 1 August 2024 and is being phased in through 2027.

**Timeline already in effect:**

- **February 2025:** Prohibited AI practices banned outright — including social scoring, manipulative AI, predictive policing based on profiling, and real-time biometric surveillance in public spaces.
- **August 2025:** Governance obligations and rules for general-purpose AI models in effect. EU AI Office operational. National competent authorities designated. Penalty regime active (up to €35 million or 7% of global turnover for prohibited practices).

**Coming into effect:**

- **August 2026:** Full application of the AI Act for most operators. High-risk AI systems in Annex III (including healthcare, law enforcement, education, critical infrastructure) must comply with comprehensive requirements: risk management systems, data governance, technical documentation, human oversight, transparency, accuracy, robustness, and cybersecurity. Transparency obligations (Article 50) also apply.
- **August 2027:** Rules for high-risk AI embedded in regulated products (medical devices, vehicles, machinery).

**Key features for the debate:**

- The Act classifies AI by _risk level_, not by technology type — exactly the proportionate, domain-specific governance the Proposition advocates.
- High-risk systems (healthcare, justice, critical infrastructure) face the strictest requirements — but are _permitted under governance_, not banned.
- The EU explicitly allows AI in high-stakes domains provided it meets safety, transparency, and human oversight standards. This is a governance model, not a prohibition model.
- The November 2025 "Digital Omnibus" simplification package shows the EU is actively iterating — adjusting timelines and reducing compliance burdens while maintaining safety standards. This is responsive governance in action.

**The move:** "The European Union — representing 450 million citizens and the world's largest single market — has concluded that AI should be _allowed_ in healthcare, law enforcement, and critical infrastructure, provided it meets rigorous safety and transparency standards. The Opposition is asking this House to be more restrictive than the EU. That is a remarkable position."

### 2.2 The United States: Competition, Federalism, and Sector-Specific Regulation

The US picture is more fragmented but no less active:

**Federal level:**

- Biden's Executive Order 14110 (October 2023) established reporting and safety obligations for frontier AI companies. Trump revoked it in January 2025, but replaced it with a series of executive actions:
    - **January 2025:** EO 14179 "Removing Barriers to American Leadership in AI" — pro-innovation, anti-regulatory framing.
    - **July 2025:** America's AI Action Plan — detailed policy roadmap emphasising US "AI dominance."
    - **December 2025:** EO "Ensuring a National Policy Framework for AI" — seeks to preempt state AI laws deemed "onerous," create a uniform federal framework, and establish a DOJ AI Litigation Task Force.
- The FDA has cleared ~950 AI-enabled medical devices through established regulatory pathways — the most mature AI governance regime in the world for a specific domain.
- Sector-specific regulation continues through existing agencies: FDA (medical devices), SEC (financial AI), NHTSA (autonomous vehicles), FTC (consumer protection).

**State level:**

- California's Transparency in Frontier AI Act and Texas's Responsible AI Governance Act both effective January 2026.
- Colorado AI Act (algorithmic discrimination) effective June 2026.
- Illinois amended its Human Rights Act to prohibit discriminatory employer use of AI.
- Multiple other states have enacted or are considering AI legislation.

**The move:** "Even under an administration explicitly committed to minimal AI regulation, the United States is building governance infrastructure — through FDA clearance pathways, state legislation, and federal frameworks. The debate is about _how_ to govern AI, not _whether_ to govern it. That debate is already settled."

### 2.3 The United Kingdom: Principles-Based, Moving Toward Legislation

The UK operates a "pro-innovation, principles-based" approach through existing sectoral regulators, guided by five cross-sector principles: safety, transparency, fairness, accountability, and contestability.

**Key developments:**

- **2023:** Bletchley Park AI Safety Summit — first international government-led summit on frontier AI safety. 29 countries sign the Bletchley Declaration.
- **2023:** AI Safety Institute (AISI) established to test frontier models.
- **February 2025:** AISI rebranded as AI Security Institute, focusing on national security and misuse risks.
- **2025:** Frontier AI Trends Report published — rigorous evaluation of model capabilities across cyber, biology, and autonomy domains.
- **2025:** AI Growth Lab proposal — cross-economy sandboxes for testing AI innovations under regulatory guidance.
- **2026 (expected):** Comprehensive AI Bill anticipated, drawing on lessons from EU AI Act and international summits.

**For the debate (in Cambridge, speaking to a UK audience):**

- The UK government explicitly endorses AI in high-stakes domains under governance — its position is closer to the Proposition than the Opposition.
- The AI Security Institute's Frontier AI Trends Report demonstrates exactly the kind of rigorous pre-deployment testing the Proposition model requires.
- MI5 Director General Ken McCallum (October 2025) acknowledged "potential future risks from non-human, autonomous AI systems" — but framed this as a reason for _deep engagement_, not prohibition.

### 2.4 China: The World's Most Extensive AI Regulatory Arsenal

China has, since 2021, enacted more binding AI-specific regulations than any other country:

- **March 2022:** Algorithmic Recommendation Provisions — transparency, fairness, user controls over personalisation, algorithm filing with the Cyberspace Administration of China.
- **January 2023:** Deep Synthesis Provisions — labelling, traceability, and moderation requirements for deepfakes and AI-generated content.
- **August 2023:** Generative AI Interim Measures — the world's first binding regulation for generative AI. Requires security assessments, LLM filing, content moderation, data protection.
- **September 2025:** AI Labelling Measures — mandatory explicit and implicit labels for AI-generated content, watermarking standards.
- **August 2025:** Draft AI Ethics Measures — requiring ethics committees and review for AI activities posing risks to life, health, dignity, or public order.
- **July 2025:** AI Plus Action Plan — targeting 70% AI penetration in key sectors by 2027, 90% by 2030. AI-powered economy by 2035.

**As of October 2025, China has approved thousands of algorithm filings** — a highly dynamic, actively enforced regulatory regime.

**The strategic point:** China is simultaneously pursuing aggressive AI deployment _and_ comprehensive regulation. They do not treat deployment and governance as contradictory. The Opposition's framing — that allowing AI to make high-stakes decisions is incompatible with safety — is rejected by every major jurisdiction on the planet.

### 2.5 Other Jurisdictions

- **South Korea:** AI Act promulgated as fundamental governing law.
- **Japan:** Governance guidelines for AI systems, sector-specific approaches.
- **Canada:** Proposed Artificial Intelligence and Data Act (AIDA).
- **Brazil:** AI regulatory framework under development.
- **India:** Sector-specific approaches with NITI Aayog guidelines.
- **International:** OECD AI Principles (42 countries), G7 Hiroshima AI Process reporting framework (February 2025), UNESCO Recommendation on Ethics of AI, Council of Europe Framework Convention on AI (2024).

**The cumulative picture:** There is no jurisdiction of any significance that has adopted the Opposition's position of prohibiting AI from making decisions about human life. Every major regulatory framework permits it under governance.

---

## III. THE INDUSTRY LANDSCAPE: WHO BUILDS AI AND HOW THEY SELF-GOVERN

### 3.1 The Major Developers and Their Safety Frameworks

As of December 2025, **twelve companies** have published frontier AI safety policies: Anthropic, OpenAI, Google DeepMind, Magic, Naver, Meta, G42, Cohere, Microsoft, Amazon, xAI, and NVIDIA.

**Anthropic** (Responsible Scaling Policy — RSP):

- Defines AI Safety Levels (ASLs) — capability thresholds that trigger escalating safety requirements.
- Models evaluated for CBRN risk, cyber capabilities, autonomous replication, and AI R&D acceleration.
- Public Benefit Corporation structure — legally required to consider safety alongside profit.
- Conducts the only human-participant bio-risk trials in the industry.
- Leads in interpretability research (attribution graphs, SAEs — see Technical Arguments supplement).
- Scored highest (C+) in the 2025 AI Safety Index from the Future of Life Institute.
- Shared Claude 3.5 Sonnet with UK AI Safety Institute and METR before deployment.

**OpenAI** (Preparedness Framework):

- Defines capability thresholds across cyber, CBRN, persuasion, and model autonomy.
- Explicitly pledges to halt training for "critical risk" models.
- Only company to publish full whistleblowing policy.
- Pre-deployment evaluation of models without safety guardrails to approximate true capabilities.
- Signed the EU AI Act Code of Practice.

**Google DeepMind** (Frontier Safety Framework):

- Domain-specific capability levels and testing.
- Collaboration with UK AI Security Institute, granting access to models for pre-deployment testing (December 2025).
- Extensive cyber and biology evaluations documented in model cards.

**Meta** (Responsible Use Guide + safety evaluations):

- Open-source approach (Llama models) — enables external scrutiny but raises proliferation concerns.
- Publishes model cards with safety evaluations for major releases.

### 3.2 The Seoul Frontier AI Safety Commitments (May 2024)

At the AI Seoul Summit, **sixteen companies** (later twenty) committed to:

- Publishing responsible scaling policies by February 2025 (most have done so).
- Pre-deployment testing for dangerous capabilities.
- Sharing evaluation results with governments.
- Implementing model weight security proportional to risk.
- Halting development/deployment if mitigations are insufficient.

**The honest assessment:** These are voluntary commitments. Several labs have weakened specific provisions over time. The Future of Life Institute's 2025 AI Safety Index gave the _best_ company (Anthropic) only a C+. The gap between leaders and laggards is widening. Voluntary pledges alone are insufficient.

**But this is the Proposition's argument, not the Opposition's:** The correct response to insufficient voluntary governance is _better mandatory governance_ — which is precisely what the EU AI Act, the UK's forthcoming legislation, and state-level US laws provide. The solution to imperfect self-regulation is regulation, not prohibition.

### 3.3 The Competitive Dynamics Argument

There are approximately 7–10 companies globally with the resources to develop frontier AI models. They operate in a competitive landscape where:

- **Safety leaders** (Anthropic, to some extent OpenAI and DeepMind) invest heavily in interpretability, pre-deployment testing, and responsible scaling — but face competitive pressure from labs that invest less.
- **The race dynamic** means that if safety-conscious labs slow down, less safety-conscious ones fill the gap. This is not hypothetical — both DeepMind and Anthropic have acknowledged that some of their safety commitments are conditional on other labs adopting similar measures.
- **Open-source proliferation** (Meta's Llama, DeepSeek, Mistral) means that capable models are widely available regardless of any single lab's policies.

**The Proposition's frame:** Prohibition of AI decision-making in high-stakes domains does not stop AI development. It stops _governed_ AI deployment. The models will exist regardless. The question is whether they are deployed within a regulatory framework — with testing, oversight, accountability, and audit trails — or whether they are deployed in the shadows, by actors with no incentive to be safe.

---

## IV. THE GEOPOLITICAL ARGUMENT

### 4.1 The US-China AI Race

This is the elephant in the debating chamber. Both the United States and China are pursuing AI dominance as a core strategic priority:

- **US:** Trump administration frames AI as essential to "national security and dominance." America's AI Action Plan (July 2025) explicitly uses the word "dominance." The December 2025 Executive Order exists to prevent state regulation from slowing US competitiveness.
- **China:** AI Plus Action Plan targets 70% AI penetration by 2027, 90% by 2030. China aims for a fully AI-powered economy by 2035. DeepSeek-R1 (January 2025) demonstrated Chinese models competing at the global frontier. Military AI ("intelligentized warfare") is a core strategic doctrine.

**The implication for the motion:** If democratic nations refuse to allow AI to make decisions about human life — in healthcare, in defence, in safety-critical infrastructure — authoritarian regimes will not follow suit. China is already deploying AI extensively in surveillance, military targeting, and healthcare. The question is not whether AI will make decisions about human life globally. It is whether _democracies with rule of law, transparency requirements, and accountability mechanisms_ will participate in that development or cede it to regimes without those safeguards.

**The move:** "The Opposition asks us to abstain from a technology that every major power on Earth is deploying. China is not having this debate. They are deploying AI in healthcare, in military systems, in critical infrastructure — under their own regulatory framework, aligned with their own values. If democracies refuse to develop governed AI systems for high-stakes decisions, we do not prevent those decisions from being made. We simply ensure they are made without democratic oversight, without human rights safeguards, and without accountability."

### 4.2 The Autonomous Weapons Dimension

This is where the Opposition will try to make their strongest geopolitical case. Lethal autonomous weapons systems (LAWS) are the hardest ground for the Proposition.

**Honest concession + reframe:**

- There are legitimate concerns about fully autonomous weapons that select and engage targets without meaningful human control. The Proposition need not defend this.
- But the motion is "AI should be _allowed_ to make decisions about human life" — not "AI should replace all human judgment in all military contexts."
- AI already makes life-affecting decisions in military contexts: missile defence systems (Patriot, Iron Dome), electronic countermeasures, autonomous navigation of vehicles to avoid casualties, medical triage in field hospitals.
- The international community is actively developing governance: the Convention on Certain Conventional Weapons (CCW) has been discussing LAWS since 2014. The 2025 International AI Safety Report addresses military applications. The Bletchley Declaration included military AI in its scope.

**The move:** "I suspect the Opposition will invoke killer robots. Let me be direct: autonomous weapons that select and engage human targets without meaningful human control raise genuine concerns, and governance is needed. But Iron Dome makes split-second decisions about whether to intercept incoming rockets over civilian populations. Patriot missile batteries decide what to engage faster than any human could. Military medical triage AI is keeping wounded soldiers alive. Are these the decisions the Opposition wants to prohibit?"

### 4.3 The "Democratic AI" Argument

This is the affirmative geopolitical case:

Democratic governance of AI requires democratic nations to _have_ AI systems to govern. You cannot regulate what you do not develop. You cannot audit what you do not deploy. You cannot set global norms from the sidelines.

The UK hosted the Bletchley Summit and established the AI Safety Institute precisely because it is a player in AI development. The EU's AI Act carries global influence precisely because European companies and citizens use AI systems. The US shapes global AI norms precisely because American companies develop frontier models.

If democratic nations prohibit AI from making decisions about human life, they:

- Lose the capacity to set safety standards (no domestic deployment = no regulatory expertise).
- Lose leverage in international negotiations (no skin in the game = no credibility).
- Lose the talent pipeline (safety researchers go where the models are deployed).
- Cede the field to jurisdictions with weaker human rights protections.

---

## V. REBUTTING FLOUDAS: THE WMD ANALOGY

Floudas argues that advanced AI should be regulated like Weapons of Mass Destruction, under an international AI Control & Non-Proliferation Treaty.

### 5.1 Where the Analogy Breaks

|WMD|AI|
|---|---|
|Designed solely to cause mass casualties|Designed for broad utility; harm is a misuse case|
|Physically scarce (enriched uranium, weaponised pathogens)|Informationally abundant (algorithms, data, compute)|
|Controlled by a handful of state actors|Developed by hundreds of companies, universities, individuals|
|Non-proliferation possible through physical control|Non-proliferation impossible for information-based technology|
|No civilian benefit from nuclear warheads|Massive civilian benefit from medical AI, safety systems, etc.|
|Treaty verification through physical inspection|No physical substrate to inspect|

### 5.2 The Practical Absurdity

A WMD-style treaty would require:

- An international body capable of monitoring every GPU cluster, every research lab, every university worldwide.
- Enforcement mechanisms against states (including the US and China) that have declared AI a core strategic priority.
- A definition of "advanced AI" that draws a clear line between a permissible system and a prohibited one — something no expert has been able to do.

China's AI Plus Action Plan targets 90% AI penetration by 2030. The US administration frames AI as essential to national dominance. Neither would sign a non-proliferation treaty that constrained their development.

### 5.3 The Historical Parallel That Actually Works

The better analogy is not nuclear weapons. It is **aviation**, **pharmaceuticals**, or **telecommunications**:

- Technologies with enormous benefit and significant risk.
- Governed through international standards bodies (ICAO, WHO, ITU), national regulators (FAA, MHRA, Ofcom), and industry self-governance.
- Not subject to non-proliferation — subject to _safety certification_.
- Anyone can build a plane; not anyone can fly one commercially without meeting standards.

This is the model the EU AI Act follows. This is the model the FDA follows for medical devices. This is the model the Proposition advocates. Floudas's WMD analogy is not just wrong — it is counterproductive, because it implies the only options are total control or total danger, when the real option is proportionate governance.

**The move:** "Mr Floudas wants to treat AI like nuclear weapons. But AI is not a bomb. It is an infrastructure. You do not ban electricity because it can electrocute people. You write wiring codes. You do not ban pharmaceuticals because they can poison people. You establish the MHRA. You do not ban aviation because planes can crash. You establish the CAA. Every one of these frameworks allows the technology to make decisions that affect human life — under governance. That is all we ask."

---

## VI. REBUTTING GARDNER: BIAS, GOVERNANCE, AND THE "NOT YET" ARGUMENT

Gardner's likely position: AI systems are biased, opaque, and unaccountable, therefore they should not make decisions about human life until these problems are solved.

### 6.1 Using Her Own Framework

Gardner herself has argued _for_ algorithmic impact assessments, audit frameworks, and responsible deployment. Her published position is not "ban AI" — it is "govern AI better." This is the Proposition's position.

- Her JUMP Forum talk calls for "regulation, audits, and impact assessments" — mechanisms the EU AI Act now provides.
- Her article in The Conversation ("Don't write off algorithms") explicitly argues that "algorithms can help in low-risk, high-impact uses when transparently trialled and scrutinised."
- Her podcast on "responsible AI" discusses the roles of investors and clinicians in oversight — a governance model, not a prohibition model.

**The move:** "Ms Gardner has spent her career building the tools to make AI accountable: impact assessments, audit frameworks, bias detection methodologies. The EU AI Act now mandates them. Her life's work is our model. I am confused about why she is opposing it."

### 6.2 The "Not Yet" Trap

The Opposition will argue: "We don't oppose AI in principle. We just think it's not ready yet."

This is the most seductive and most dangerous version of the Opposition case, because it sounds reasonable while being unfalsifiable. There is no point at which the "not yet" faction declares victory. The goalposts will always move.

**Counter-arguments:**

- **What threshold?** Ask the Opposition to specify the exact conditions under which they _would_ allow AI to make decisions about human life. If they cannot, their position is not "not yet" but "never."
- **Comparative standard:** "Not ready" compared to what? The current system produces 250,000 deaths per year from medical error in the US alone. The status quo is not safe. It is the thing we are trying to improve.
- **Already happening:** 950 FDA-cleared AI medical devices. This is not a future question. The Opposition's "not yet" requires withdrawing systems that are already saving lives.
- **Innovation paradox:** You cannot improve AI safety without deploying AI in governed environments and learning from real-world performance. The pharmaceutical model requires clinical trials — real-world deployment under governance — before approval. The Opposition's model would ban clinical trials.

---

## VII. THE PROPOSITION'S GOVERNANCE MODEL (What We're Actually Arguing For)

To neutralise the Opposition's strongest attacks, the Second Proposition should make the governance model explicit:

1. **Risk-proportionate regulation** — the EU AI Act model. Higher-risk applications face stricter requirements. No blanket permission; no blanket prohibition.
    
2. **Pre-deployment safety evaluation** — models tested for dangerous capabilities before release, as Anthropic, OpenAI, and DeepMind already do, and as the UK AI Security Institute facilitates.
    
3. **Mandatory transparency and auditability** — algorithmic impact assessments, interpretability requirements, public documentation. The tools exist (attribution graphs, SAEs, standardised benchmarks).
    
4. **Human oversight appropriate to the domain** — not necessarily a human in every loop, but meaningful human oversight at the system level. A radiologist reviews AI-flagged scans. A clinician oversees AI triage. A commander authorises weapon system engagement parameters.
    
5. **Accountability frameworks** — clear liability chains. The EU AI Act assigns obligations to providers, deployers, and importers. When AI errs, someone is legally responsible.
    
6. **International coordination** — through existing and developing mechanisms (AI Safety Summits, OECD principles, G7 Hiroshima Process, bilateral agreements).
    
7. **Continuous monitoring and iteration** — post-deployment surveillance, incident reporting, and regulatory updating. The Digital Omnibus shows the EU is already doing this.
    

**The move:** "We are not asking this House to hand AI systems unchecked power over human life. We are asking this House to recognise that a governance framework already exists — in EU law, in FDA regulation, in UK policy, in international agreements — that _allows_ AI to make these decisions under strict conditions. Voting No tonight means rejecting that framework. It means telling the EU, the FDA, and every hospital using AI diagnostic tools that they are wrong. It means telling patients whose cancers were caught by AI screening that the system that saved their life should be prohibited."

---

## VIII. QUICK-REFERENCE: POLICY FACTS FOR THE SPEECH

|Fact|Source|Use|
|---|---|---|
|EU AI Act: prohibited practices banned Feb 2025|EU Official|"Already in force"|
|Fines up to €35M or 7% global turnover|AI Act Art. 99|"Real teeth"|
|950 FDA-cleared AI medical devices|FDA, Stanford 2025|"Already deployed"|
|20 companies signed Seoul Frontier Safety Commitments|METR Dec 2025|"Industry buy-in"|
|12 companies published frontier safety policies|METR Dec 2025|"Self-governance emerging"|
|China: 70% AI penetration target by 2027|AI Plus Action Plan|"They're not waiting"|
|UK AI Bill expected 2026|White & Case tracker|"Legislation coming"|
|Bletchley Declaration: 29 countries|DSIT 2023|"International consensus"|
|International AI Safety Report: 100 experts, 30 countries|Bengio et al. 2025|"Scientific consensus"|
|Trump Dec 2025 EO: uniform national framework|Federal Register|"Even deregulators want governance"|
|GPT-4 outperforms physicians in diagnostic reasoning|Goh et al., JAMA|"The evidence is in"|
|Best AI company safety score: C+ (Anthropic)|FLI Safety Index 2025|"Room for improvement — which is why we need regulation, not prohibition"|