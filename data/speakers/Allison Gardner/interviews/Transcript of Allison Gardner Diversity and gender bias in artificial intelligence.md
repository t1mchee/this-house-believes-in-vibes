[Music] does I'm going to tell you about one tell you about how buyers can be built into machine algorithms to perform discrimination how that bias is built in and why that bias is to built is built in because of a lack of diversity and I'm gonna focus on why there are not women in in the field of tech particularly AI and I am going to completely disagree with the concept of the leaky pipeline the pipeline is not leaking women are being forced out of it and I'll tell you why that happened so I'm very grumpy so some examples of where we can get bias in tech so this was the first one I know the team who developed today this was a chatbot that they released on Twitter and it was a learning of an algorithm to learn from its environment so within 24 hours of being on Twitter and learning from the feedback it was getting it turns into this Hitler loving sex robots and and it was awful it was so bad that they had to take themselves down I have forgot to set my timer off so you couldn't I'm gonna have to watch you it was awful and I was so embarrassed and another one was Google with their photo tagging and they found that they were actually photo tagging people with black faces as guerrillas and when this was flagged up by one of their tech guys and so this is ridiculous they tried to fix it they couldn't fix it so now what they've done is now you can't tag gorillas as gorillas they've removed gorillas because it was the only way they could fix it so that's what that headline says so this is so endemic then there's these these are coming very popular now these these child-abuse prediction algorithms so they started off in the States a very subtly no well-written algorithm and al-ghani I probably pronouncing that wrong by an academic open source but she admits that it's bias because the data that they used to build odd comes primarily from poor families so it's biased against those in poverty now evidence shows that children can be a blue abuse and neglect regardless of and economic status but unfortunately the rich did not get brought to the attention of social services the way that the poor do so the data that this algorithm is built on is biased and we have five councils in the UK now have had private companies develop algorithms and they're utilizing them and the answer is it's always don't worry snitches it relieves the social workers of all that worry about all those meetings about deciding whether to do something and they can go and do what they do best and work with the families and now I'm also a councillor in my local council and that it's not what will happen what will happen is instead of having six social workers around a table discussing a case what they'll do is they'll have three because they won't need six because they're using this algorithm and you will end up not only reducing in staff but you'll also end up d skilling as well which is what's happened in the states we've also had things like in fraud detection in Newcastle in the UK if you're black your university application is 21 more times more likely to be flagged up as fraudulent so making it harder for you to access University this was self-driving cars with this big one in the AI ethics about you know who do you run over and they found that it's actually more likely to run over a black person than a white person this study claims so that was a new one that's just recently come out within that the financial sector loans mortgage all based on algorithms and again inbuilt bias into those so people who are poor or who have got negative you know social status for example are more less likely to get the loads and the mortgages and the preferential treatment and so this again further in builds this this inequality that you get and of course Amazon is possibly the better known one that their recruitment and algorithm that they had actually was biased against women and this happens quite a lot with with these recruitment so be very careful well if you think that take we'll solve a lot of these problems that early on this morning about unconscious bias I heard about how you know all these ways of no choosing or Orchestra you know women who playing in the orchestra and they do them by blind auditions as well maybe if we could devise an algorithm to detect the best musical and player it won't matter if they're male or female but it could be built on faulty data so you have to be very careful of all of these things I'm not really negative there are some really really good AI apps out there are algorithms out there so this is the one to detect skin cancer they found that it's 95 percent correct compared to eighty seven point five percent of dermatologists which sounds fantastic doesn't it okay one thing I'll point out there's a another article in Nature that looked at a different algorithm that could actually diagnose and many illnesses and they found the big big adverted lines was algorithm predicts and or diagnosis diseases better than doctors until you look at the data and what it does is it diagnoses diseases better than trainee doctors but not as better as experienced senior doctors so the question that you have is that they got all great what we'll do is we'll use those algorithms to do the diagnosing and we'll train the doctors on using that are we going to never get to the point of the old senior doctors who could diagnose better than the algorithm are we go to limit how good we get and D skill our doctors and how we train an algorithms like this aren't perfect so they can go wrong again on the data but they have if your data said that you've done all your analysis on was built you know from data from a particular country so the UK and is biased for white skin you're going to get less quality for black skin diagnosis and again you're going to get health inequalities building in so when you start using these algorithms you need to ask those questions you need to ask about the metrics what was your dataset like here's another example this is a very famous one joy bull and Winnie and Tim MacGregor Gabriel did this without facial recognition and they found that there was with various the big organizations that wore knife IBM's for example pretty good on male faces not point three percent error rate on the female faces thirty four point seven and some of the data sets that have been built for some of these facial recognition are so biased seventy to eighty percent white and seventy to eighty percent male except when they want them to be slightly different and I'll come to that in a second but before I move on I'm talking about data sets and algorithms so a very quick explainer do not ever fall foul foul of the bat box argument oh we can't explain our algorithm it's a black box it's all very important you know you know we can't reveal it to you don't even need to know about that your data huge data sets is always dirty 20 piece of what 80 percent of the data scientists time would be spent cleaning that data getting rid of all the incomplete data sets and then processing it so that the algorithm could understand it so turning a yes and a no into a 1 and a 0 or a 0 into 1 depending on how I want to define it then we've got to feature select so of all of these these categories that we're going to do which ones are we going to put through our algorithm which ones are more important who makes that decision statistics and humans not the algorithm and then we might weight it depending on the algorithm we can actually say some features are more important or more relevant than others then we shove it through the algorithm the machine learning and then when we get our result out there's more stuff that we do we look at our metrics how many false positives do we have how many false negatives where do we want our cutoff to make that prediction how what do we want to reduce what's the most risky is putting an innocent person in prison more risky than releasing a violent person out into society where am I gonna put my cut-offs which brings me to a very famous case and nine minutes oh my goodness Ivo halfway through so this is compass and compass actually was bought out as a whether you should put somebody in prison or not and it was found by ProPublica said it's biased it's really biased against male I'm black black men okay North Point said well no it isn't it's absolutely got the metrics we've got the evidence it's not it's not biased here it is and the problem was it depended on the definition of fairness that you used so again the metrics aren't perfect and I would also say be very careful about this over Alliance and algorithms and thinking that they are perfect that it's actually only about sixty five percent accurate is compass and it's actually no more accurate than a very simple model made of two characteristics compared to compasses 137 and it's just as accurate if you just look at what's the age and previous convictions and if you also got a load of people around so a pick six of you from this audience ago right I want you to decide whether we should put that person in prison or not you would be as accurate as that algorithm that they're spending hundreds of thousands on so they are not perfect so again know how accurate your algorithm is it is not the quick fix that you think so a lot of these are really really sort of crazy issues how can we allow this machine bias to be built-in and this faulty data and this bias datasets that nobody everything to ask what's your proportion there and another little thing that you need to do algorithms can't cheats so this is the classic mode for detecting hip fractures from an image detecting you think that's fairly fairly benign but what the algorithm learned to do was to define by the actual patient's age and the Machine that it used to do the the the imaging because if it was a mobile machine that it gave the impression while that person is not mobile can't come to hospital they must be really ill so we're going to predict that as a fracture it ignored all the personal data about about the person at all in the image it you know and they do cheat another one for quizzes how it was designed to answer quizzes and it had to get this high high response rate what the algorithm did is it went and it found the arts a file that was buried elsewhere deep in the program deleted it so it always got 100 percent accuracy they they know are they dead she soaps let's carry on why have we got to this point well it's because there is a lack of diversity in the development chains development process and inclusivity as well as really important so from identifying that the type of problem we want to solve with tech all the way through to developing it and then evaluating it you can start building in unconscious bias because you do not have diverse teams and if you are starting to produce algorithms that we are using within society to determine very important things that is going to embed discrimination inequality in our society as we enter the fourth Industrial Revolution so I asked for you to think about cynicism so with five minutes to go how have we led to this situation of a lack of diversity right we lack women proof that we lack women's so this was a work by element AI and wired and they sort of did some some analysis and they found for example that Google AI research there are 21 percent women in tech in AI only 10 percent so a is worse than tack Facebook 22% of in in tack 15% in AI and overall it comes to about 12% whirring isn't it when we're getting this really important algorithms being developed and utilized and yet we have this this limit of women in tech and I'm just focusing on women now but it's wider as well it's very worrying it's and we're not going to get any better this was also from the same level of research and this is the amount of researchers I I know and this is just from 4,000 researchers who attended the three key key conferences and this was the evaluation of how many were women and how many were men now you can say well that's just how a measure of how many women can get to conferences but even that in itself tells you something and it's not quite correct because I know Virginia Dignam who was an amazing amai research is in Sweden so it's not perfect okay but these are the people that we are hoping to be trained you know to be producing our algorithms for us and it's so limited so how did we get to this stage and why is it getting worse in all of the innovations and cold clubs and projects that we're doing to try and encourage women into tech actually we're getting worse we're in the other STEM fields it's getting better and this is a Western issue as well in the UK in the States in particular it doesn't happen elsewhere there's an interesting question to look at and what happened around about this time 1984 what was special about them loads of different reasons but before that just before I talk about 1984 just a really important to remember originally when coding started and programming was invented it was a female field 30 to 50% with females a fantastic history women in coding so from Ada Lovelace who wrote the very first computer program to Grace Hopper who developed and COBOL and the type of programming language that we use today so we don't have to learn to machine code to the people who did the Colossus programmers in Bletchley Park in the Second World War to the first electronic computer ENIAC female coders interestingly the ENIAC ladies were not invited to the to do a celebratory meal when it was actually actually produced they were left out the actual programmers they did not invite imagine that happening nowadays and it was sold as the computer girls you know this lovely machine great thing it was in a really exciting female who's female but then something happened suddenly it got to the point of going hey this this has taken off this programming computer business this is turning into quite a lucrative profession so the men moved in as the respected need for programmers rose and the pay the men became more interested in the field and then they started to redefine the field rather than this lowly machine grade thing that you couldn't get promoted from suddenly became an executive level career and to boost the fields prestige they started having professional associations pushing the educational requirements that benefited men over women and the skills that men at school might have learned and discouraging the hiring of women it was active and certainly within the recruitment within the UK and I recommend a maori hicks book to look at how it was a deliberate decision by the UK civil service and how they were going to develop the next white heat of technology they started to introduce aptitude tests and with job candidates so this is in the 60s and 70s ranking candidates on masculinized skills deliberately and ours as the tests were circulated amongst male only group fraternities so this is how women were forced out of it and they started to use personality profiles because to program two psychologists said that it was this particular trait who didn't lack antisocial tendencies they were creating this nerd image even then and they're disinterested in people it was fake oh I'm going back sorry and 1984 was when the game boys and Nintendo consoles came out and they started marking tick marketing it to men and boys so this is how it got changed and we haven't got Betsey yet and about 45% of women is set fields but particularly tech will leave within the first year because it's a hostile environment still because that's how the field has been built we are still getting issues like women can't do Tech and then currently there's a big hoo-ha going around in the UK because there was a letter in the times from the CERN physicist who said women can't do physics and they'd gave him that that push and so we need to change the culture the culture isn't built so we need to change how code is taught in schools how we recruit university students how we recruited to jobs which is what you're talking about change the workplace culture change the marketing and imaging of tech and really just cut the we need to be honest that this is a deliberately hostile environment to to women and we need to create the environment where it becomes a requirement that we get we stop this attitude so I'm working on introducing regulation for algorithms based upon GDP our algorithmic impact assessments algorithm audits and certification and then AI fairness stamps and this will require diverse and inclusive teams in the development process and so again because I'm run out of time now so I won't spend too much long on this but this is what we're going to be coming towards this is where we will will go so if you're introducing AI into your business be thinking you can't just introduce and think it's a cure-all you're going to be expected to have some level of ethical production going on so this is AI now example so in an algorithmic impact assessment AI order take the certification citizens panels regulation and legislation is going to come forward and they're all going to be used in utilizing the gdpr as well this is what I'm working with in the UK and with the government at the moment to see how we can bring that in so yeh gdpr thank you for that and various organizations such as I Triple E are working on this to develop the standards which I work on an algorithmic bias and we're talking about diversity so with my network that I've started women leading in AI so what we're campaigning for is building requirement to diverse teams and ethics panels for AI certification fairness certification and trust marks will be given a competitive edge so a company that says well I'm developing this algorithm and it's got a trust mark so you can trust me I as a counselor I'm going to procure that algorithm over one that hasn't gone through it because it won't have that endemic bias built into it they'll have tried to work it out and that would be a profit incentive for companies and hopefully what we will do by doing that prophecy profit incentive will have fair algorithms but maybe we force back the change in the tech community in the tech culture that is less hostile to women because now they're going to need us back because it's going to cost them if they don't have us ok and I'm gonna leave it there cuz I'm way over run and thank you for your attention [Applause]