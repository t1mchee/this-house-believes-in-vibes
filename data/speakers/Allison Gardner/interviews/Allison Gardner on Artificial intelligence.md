Speaker 1

I'm Ben Horton, and you're listening to a summer special episode of Undercurrents, the Chatham House podcast.

Hello and welcome back to this, the second special episode we've got for you over our summer break, where we're bringing you a couple of interviews that we recorded before we all went off on holiday sunning ourselves on lovely European beaches. This interview is with Alison Gardner, who is a lecturer at the University of Kiel and is a world expert on artificial intelligence. We spoke about AI, the rise of these new technologies and... whether regulators have the ability to ensure that these technologies properly represent society as it exists and does not discriminate against minorities and women in particular. This interview was recorded back in July at Chatham House's International Policy Forum on Gender and Growth, which is run by our Global Economy and Finance Department. And it was a really, really interesting conversation. Hope you enjoy it.

OK, so now I'm joined by Dr Alison Gardner, who is lecturer in data science at Keele University. And we're here to talk today about artificial intelligence and how it's potentially going to change the world. Something terrifying that I feel I ought to understand more about. So I'm very glad that Alison's here. Thank you so much for coming in.

Speaker 2

Thank you very much for inviting me. So sorry, this

Speaker 1

is a horrendously broad question to begin with. Artificial intelligence, what is its transformative potential? How is it going to be changing our world

Speaker 2

in the near future? The big argument is that it's the fourth industrial revolution and it will revolutionise the world of work. It will make the world of work quicker and more efficient and therefore we won't need to work. The sales pitch is we won't need to work five days a week. We can work less times and have more time for leisure. So this is the sales pitch for AI because we can do more very quickly. So instead of a lawyer or trainee lawyer sitting down and going through loads and loads and loads and loads of past law cases, we could use an AI to read through all of those and flag up relevant articles and it's time saving. That's the sales pitch. And that's why it can be very useful in many, many areas. And another reason why people are very keen on it is they feel that it will allow, it would remove human bias. Because if you're using... an AI people think it's objective and they think it is accurate and therefore you will get better results and that is the sort of thinking around it when people sort of claim of the virtues of AI. But it can also, done in the correct way, democratise the world. It can give agency to people, access. There's some good discussions earlier on about how people can have digital wallets and digital identity for refugees so that they can't carry documents with them, but they can keep things in the cloud. So that's less AI and more digital. But there are benefits too, but we have to utilise it in the correct way.

Many, many types of technology. It might be developed for good, but it can also be abused. And so it's how we implement the systems that's very important rather than the technology itself, in my view. I mean, it sounded

Speaker 1

very cushy. I thought, I'll put my feet up. It's all going to be good. We can relax. So what is the...

Speaker 2

What's the small print behind that sales pitch? I wonder about the logic behind this because, you know, do you really think companies are going to pay people the equivalent of five days as they do now if somebody's only there for three days? People think, oh, yes, I'll earn the same, but I'll have more free time. They won't earn the same. And one of the kickbacks for this, we're going away from AI a bit, but we're talking about the changing world of work, which AI impacts, is that what you're getting is the gig economy. zero hour contracts you're getting less secure work as well as many key pieces of work and labor markets being threatened by the implementation of AI and those jobs disappearing. And again, you get this big argument, oh, well, we will create more jobs. There'll just be new types of jobs via AI than the old type of jobs. And that may well be, but what do we do in the transition phase? And what do we do about those people? When we've seen this, when we've had great technological disruption, that you can have a whole swathes of people, whole communities that suddenly become disenfranchised. and they haven't retrained and they haven't accessed the new world of work. And this is where we have some problems. So the impact of AI, again, has much wider implications to the future as to how we address our economy, how we address our world of work. And whenever you have something like this, the people who are usually most badly affected are women. So again, so this is my area with the idea of gender and diversity within the field of AI, but it's also its impact will also impact disproportionately on women as well. OK,

Speaker 1

well, let's turn to that aspect of AI then. So why is it that you think that there is this potentially negative impact on women?

Speaker 2

Okay. Well, in fact, it might threaten traditionally female roles, but one of the things with AI, and going back into the technology side of things, is how AI is developed. And we have a real issue with machine bias at the moment within the field of AI that can actually... embed discrimination within our systems so if you think about an example be if we use artificial intelligence or machine learning there's slightly two different areas but very related in determining mortgage applications or determining loan applications and that system is actually a bias where it will disproportionately reject female applicants because they are usually coming from a less traditional working background with lower pay or with job gaps or so on and so forth, or haven't had bank accounts because they've been in a traditional role where they haven't. been accessing with that one so that system then will be trained on past financial data of who's paid and stuff and therefore it will just re reinforce current biases impeded by humans and it becomes embedded within our systems and we don't question it same with recidivism algorithms and predictive policing algorithms are very

in terms of that they can be discriminatory against people of colour. And so there's some classic examples such as Compass, which is a sort of de facto regular example, which was... It was meant to be a post-sentencing tool for judges to determine, I think, parole. But it started becoming used as a pre-sentencing tool. So when you're looking at whether somebody should go into prison or whether they'd be released on bail or put into a different type of community programme, this algorithm would predict the likelihood of that person re-offending. But it's built upon... data which is biased um it's disproportionately has black people within that because they've been you know societal issues have meant they've been disproportionately targeted and and and and um imprisoned for crime and so that's what it's built so it becomes embedded and if you do if you use these systems unquestioningly we're going to have discrimination

hardwired into our society.

Another example would be a classic one of algorithms that can detect skin cancer. So they're fantastic. So you take a picture of a questionable mole and it will give you an idea of the probability of that being cancerous and what you want to do. However, if that's been built on data that has come predominantly from images of white skin. Yeah. Because, you know, the environment where that data has been collected... they are actually less likely to predict cancer with people with black skin. So if you can imagine, therefore, implementing that system within the NHS or within yourself, and it's fantastic, you're going to find that people of colour are going to have higher rates of cancer because it's not going to be picked up. So it's this very subtle discrimination that can occur, and it's inbuilt within, usually biased data sets.

Gender Shades project, Joy Bull and Winnie and Timnick Ebruj from MIT did a fantastic project where they looked at facial recognition. from images and whether the algorithm could correctly identify people as male or female. And they found that the error rates for black females incorrectly identified at levels of 34%. White male, the error rate was only 0.03% or something like that, because it looked at the data sets was built predominantly 70 to 80% on white males. And so if you're looking at using facial recognition for whatever reason, and there's a lot of issues with facial recognition going on all over the world at the moment as to whether that's an infringement on privacy and whether it's accurate in the first place.

That's questionable. That's going to have bias against women and bias against people of colour. So there's some real issues there.

Speaker 1

How does one go about cleaning up those data sets? Is that something that we can do? possible or should we just be moving away from relying on these to begin with that is a superb

Speaker 2

question so it goes away now from the high level discussions that everybody's having so like right okay we've identified this as an issue right so what are we physically going to do about it so there are a number of things we can do so um so a one of the problems with these biased data set that you have these huge huge You know, what you think in retrospect is an obvious mistake and how on earth can bright, intelligent people... put through a system so obviously biased, you know, what's going on? And it's not intentional, but it's because the development groups are so homogenous that they don't see the obvious. You know, when you're a white male, you're not going to look at a white male data set and see something odd. So it goes through unquestionably. So one thing we need to do is increase the women in AI, but specifically working within development and not peripheral, but actually in development so that you have have these mixed teams that can actually come up with interesting problems to solve that might not be thought of before, but also to address the whole way through that problem of these errors. So when you're developing a system, you'll start with your data set. So is it biased? So can you start doing various de-biasing? processes within that data set to try and reduce that level so there's various things you can do depending on what your process is you then have to do data cleaning the data normalization the pre-processing that you have to do and even with pre-processing there's questions you ask about whether how you It depends on the question you form, how you would say, if you had a yes, no answer, do I turn that into a yes, into a one or a zero? What is my question? So it's quite subtle, actually. You then have your feature selection. So which features do I put through the algorithm? So you go through a whole load of feature selection. So what values am I putting on? What features I'm going to include?

algorithms that do that for you but sometimes it's statistical but actually there is a little bit of human eye judgment in there and again so buyers could inject there in some machine learning algorithms you set the weighting so which features are more important than the others so you weight more heavily so again there's decisions making there so this is not an auto you know this is not a process that does not have human decision making all the way through it incredibly hands-off yeah so you send and the big thing put it through the algorithm which can augment any bias that's already there coming out of the other end of the system you might have if it's a prediction or a recommendation or a classification system so it might be that this information comes out at a various probability level. So let's just say, for example, let's stick with recidivism algorithm. So this person is likely to re-offend at a probability of 70%. So now, where do I put my threshold? Do I put my threshold as, well, it's 50%, so anybody over 50%, they're likely to re-offend, we put them into prison. And how does that affect my accuracy of prediction? How many false positives would I have? So that's people who would not re-offend, but I'm putting into prison anyway, so the harm there. How many false negatives would I have? So how many violent offenders who will re-offend am I letting? go free which is the biggest risk which is the biggest harm where's my value judgment there and where do I set my threshold for the probabilities at 50 60 70 80 how does that affect my accuracy and and how do I implement that system so again you can see how this human decision making and that can be particularly at that point based upon value judgments and opinion. So this broad spectrum of diverse developers is really important. And finally, what's really important is... A lot of these systems are implemented and the get-out clause I'm seeing quite a bit, some of them with some government systems that they're using, where they say, well, it doesn't matter if it's not quite perfect because we have a human at the end who is making the final decision and it's just feeding into their decision-making process. This is from GDPR, Article 22, I think,

or is it 15? I always get more mixed up. But this idea of the human decision... I mean, you're not solely processed by an algorithm. The problem with that is that people default to the algorithmic decision. And so it's a bit of a human in the loop hole, as I call it, rather than a human in the loop. And it depends on the training level of that person as to how much weight they give to the algorithmic decision or not. So the more... experienced you are and the more knowledgeable you are the more cynical you tend to be as research has found so there is an issue there that if you have um a lower skilled you know you know workers they will default to the algorithm and there's a hidden issue sorry a bit of a passion issue again Findings are coming across that the utilisation of these algorithms can de-skill our workforce because they depend upon this. Their decision-making skills of themselves can actually be reduced and it is a worry. And of course, corporations or public sectors that are strapped for cash, if they can do things with fewer people quicker and at a lower wage level, they will. And so we start losing this expertise and this human touch.

a whole way along the chain and all the issues can exist so how do we solve it? Developing really good quality standards so that we can start working at how when if you're developing a system, what is the best process and protocols to work through to make sure that you do bias. So I work on IEEE P7003 algorithmic bias standard. And we're sort of instilling that of how you do stakeholder analysis, how you do algorithmic impact assessments, how you should engage with all the stakeholders, have diverse teams, as well as how you would look. Look at measuring and de-biasing as you go along and doing the fairness metrics to check for that bias on the outcome. And again, ISO are doing the same. So these standards are really important because any regulation that people are talking about, we need to regulate AI.

they will possibly default to the standards. And so it is actually at the standard level we need to keep things going. And then education as well. And education is always core for everything. Absolutely. Yeah, yeah, yeah.

Speaker 1

Just going back to the idea about the sort of skills that people need. I mean, the way you were talking about the sort of decisions that have to be made about what to include and what not to include, particularly when you're coming back to the data set, it sounds almost like the training that you need is something like... you need far more sort of ethics involved, right? It's completely about kind of moral decisions and things. It's not necessarily about, or it's about technical expertise, but also about these kind of sort of softer things. Do you think that the industry is acknowledging this? Do you think that... they're going to be recruiting a different kind of person?

Speaker 2

I think the industry are calling for a broader type of person. History has defined, based on a research finding many years ago in the 60s, I think, from two psychologists saying, oh, the best type of programmer is male with poor social skills. And it basically constructed the image of the nerd, and this is what we've created. That's a stereotype in itself. It is a stereotype, but it was a created stereotype, to be truthful. And the field has... developed to sort of you know in that that in that frame um but you'll you'll hear that no we need people with broader skills um and i hate the term soft skills because i think they're the complex skills because they're the skills ai can't replicate um rather than the hard skills um which i you know so there's there's there's hints in the genderization of language there you think hard skills and soft skills and masculine and feminine but anyway um They are really calling out for it. And I think universities are beginning to wake up to it a bit slowly. We do need to have computer scientists being trained and taught within colleges and universities where ethics is embedded across the board and the need for diversity as well across the board of the subject areas in all modules, not as a separate. elective module that they can do so but but all the way through that this is something that is this totally core to the subject area and a key thing um as i work within the field of data science but ai as well is it is actually multidisciplinary what you've just talked about is these questions that you're asking are actually You know, you need the philosophers, you need the psychologists, you need the social scientists, you need the humanities within there. You know, all with those skills that they have as to what questions need to be asked, how you ask them, and what decisions do you make. And it's that multidisciplinary approach that I think is going to be embedded. So, yes, I want to bring ethics. into computer science courses. So Harvard, for example, do this called embedded ethics. I'm trying to implement it the same at Keele with our degree apprenticeship programme. But we should also leap out. of the computer science department, into other departments within universities. So I'd like to see data science or AI for law modules within law courses, within philosophy courses. So to train this wider network up, because this is one of the most multidisciplinary fields you can come across. So that's a superb question. And I think we're beginning to have that discussion now, but not quickly enough.

Speaker 1

Now, I'd just like to move away slightly from sort of square on AI issue and talk maybe a bit more about the technology industry. Now, I was reading a report recently about the gender balance in the tech sector and how it's massively, overwhelmingly male dominated. But that wasn't always the case, was it? I think historically, actually, some of the sort of... pioneering key figures in computer science and tech were women. Why do you think there's been this shift?

Speaker 2

There is... It was deliberate. It was designed. There's a great book by Mar Hicks, which is, and I can't remember the name of it, so sorry, Mar Hicks, but do Google it, looked at how the field of computer science was deliberately re-engineered to be male-orientated. So originally, yes, the founders... Well, mainly women. We walk past the house of Ada Lovelace. I shall have my selfie outside of that when I leave.

And... And it was originally seen as a secretarial role. It was a machine-grade position with the 50s and 60s. So it was very much 50-50 and female. But, of course, that was limited to those women. They couldn't get promoted. It was assumed that when they got married they'd leave because that was the culture then within the Western world, within the UK. So I'm talking from a UK perspective. And what happened was when it started to realise that computer science and programming was going to be a very... valuable field. As often happens, that's when the men moved in. They redefined the field as something that wasn't machine grade, as an executive grade position. And you actually had things like the machine grade programmers were teaching these men that were coming in that were selected via personality profiles and that were designed to prefer male characteristics, as was then, or male qualifications that men were more likely to do than females. And train them up and then they'd become their... her boss there's a good story at the beginning of that one and so it developed um and women were eventually pushed out and it was created deliberately engineered to be this male field and women were prevented from from continuing through it i mean there are a few that barge through those barriers. But as a rule, that was the case. And then we started to readdress that problem, as we did with all STEM subjects. And then there's a wonderful graph that came out with ACM and Wired, some research that they did, where around about 1984, where all the other STEM subjects were still increasing in their gender parity, suddenly computer science started going backwards and going down again. And so to the point now where we're looking at within... AI, for example, about 12%, 9 to 12% of actual developers within AI are women. And why has that happened?

You know, why is that happening? Women are perfectly capable. Clearly they were because it used to be a female field. So all this business of this, this, this, I don't know, this. wild west attitude and this kudos giving to maths and this over complication of things made it quite an unfriendly place for women and it was just constructed so they've just left the actual and culture and the environment is so unfriendly to women that even if they enter it they leave pretty quickly because A, they lose confidence and B, it's just not a pleasant place to be. And so we're losing them from the field as well. So we're not getting them in. And when we do get them in, we're losing them. And it is a real issue. And it's got nothing to do with ability. And it actually has got nothing to do with confidence because, again, research shows that... Men and women start with the same level of confidence and enthusiasm, but it just gets bashed out of the women, very suckily sometimes, and they give up and go.

And one story, just recently, this is 2019. OK, so just 2019. We're not 1950 or 1970. This is 2019. And I had a friend who returned to tech after a career break and she, you know, upskilled the programming again and went back into tech. And, you know, she was ever so welcome. They were really enthusiastic to welcome her and they wanted to reassure her what a fantastic workplace she was coming into because they had a table football machine and they had the meetings in the local lap dancing club.

true story and you think really you know so so we're still in this this this world of ping pong tables and bean bags on the floor and you know culture which needs to change really okay so obviously

Speaker 1

that's a pretty stark picture that you've painted there um do you think that has to be addressed before we can even get on to addressing any of the more kind of technical sort of scary data stuff we were talking about earlier

Speaker 2

No, because I think we can actually use the problems that have been highlighted with the scary stuff because that's really thrown up quite a lot of interest. It's got politicians interested, it's got high-level people interested in it as the impact and the threats to this, so we can actually utilise that. that focus um to start working backwards i mean we've been people have been working on this issue for years and years and years you know we we've i run a women leading in ai which is a women's network um it's not one where you do training courses for women and learn to be confident because we believe women are confident and capable already this is just to give a power of a wide connection of women. So it's not just me you're talking to, you're talking to a whole bunch of very influential women. And that comes, you know, and that helps me in my access to my place in my places like this, like Chatham House.

So we can use that problem to try and address this other problem.

For years, as I said, we've been doing things like mentoring schemes, we've been doing things like girl code clubs. They haven't made a difference because... We haven't changed the culture. So if we develop regulations and standards that require diverse development teams, and if within those requirements you are going to procure a system and you need to show that that system has been developed ethically, you want certification, for example, or a trust mark. There's conversations about that now. And to gain that, you have to show, A, you've got diversity within your team development. been ethically developed that now gives a profit incentive to those companies once you hit the pockets then they're going to start going we actually need these teams this is going to affect us now and I'm sorry you know we're going to have to have these diverse teams and we're going to have to look at why there is this culture and if there is unconscious bias within behaviour in the workplace. When women stand up and say, I'm sorry, but that's not sensible behaviour, don't speak to me about it, it's going to have equal voice. They can't be pushed away with, that's just banter, we were just joking, or is it time of the month comments, which believe it or not we still get, that they have equal voice because it's going to be important. So I'm hoping that this will be the nudge. Regulation and standards and certification will be the nudge that we finally need to get the men and the people, you know, the CEOs to wake up and realize that they have to actually do something like that. And that diversity isn't just a women's issue. It's actually a very, very serious societal issue and company issue as well. Absolutely.

Speaker 1

And just lastly. Are you optimistic about this? Do you think this is changing? Do you think we're going to move in the right direction?

Speaker 2

It's a race. Because the tech is developing so fast, faster than we can regulate. And there is a lot of lobbying. There's a lot of interested, huge multinational corporations wanting to control the development of the standards, development of the regulation and to get this out there quickly. So there's a big groundswell that's happening. So you've seen recently that there's been calls for a moratorium on facial recognition technology because we just don't think we're there. yet and this is what people must understand these algorithms aren't as accurate or as objective as people think they are and this is really important so this idea of this moratorium so that we can allow regulation standards to catch up before there's real harm done or because before these systems are so embedded it's going to be hard to retrofit them so um am i am i hopeful um i i am inspired by the fact that there is a lot of interest, that the discussions within standards are going in the right direction, but I am conscious of the power. The interests of the multinational organisations and the high level discussions within governments that we're not aware of. So it's a battle. I actually really think it's a frontline battle. I actually, in my mind, I call it a fight. You know, this is the race to get good AI for the benefit of humans or AI that will just entrench capitalism and profit, as we have been for many years, over and above everything else. cause real harm so I'm optimistic because I'm in it and I'm going to fight tooth and nail for it and I know lots of good people are doing the same Alison

Speaker 1

Gardner thanks so much for joining us

Speaker 2

thank you