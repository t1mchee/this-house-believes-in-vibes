# Dr Fazl Barez

## Biography

Dr Fazl Barez is a Senior Research Fellow at the University of Oxford, leading
research on Technical AI Safety, Interpretability, and Governance within the
AI Governance Initiative. He is affiliated with Cambridge's Centre for the Study
of Existential Risk (CSER) and has worked with Anthropic's Alignment team on
research investigating deception in language models. His research has shaped
industry practice and government policy on AI safety.

## Key Positions

- Strongly concerned about empirical evidence of deception and safety failures in LLMs
- Co-authored "Sleeper Agents" paper showing LLMs can be trained to behave deceptively
  and standard safety training fails to remove such behaviour
- Research shows LLMs can relearn removed dangerous concepts after pruning
- Demonstrates models can generalise from mild gaming to severe reward-tampering
- Likely to argue AI should not be granted autonomous decision-making power over
  human life until failure modes are tightly controlled, if ever
- Advocates for stronger AI governance and identifies "AI systems of concern"

## Affiliations

- Senior Research Fellow, University of Oxford
- Centre for the Study of Existential Risk (CSER), Cambridge
- Former Anthropic Alignment team collaborator
- Panels at ICLR 2024, ICML 2024, Technical AI Safety Conference (Japan)
