"""
Ensemble debate exploration with epoch-based student learning.

Runs multiple debates organised into EPOCHS.  After each epoch, a coaching
memo is generated for the two student speakers (only), who incorporate the
feedback into subsequent epochs.  Expert speakers always give their natural
best arguments ‚Äî they receive no coaching.

Variability comes from:
  1. RETRIEVAL VARIATION ‚Äî each run sees a random subsample of passages.
  2. LLM STOCHASTICITY ‚Äî temperature-driven sampling.
  3. STUDENT LEARNING ‚Äî students improve across epochs via coaching memos.

Output is structured JSON for easy visualization:
  - run_N_data.json   per-run full DivisionResult
  - summary.json      enriched per-run + per-epoch statistics
  - epochs.json       learning trajectory and coaching memos
  - synthesis.txt     cross-run meta-analysis (o3)

Usage:
    python -m src.ensemble                            # 3 epochs √ó 4 runs
    python -m src.ensemble --epochs 4 --runs-per-epoch 5
    python -m src.ensemble --deep                     # o3 for speakers
    python -m src.ensemble --no-synth                 # skip synthesis
    python -m src.ensemble --seed 42                  # reproducible sampling
"""

from __future__ import annotations

import argparse
import asyncio
import json
import logging
import random
import sys
import time
import warnings
from datetime import datetime
from pathlib import Path

from langchain_openai import ChatOpenAI
from rich.console import Console
from rich.panel import Panel
from rich.progress import BarColumn, MofNCompleteColumn, Progress, SpinnerColumn, TextColumn, TimeElapsedColumn, TimeRemainingColumn
from rich.table import Table

import src.config as cfg
from src.coaching import STUDENT_IDS, generate_coaching_memo
from src.corpus.ingest import build_passage_pool
from src.graph import DebateGraphState, build_debate_graph
from src.models import CoachingMemo, DivisionResult, Side, SpeakerProfile, StyleProfile
from src.run import (
    MOTION,
    PROP_SPEAKERS,
    OPP_SPEAKERS,
    run_phase_0,
    save_transcript,
)

# Suppress ALL Pydantic serialization warnings ‚Äî they are cosmetic and flood
# the terminal buffer, hiding real errors.
warnings.filterwarnings("ignore", category=UserWarning, module="pydantic")

log = logging.getLogger("ensemble")


# ---------------------------------------------------------------------------
# Passage pool management
# ---------------------------------------------------------------------------

async def build_all_passage_pools(
    speakers,
    motion: str,
    console: Console,
) -> dict[str, list[str]]:
    """
    Build a large, diverse passage pool for each speaker.

    Uses multiple retrieval queries (generated by gpt-4o) to surface
    different facets of each speaker's corpus.  The pools are built once
    and then randomly subsampled per-run.

    Returns:
        Dict mapping speaker_id ‚Üí list of unique passages (typically 15-25).
    """
    pools: dict[str, list[str]] = {}
    seen_collections: set[str] = set()

    for speaker in speakers:
        # Shared-corpus speakers (e.g. both student speakers) build one pool
        if speaker.corpus_collection in seen_collections:
            # Reuse the pool already built for this collection
            donor_id = next(
                sid for sid, p in pools.items()
                if any(
                    s.corpus_collection == speaker.corpus_collection
                    for s in speakers
                    if s.id == sid
                )
            )
            pools[speaker.id] = pools[donor_id]
            console.print(
                f"  üìé {speaker.name} ({speaker.id}) ‚Äî reusing pool "
                f"from shared corpus ({len(pools[donor_id])} passages)"
            )
            continue

        console.print(f"  üîç Building passage pool for {speaker.name}‚Ä¶", end=" ")
        pool = await build_passage_pool(speaker, motion)
        pools[speaker.id] = pool
        seen_collections.add(speaker.corpus_collection)
        console.print(f"[green]{len(pool)} passages[/green]")

    return pools


def subsample_passages(
    pools: dict[str, list[str]],
    k: int = 10,
) -> dict[str, list[str]]:
    """
    Randomly subsample k passages per speaker from their pool.

    If a speaker's pool has ‚â§ k passages, all are used (no randomness for
    that speaker ‚Äî their variation comes from temperature alone).
    """
    overrides: dict[str, list[str]] = {}
    for speaker_id, pool in pools.items():
        if len(pool) <= k:
            overrides[speaker_id] = list(pool)
        else:
            overrides[speaker_id] = random.sample(pool, k)
    return overrides


# ---------------------------------------------------------------------------
# Speaker order randomization
# ---------------------------------------------------------------------------

def shuffle_speaker_order(
    prop_speakers: list[SpeakerProfile],
    opp_speakers: list[SpeakerProfile],
) -> tuple[list[SpeakerProfile], list[SpeakerProfile]]:
    """
    Return shuffled copies of the speaker lists with updated speaking_position.

    The Cambridge Union alternation (Prop, Opp, Prop, Opp, Prop, Opp) is
    preserved, but WHICH speaker on each side speaks 1st/2nd/3rd is randomized.
    """
    shuffled_prop = random.sample(prop_speakers, len(prop_speakers))
    shuffled_opp = random.sample(opp_speakers, len(opp_speakers))

    # Update speaking_position to reflect the new order
    prop_out = [
        s.model_copy(update={"speaking_position": i + 1})
        for i, s in enumerate(shuffled_prop)
    ]
    opp_out = [
        s.model_copy(update={"speaking_position": i + 1})
        for i, s in enumerate(shuffled_opp)
    ]
    return prop_out, opp_out


# ---------------------------------------------------------------------------
# Structured JSON helpers
# ---------------------------------------------------------------------------

def _division_to_dict(division: DivisionResult | None) -> dict | None:
    """Serialize a DivisionResult to a JSON-safe dict."""
    if division is None:
        return None
    return json.loads(division.model_dump_json())


def _extract_per_speaker_scores(division: DivisionResult | None) -> list[dict] | None:
    """Extract per-speaker rubric scores as flat dicts for easy visualization."""
    if not division or not division.rubric:
        return None
    return [
        {
            "speaker_name": s.speaker_name,
            "side": s.side.value,
            "argument_strength": s.argument_strength,
            "rebuttal_quality": s.rebuttal_quality,
            "evidence_grounding": s.evidence_grounding,
            "rhetorical_effectiveness": s.rhetorical_effectiveness,
            "persona_fidelity": s.persona_fidelity,
            "overall": s.overall,
        }
        for s in division.rubric.scores
    ]


def save_run_data_json(
    result: dict,
    filepath: Path,
) -> None:
    """Save the full structured data for a single run as JSON."""
    data = {
        "run_number": result["run_number"],
        "epoch": result.get("epoch", 1),
        "speaking_order": result.get("speaking_order"),
        "winner": result.get("winner"),
        "margin": result.get("margin"),
        "ayes": result.get("ayes"),
        "noes": result.get("noes"),
        "confidence": result.get("confidence"),
        "rubric_winner": result.get("rubric_winner"),
        "annotation_winner": result.get("annotation_winner"),
        "annotation_prop_score": result.get("annotation_prop_score"),
        "annotation_opp_score": result.get("annotation_opp_score"),
        "engagement_winner": result.get("engagement_winner"),
        "engagement_prop_votes": result.get("engagement_prop_votes"),
        "engagement_opp_votes": result.get("engagement_opp_votes"),
        "engagement_pass_agreement": result.get("engagement_pass_agreement"),
        "struct_winner": result.get("struct_winner"),
        "word_counts": result.get("word_counts"),
        "n_pois": result.get("n_pois"),
        "n_accepted": result.get("n_accepted"),
        "per_speaker_scores": _extract_per_speaker_scores(result.get("division")),
        "division": _division_to_dict(result.get("division")),
    }
    filepath.write_text(json.dumps(data, indent=2), encoding="utf-8")


# ---------------------------------------------------------------------------
# Single run execution
# ---------------------------------------------------------------------------

async def run_single_variation(
    styles: dict[str, StyleProfile],
    passage_overrides: dict[str, list[str]],
    strategy_directives: dict[str, str],
    console: Console,
    ensemble_dir: Path,
    run_number: int,
    total_runs: int,
    epoch: int = 1,
) -> dict:
    """Execute a single debate with the given passage subsample and coaching."""
    # Randomize which speaker on each side goes 1st/2nd/3rd
    prop_shuffled, opp_shuffled = shuffle_speaker_order(PROP_SPEAKERS, OPP_SPEAKERS)

    prop_order = ", ".join(s.name.split("(")[-1].rstrip(")") if "(" in s.name else s.name.split()[-1]
                           for s in prop_shuffled)
    opp_order = ", ".join(s.name.split("(")[-1].rstrip(")") if "(" in s.name else s.name.split()[-1]
                          for s in opp_shuffled)
    console.print(
        f"\n[bold yellow]‚îÄ‚îÄ Epoch {epoch} ¬∑ Run {run_number}/{total_runs} "
        f"‚îÄ‚îÄ[/bold yellow]"
    )
    console.print(f"  [dim]Prop order: {prop_order}  |  Opp order: {opp_order}[/dim]")

    graph = build_debate_graph()

    initial_state: DebateGraphState = {
        "motion": MOTION,
        "prop_speakers": prop_shuffled,
        "opp_speakers": opp_shuffled,
        "styles": styles,
        "strategy_directives": strategy_directives,
        "passage_overrides": passage_overrides,
        "speaker_data": {},
        "speeches": [],
        "pois": [],
        "current_speech_index": 0,
        "definitions": None,
        "contestation": None,
        "definitions_context": "",
        "division": None,
        "verdict_raw": "",
        "iteration": 1,
        "history": [],
        "should_terminate": False,
    }

    result = await graph.ainvoke(initial_state)

    # --- Collect metrics ---
    word_counts = {s.speaker_name: s.word_count for s in result["speeches"]}
    n_pois = len(result["pois"])
    n_accepted = sum(1 for p in result["pois"] if p.accepted)

    division = result.get("division")
    verdict_raw = result.get("verdict_raw", "")

    winner = margin = None
    ayes = noes = confidence = None
    rubric_winner = struct_winner = annotation_winner = None
    annotation_prop_score = annotation_opp_score = None
    engagement_winner = None
    engagement_prop_votes = engagement_opp_votes = None
    engagement_pass_agreement = None
    if division:
        winner = "PROPOSITION" if division.winner == Side.PROPOSITION else "OPPOSITION"
        margin = division.margin
        ayes = division.ayes
        noes = division.noes
        confidence = division.confidence
        if division.rubric:
            rubric_winner = division.rubric.rubric_winner.value.upper()
        if division.annotation:
            annotation_winner = division.annotation.winner.value.upper()
            annotation_prop_score = division.annotation.prop_score
            annotation_opp_score = division.annotation.opp_score
        if division.engagement:
            engagement_winner = division.engagement.winner.value.upper()
            engagement_prop_votes = division.engagement.prop_votes
            engagement_opp_votes = division.engagement.opp_votes
            engagement_pass_agreement = division.engagement.pass_agreement
        if division.argument_audit:
            struct_winner = division.argument_audit.structural_winner.value.upper()

    # --- Print summary ---
    for speech in result["speeches"]:
        side = "PROP" if speech.side == Side.PROPOSITION else "OPP"
        console.print(f"  {speech.speaker_name} ({side}): {speech.word_count} words")

    if winner:
        w_style = "green" if winner == "PROPOSITION" else "red"
        layers = f"Engagement={engagement_winner or '?'}"
        if engagement_prop_votes is not None:
            layers += f" ({engagement_prop_votes}-{engagement_opp_votes}"
            layers += f" {'‚úìagree' if engagement_pass_agreement else '‚ö†disagree'})"
        if annotation_winner:
            layers += f" Annotation={annotation_winner}"
        if rubric_winner:
            layers += f" Rubric={rubric_winner}"
        if struct_winner:
            layers += f" Structure={struct_winner}"
        console.print(
            f"  Verdict: [{w_style}]{winner}[/{w_style}] ({margin})"
        )
        console.print(f"  Layers: {layers}")
    else:
        console.print("  Verdict: [dim]parsing failed ‚Äî see raw verdict[/dim]")

    console.print(f"  POIs: {n_pois} offered, {n_accepted} accepted")

    # --- Save individual transcript (text) ---
    run_path = ensemble_dir / f"run_{run_number}.txt"
    save_transcript(result, console, filepath=run_path)

    # Capture the actual speaking order for this run
    speaking_order_names = []
    for i in range(3):
        speaking_order_names.append(prop_shuffled[i].name)
        speaking_order_names.append(opp_shuffled[i].name)

    run_result = {
        "run_number": run_number,
        "epoch": epoch,
        "speaking_order": speaking_order_names,
        "word_counts": word_counts,
        "n_pois": n_pois,
        "n_accepted": n_accepted,
        "winner": winner,
        "margin": margin,
        "ayes": ayes,
        "noes": noes,
        "confidence": confidence,
        "rubric_winner": rubric_winner,
        "annotation_winner": annotation_winner,
        "annotation_prop_score": annotation_prop_score,
        "annotation_opp_score": annotation_opp_score,
        "engagement_winner": engagement_winner,
        "engagement_prop_votes": engagement_prop_votes,
        "engagement_opp_votes": engagement_opp_votes,
        "engagement_pass_agreement": engagement_pass_agreement,
        "struct_winner": struct_winner,
        "verdict_raw": verdict_raw,
        "division": division,
    }

    # --- Save structured JSON per run ---
    json_path = ensemble_dir / f"run_{run_number}_data.json"
    save_run_data_json(run_result, json_path)

    return run_result


# ---------------------------------------------------------------------------
# Results table
# ---------------------------------------------------------------------------

def print_results_table(
    results: list[dict],
    console: Console,
    title: str = "Ensemble Results",
) -> None:
    """Print a rich table summarising run outcomes."""
    table = Table(title=title, show_header=True)
    table.add_column("Run", width=4)
    table.add_column("Ep", width=3)
    table.add_column("Engagement", width=12)
    table.add_column("Votes", width=7)
    table.add_column("Passes", width=6)
    table.add_column("Margin", width=9)
    table.add_column("Annot.", width=10)
    table.add_column("Rubric", width=10)
    table.add_column("Struct", width=10)
    table.add_column("Words", width=6)

    for r in results:
        winner = r.get("winner", "?")
        w_style = (
            "green" if winner == "PROPOSITION"
            else "red" if winner == "OPPOSITION"
            else "dim"
        )
        avg_words = (
            sum(r["word_counts"].values()) / max(len(r["word_counts"]), 1)
            if r["word_counts"] else 0
        )
        eng = r.get("engagement_winner", "?")
        eng_p = r.get("engagement_prop_votes")
        eng_o = r.get("engagement_opp_votes")
        votes_str = f"{eng_p}-{eng_o}" if eng_p is not None else "?"
        pass_agree = r.get("engagement_pass_agreement")
        pass_str = "‚úì" if pass_agree else ("‚ö†" if pass_agree is not None else "?")
        ann = r.get("annotation_winner", "?")
        rub = r.get("rubric_winner", "?")
        struct = r.get("struct_winner", "?")

        table.add_row(
            str(r["run_number"]),
            str(r.get("epoch", "?")),
            f"[{w_style}]{winner}[/{w_style}]",
            votes_str,
            pass_str,
            r.get("margin", "?"),
            ann or "?",
            rub or "?",
            struct or "?",
            f"{avg_words:.0f}",
        )

    console.print(table)


# ---------------------------------------------------------------------------
# Cross-run synthesis
# ---------------------------------------------------------------------------

async def synthesize_runs(
    results: list[dict],
    coaching_memos: list[CoachingMemo],
    console: Console,
    ensemble_dir: Path,
) -> str:
    """
    Feed all N verdicts to o3 for a rigorous cross-run meta-analysis.

    Identifies invariant clashes, swing arguments, speaker dynamics,
    outcome stability, and a best-estimate prediction.
    """
    console.print("\n[bold blue]‚ïê‚ïê ENSEMBLE SYNTHESIS (o3) ‚ïê‚ïê[/bold blue]\n")

    # Build the input for synthesis
    run_summaries = []
    for r in results:
        winner = r.get("winner", "Unknown")
        margin = r.get("margin", "unknown")
        ayes = r.get("ayes", "?")
        noes = r.get("noes", "?")
        confidence = r.get("confidence", "?")
        rubric_winner = r.get("rubric_winner", "?")
        struct_winner = r.get("struct_winner", "?")
        epoch = r.get("epoch", "?")
        # Truncate raw verdict to keep prompt manageable
        verdict_excerpt = (
            r["verdict_raw"][:4000] if r["verdict_raw"] else "(no verdict)"
        )
        annotation_winner = r.get("annotation_winner", "?")
        ann_p = r.get("annotation_prop_score")
        ann_o = r.get("annotation_opp_score")
        ann_scores = (
            f" (P:{ann_p:.0f} vs O:{ann_o:.0f})"
            if ann_p is not None else ""
        )
        eng_winner = r.get("engagement_winner", "?")
        eng_p = r.get("engagement_prop_votes")
        eng_o = r.get("engagement_opp_votes")
        eng_votes = f" ({eng_p}-{eng_o})" if eng_p is not None else ""
        eng_pass = r.get("engagement_pass_agreement")
        eng_pass_str = " passes-agree" if eng_pass else (" passes-DISAGREE" if eng_pass is not None else "")
        run_summaries.append(
            f"=== RUN {r['run_number']} (Epoch {epoch}) ===\n"
            f"Engagement Verdict (PRIMARY): {eng_winner}{eng_votes}{eng_pass_str}\n"
            f"Annotation Verdict: {annotation_winner}{ann_scores} ({margin})\n"
            f"Rubric Winner: {rubric_winner}\n"
            f"Structural Winner: {struct_winner}\n"
            f"Multi-layer analysis:\n{verdict_excerpt}\n"
        )

    all_summaries = "\n".join(run_summaries)

    # Coaching trajectory summary
    coaching_section = ""
    if coaching_memos:
        coaching_lines = []
        for memo in coaching_memos:
            coaching_lines.append(
                f"Epoch {memo.epoch}: mean student score={memo.mean_overall_score:.1f}/10"
            )
            if memo.actionable_advice:
                for adv in memo.actionable_advice[:3]:
                    coaching_lines.append(f"  ‚Ä¢ {adv}")
        coaching_section = f"""
STUDENT LEARNING TRAJECTORY:
The two student Proposition speakers received iterative coaching across epochs.
{chr(10).join(coaching_lines)}
"""

    synthesis_prompt = f"""You are a debate analyst studying {len(results)} simulations
of the same Cambridge Union exhibition debate, run across multiple epochs.

Motion: "{MOTION}"

Each simulation used the same six speakers and the same underlying corpus of
their real writings, speeches, and positions.  The sources of variation were:
  1. RETRIEVAL VARIATION ‚Äî each run saw a different random subsample of
     passages from each speaker's corpus.
  2. LLM STOCHASTICITY ‚Äî temperature-driven sampling produced different
     framings, structures, and rhetorical choices each time.
  3. STUDENT LEARNING ‚Äî the two student Proposition speakers received coaching
     between epochs, incorporating feedback from previous debates.

The four expert speakers (Shevlin, Barez, Gardner, Floudas) received NO coaching ‚Äî
they always made their natural best arguments.

Each debate was judged using a MULTI-LAYER system:
  Layer 1 (RUBRIC): Each speech scored independently on 5 dimensions
  Layer 2a (ANNOTATION): Every claim extracted and classified, every rebuttal
    mapped with effectiveness questions, then a MECHANICAL score computed
    arithmetically ‚Äî provides a transparency view of the argument flow.
  Layer 2b (ENGAGEMENT ‚Äî PRIMARY VERDICT): 3 LLM judges √ó 2 passes evaluate
    the debate with anonymized sides (Team A/B, no real names). The sides are
    SWAPPED between passes to detect position bias. Judges evaluate engagement
    quality, argument quality, rebuttal effectiveness, and coherent narrative.
  Layer 3 (STRUCTURE): Argument graph audit tracking claim survival
{coaching_section}
Here are the results of all {len(results)} runs:

{all_summaries}

Provide a rigorous cross-run analysis:

1. OUTCOME DISTRIBUTION
   - How many times did each side win?  What margins?
   - How stable or fragile is the overall result?
   - Is there a clear favourite, or is this genuinely too close to call?
   - Did outcomes shift across epochs as students improved?

2. INVARIANT CLASHES
   - Which fundamental tensions appeared in EVERY (or nearly every) run?

3. SWING ARGUMENTS
   - Which arguments, when they appeared, correlated with a side winning?

4. STUDENT LEARNING TRAJECTORY
   - Did the student speakers measurably improve across epochs?
   - What effect did their improvement have on overall outcomes?
   - What was their most effective adaptation?

5. SPEAKER DYNAMICS
   - Which speakers were consistently strong / weak across all runs?
   - Who is the "X-factor"?

6. CRITICAL DYNAMICS
   - What determines the outcome in close runs?

7. PREDICTION
   Based on all simulations:
   a) How would this debate MOST LIKELY unfold at the actual Cambridge Union?
   b) Which side would MOST LIKELY win, and with what confidence?
   c) What would the 2-3 decisive moments be?

8. OPTIMAL STRATEGY RECOMMENDATIONS
   For each side, what is their best strategy given all the data?

Be rigorous, specific, and reference evidence from the individual runs.
Give probabilities and confidence levels where possible."""

    # Use o3 for synthesis ‚Äî the one call worth paying for
    synthesis_llm = ChatOpenAI(
        model="o3",
        temperature=1,
        max_completion_tokens=16384,
    )

    with console.status(
        "Synthesizing across all runs with o3 ‚Äî this may take a minute‚Ä¶",
        spinner="dots",
    ):
        response = await synthesis_llm.ainvoke(synthesis_prompt)
        synthesis_text = response.content

    # Display
    console.print(Panel(
        synthesis_text,
        title="[bold]üî¨ ENSEMBLE SYNTHESIS[/bold]",
        border_style="blue",
        width=120,
    ))

    # Save synthesis report
    synthesis_path = ensemble_dir / "synthesis.txt"
    lines = [
        f"ENSEMBLE SYNTHESIS ‚Äî {len(results)} RUNS",
        f"Motion: {MOTION}",
        f"Generated: {datetime.now().isoformat()}",
        "=" * 80,
        "",
        synthesis_text,
    ]
    synthesis_path.write_text("\n".join(lines), encoding="utf-8")
    console.print(f"\n  üíæ Synthesis saved to [bold cyan]{synthesis_path}[/bold cyan]")

    return synthesis_text


# ---------------------------------------------------------------------------
# Save enriched summary + epochs JSON
# ---------------------------------------------------------------------------

def save_enriched_summary(
    results: list[dict],
    coaching_memos: list[CoachingMemo],
    ensemble_dir: Path,
    n_epochs: int,
    runs_per_epoch: int,
) -> None:
    """Save enriched summary.json and epochs.json for visualization."""

    # --- Enriched summary.json ---
    outcomes = []
    for r in results:
        entry = {
            "run_number": r["run_number"],
            "epoch": r.get("epoch", 1),
            "speaking_order": r.get("speaking_order"),
            "winner": r.get("winner"),
            "margin": r.get("margin"),
            "ayes": r.get("ayes"),
            "noes": r.get("noes"),
            "confidence": r.get("confidence"),
            "rubric_winner": r.get("rubric_winner"),
            "annotation_winner": r.get("annotation_winner"),
            "annotation_prop_score": r.get("annotation_prop_score"),
            "annotation_opp_score": r.get("annotation_opp_score"),
            "engagement_winner": r.get("engagement_winner"),
            "engagement_prop_votes": r.get("engagement_prop_votes"),
            "engagement_opp_votes": r.get("engagement_opp_votes"),
            "engagement_pass_agreement": r.get("engagement_pass_agreement"),
            "struct_winner": r.get("struct_winner"),
            "word_counts": r.get("word_counts"),
            "n_pois": r.get("n_pois"),
            "n_accepted": r.get("n_accepted"),
            "per_speaker_scores": _extract_per_speaker_scores(r.get("division")),
        }
        outcomes.append(entry)

    summary = {
        "motion": MOTION,
        "n_runs": len(results),
        "n_epochs": n_epochs,
        "runs_per_epoch": runs_per_epoch,
        "timestamp": datetime.now().isoformat(),
        "outcomes": outcomes,
        "prop_wins": sum(1 for r in results if r.get("winner") == "PROPOSITION"),
        "opp_wins": sum(1 for r in results if r.get("winner") == "OPPOSITION"),
        "prop_win_rate": (
            sum(1 for r in results if r.get("winner") == "PROPOSITION") / len(results)
            if results else 0
        ),
    }
    summary_path = ensemble_dir / "summary.json"
    summary_path.write_text(json.dumps(summary, indent=2), encoding="utf-8")

    # --- epochs.json ---
    epochs_data = []
    for epoch_num in range(1, n_epochs + 1):
        epoch_results = [r for r in results if r.get("epoch") == epoch_num]

        # Student scores for this epoch
        student_scores: list[float] = []
        all_speaker_scores: dict[str, list[float]] = {}
        for r in epoch_results:
            div = r.get("division")
            if div and div.rubric:
                for s in div.rubric.scores:
                    all_speaker_scores.setdefault(s.speaker_name, []).append(s.overall)
                    if s.speaker_name.startswith("Student"):
                        student_scores.append(s.overall)

        mean_student_score = (
            sum(student_scores) / len(student_scores) if student_scores else None
        )

        # Per-speaker means
        speaker_means = {
            name: sum(scores) / len(scores)
            for name, scores in all_speaker_scores.items()
        }

        prop_wins = sum(1 for r in epoch_results if r.get("winner") == "PROPOSITION")
        opp_wins = sum(1 for r in epoch_results if r.get("winner") == "OPPOSITION")

        # Coaching memo for this epoch (if available)
        memo_data = None
        matching_memos = [m for m in coaching_memos if m.epoch == epoch_num]
        if matching_memos:
            memo = matching_memos[0]
            memo_data = {
                "strengths": memo.strengths,
                "weaknesses": memo.weaknesses,
                "missed_rebuttals": memo.missed_rebuttals,
                "actionable_advice": memo.actionable_advice,
                "full_memo": memo.full_memo,
                "mean_overall_score": memo.mean_overall_score,
            }

        epochs_data.append({
            "epoch": epoch_num,
            "n_runs": len(epoch_results),
            "prop_wins": prop_wins,
            "opp_wins": opp_wins,
            "mean_student_score": mean_student_score,
            "speaker_means": speaker_means,
            "coaching_memo": memo_data,
        })

    epochs_path = ensemble_dir / "epochs.json"
    epochs_path.write_text(json.dumps(epochs_data, indent=2), encoding="utf-8")


# ---------------------------------------------------------------------------
# Main entry point
# ---------------------------------------------------------------------------

async def main():
    parser = argparse.ArgumentParser(
        description="Ensemble debate exploration with epoch-based student learning"
    )
    parser.add_argument(
        "--epochs", type=int, default=3,
        help="Number of learning epochs (default: 3)",
    )
    parser.add_argument(
        "--runs-per-epoch", type=int, default=4,
        help="Debates per epoch (default: 4)",
    )
    parser.add_argument(
        "--deep", action="store_true",
        help="Use o3 for all runs (expensive; default uses gpt-4o)",
    )
    parser.add_argument(
        "--no-synth", action="store_true",
        help="Skip the cross-run synthesis step",
    )
    parser.add_argument(
        "--no-coaching", action="store_true",
        help="Disable student coaching (all epochs are independent)",
    )
    parser.add_argument(
        "--seed", type=int, default=None,
        help="Random seed for reproducible passage subsampling",
    )
    parser.add_argument(
        "--sample-k", type=int, default=10,
        help="Number of passages to subsample per speaker per run (default: 10)",
    )
    args = parser.parse_args()

    if args.seed is not None:
        random.seed(args.seed)

    console = Console(width=120)
    total_runs = args.epochs * args.runs_per_epoch

    # --- Header ---
    mode_label = "Deep (o3)" if args.deep else "Exploration (gpt-4o)"
    coaching_label = "OFF" if args.no_coaching else f"{args.epochs} epochs"

    console.print()
    console.print(Panel(
        f"[bold]{MOTION}[/bold]\n\n"
        f"Epochs: {args.epochs}  ¬∑  Runs/epoch: {args.runs_per_epoch}  ¬∑  "
        f"Total: {total_runs}\n"
        f"Mode: {mode_label}  ¬∑  Coaching: {coaching_label}  ¬∑  "
        f"Passage sample: {args.sample_k}/pool\n"
        f"[dim]Variation: retrieval subsampling + LLM stochasticity"
        f"{' + student learning' if not args.no_coaching else ''}[/dim]",
        title="üî¨  ENSEMBLE EXPLORATION",
        border_style="magenta",
    ))

    # --- Switch to exploration models if not --deep ---
    # NOTE: Even in exploration mode we now keep o3 for speakers (user preference).
    # Only POI and verdict use cheaper models.
    if not args.deep:
        console.print(
            "\n[dim]Exploration mode: o3 for speeches, "
            "o4-mini for POIs, gpt-4o for verdict[/dim]"
        )
        cfg.POI_LLM = ChatOpenAI(
            model="o4-mini",
            temperature=1,
            max_completion_tokens=1024,
        )
        cfg.VERDICT_LLM = ChatOpenAI(
            model="gpt-4o",
            temperature=0.7,
            max_tokens=8192,
        )

    # --- Phase 0: Corpus ingestion (one-time, reuses ChromaDB) ---
    console.print("\n[bold blue]‚ïê‚ïê PHASE 0: CORPUS (one-time) ‚ïê‚ïê[/bold blue]\n")
    all_speakers = PROP_SPEAKERS + OPP_SPEAKERS
    styles = await run_phase_0(all_speakers, console)
    console.print(f"\n  üìö {len(styles)} speakers ready.\n")

    # --- Build passage pools (one-time, ~6 cheap gpt-4o calls) ---
    console.print("[bold blue]‚ïê‚ïê BUILDING PASSAGE POOLS ‚ïê‚ïê[/bold blue]\n")
    pools = await build_all_passage_pools(all_speakers, MOTION, console)

    pool_sizes = {sid: len(p) for sid, p in pools.items()}
    console.print(f"\n  Pool sizes: {pool_sizes}")
    console.print(
        f"  Subsampling {args.sample_k} passages per speaker per run ‚Üí "
        f"natural argument variation.\n"
    )

    # --- Output directory ---
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    ensemble_dir = Path(f"output/ensemble_{timestamp}")
    ensemble_dir.mkdir(parents=True, exist_ok=True)

    # Save pool metadata for reproducibility
    pool_meta = {
        sid: {"pool_size": len(p), "sample_k": args.sample_k}
        for sid, p in pools.items()
    }
    (ensemble_dir / "pool_metadata.json").write_text(
        json.dumps(pool_meta, indent=2), encoding="utf-8"
    )

    # --- Set up file logging so errors survive terminal-buffer overflow ---
    log_path = Path(f"output/ensemble_{timestamp}.log")
    log_path.parent.mkdir(parents=True, exist_ok=True)
    file_handler = logging.FileHandler(log_path, encoding="utf-8")
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(
        logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
    )
    logging.getLogger().addHandler(file_handler)
    logging.getLogger().setLevel(logging.INFO)
    console.print(f"  üìù Log file: [dim]{log_path}[/dim]\n")

    # --- Epoch loop with progress bar ---
    all_results: list[dict] = []
    coaching_memos: list[CoachingMemo] = []
    current_coaching_memo: CoachingMemo | None = None
    global_run_counter = 0
    n_failures = 0
    run_times: list[float] = []  # seconds per run, for ETA
    ensemble_start = time.time()

    # Overall progress bar (persists across epochs)
    progress = Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]{task.description}"),
        BarColumn(bar_width=40),
        MofNCompleteColumn(),
        TextColumn("¬∑"),
        TimeElapsedColumn(),
        TextColumn("¬∑"),
        TimeRemainingColumn(),
        console=console,
    )
    overall_task = progress.add_task("Overall", total=total_runs)
    progress.start()

    for epoch in range(1, args.epochs + 1):
        progress.update(
            overall_task,
            description=f"Epoch {epoch}/{args.epochs}",
        )
        console.print(
            f"\n[bold magenta]{'‚ïê' * 50}[/bold magenta]"
        )
        epoch_label = (
            "baseline ‚Äî no coaching" if epoch == 1 else "with coaching"
        )
        console.print(
            f"[bold magenta]  EPOCH {epoch}/{args.epochs}  "
            f"({epoch_label})[/bold magenta]"
        )
        console.print(
            f"[bold magenta]{'‚ïê' * 50}[/bold magenta]"
        )

        # Build strategy directives from coaching memo (students only)
        strategy_directives: dict[str, str] = {}
        if current_coaching_memo and not args.no_coaching:
            for sid in STUDENT_IDS:
                strategy_directives[sid] = current_coaching_memo.full_memo
            console.print(
                f"  üìã Coaching memo active (epoch {current_coaching_memo.epoch}, "
                f"mean student score: {current_coaching_memo.mean_overall_score:.1f}/10)"
            )

        # Run K debates in this epoch
        epoch_results: list[dict] = []
        for i in range(args.runs_per_epoch):
            global_run_counter += 1
            run_start = time.time()

            overrides = subsample_passages(pools, k=args.sample_k)
            try:
                result = await run_single_variation(
                    styles=styles,
                    passage_overrides=overrides,
                    strategy_directives=strategy_directives,
                    console=console,
                    ensemble_dir=ensemble_dir,
                    run_number=global_run_counter,
                    total_runs=total_runs,
                    epoch=epoch,
                )
                epoch_results.append(result)
                all_results.append(result)
            except Exception as e:
                log.exception("Run %d failed", global_run_counter)
                console.print(
                    f"  [bold red]‚ùå Run {global_run_counter} FAILED: "
                    f"{type(e).__name__}: {e}[/bold red]"
                )
                n_failures += 1

                # Back off after a failure to let rate limits reset.
                # Short runs (< 60s) almost always mean a rate-limit cascade.
                run_elapsed_so_far = time.time() - run_start
                if run_elapsed_so_far < 60:
                    backoff = 30  # seconds
                    console.print(
                        f"  [dim]‚è≥ Backing off {backoff}s to let rate limits reset‚Ä¶[/dim]"
                    )
                    await asyncio.sleep(backoff)

            # Update progress & timing
            run_elapsed = time.time() - run_start
            run_times.append(run_elapsed)
            progress.advance(overall_task)

            # Running tally
            prop_so_far = sum(1 for r in all_results if r.get("winner") == "PROPOSITION")
            opp_so_far = sum(1 for r in all_results if r.get("winner") == "OPPOSITION")
            avg_time = sum(run_times) / len(run_times)
            remaining = total_runs - global_run_counter
            eta_min = (avg_time * remaining) / 60
            console.print(
                f"  [dim]‚è±  Run took {run_elapsed / 60:.1f}min  ¬∑  "
                f"Tally: Prop {prop_so_far} ‚Äì Opp {opp_so_far}  ¬∑  "
                f"{n_failures} failed  ¬∑  "
                f"ETA: ~{eta_min:.0f}min for {remaining} remaining[/dim]"
            )

        # Epoch summary
        ep_prop = sum(1 for r in epoch_results if r.get("winner") == "PROPOSITION")
        ep_opp = sum(1 for r in epoch_results if r.get("winner") == "OPPOSITION")
        total_prop = sum(1 for r in all_results if r.get("winner") == "PROPOSITION")
        total_opp = sum(1 for r in all_results if r.get("winner") == "OPPOSITION")
        console.print(
            f"\n  üìä Epoch {epoch}: Prop {ep_prop} ‚Äì Opp {ep_opp}  |  "
            f"Overall: Prop {total_prop} ‚Äì Opp {total_opp}"
        )

        # Generate coaching memo (skip after last epoch)
        if epoch < args.epochs and not args.no_coaching and epoch_results:
            console.print(
                f"\n  üéì Generating coaching memo for student speakers‚Ä¶"
            )
            try:
                memo = await generate_coaching_memo(
                    results=epoch_results,
                    epoch=epoch,
                    prior_memo=current_coaching_memo,
                )
                coaching_memos.append(memo)
                current_coaching_memo = memo
                console.print(
                    f"  ‚úÖ Coaching memo generated "
                    f"(mean student score: {memo.mean_overall_score:.1f}/10)"
                )
                # Preview key advice
                if memo.actionable_advice:
                    for adv in memo.actionable_advice[:3]:
                        console.print(f"    ‚Üí {adv[:100]}‚Ä¶" if len(adv) > 100 else f"    ‚Üí {adv}")
            except Exception as e:
                log.exception("Coaching memo generation failed for epoch %d", epoch)
                console.print(
                    f"  [bold red]‚ö†Ô∏è Coaching memo FAILED: {type(e).__name__}: {e}[/bold red]"
                )
                console.print(
                    f"  [dim]Continuing to next epoch with previous coaching (or none)‚Ä¶[/dim]"
                )
                # Wait a bit in case it was a rate limit
                await asyncio.sleep(15)

    progress.stop()
    total_elapsed = time.time() - ensemble_start
    completed = len(all_results)
    avg_min = sum(run_times) / len(run_times) / 60 if run_times else 0
    console.print(
        f"\n  ‚úÖ {completed}/{total_runs} runs completed in "
        f"{total_elapsed / 60:.1f} minutes "
        f"(avg {avg_min:.1f} min/run"
        f"{f', {n_failures} failed' if n_failures else ''})\n"
    )

    # --- Overall results table ---
    console.print(f"\n[bold blue]‚ïê‚ïê ALL RESULTS ({len(all_results)} runs) ‚ïê‚ïê[/bold blue]\n")
    print_results_table(all_results, console, title="All Epoch Results")

    prop_wins = sum(1 for r in all_results if r.get("winner") == "PROPOSITION")
    opp_wins = sum(1 for r in all_results if r.get("winner") == "OPPOSITION")
    unknown = len(all_results) - prop_wins - opp_wins

    console.print(
        f"\n  Proposition: {prop_wins}/{len(all_results)} "
        f"({100 * prop_wins / len(all_results):.0f}%)"
    )
    console.print(
        f"  Opposition:  {opp_wins}/{len(all_results)} "
        f"({100 * opp_wins / len(all_results):.0f}%)"
    )
    if unknown:
        console.print(f"  Unresolved:  {unknown}/{len(all_results)}")

    # --- Student learning trajectory ---
    if coaching_memos:
        console.print(
            f"\n[bold blue]‚ïê‚ïê STUDENT LEARNING TRAJECTORY ‚ïê‚ïê[/bold blue]\n"
        )
        for epoch_num in range(1, args.epochs + 1):
            epoch_results = [r for r in all_results if r.get("epoch") == epoch_num]
            student_scores: list[float] = []
            for r in epoch_results:
                div = r.get("division")
                if div and div.rubric:
                    for s in div.rubric.scores:
                        if s.speaker_name.startswith("Student"):
                            student_scores.append(s.overall)
            mean = sum(student_scores) / len(student_scores) if student_scores else 0
            bar = "‚ñà" * int(mean * 2) if mean else ""
            label = "baseline" if epoch_num == 1 else f"coached"
            console.print(
                f"  Epoch {epoch_num} ({label}): "
                f"{mean:.1f}/10 {bar}"
            )

    # --- Save enriched summary + epochs ---
    save_enriched_summary(
        results=all_results,
        coaching_memos=coaching_memos,
        ensemble_dir=ensemble_dir,
        n_epochs=args.epochs,
        runs_per_epoch=args.runs_per_epoch,
    )

    # --- Cross-run synthesis ---
    if not args.no_synth:
        await synthesize_runs(all_results, coaching_memos, console, ensemble_dir)

    console.print(
        f"\n  üìÅ All outputs saved to [bold cyan]{ensemble_dir}[/bold cyan]\n"
    )


if __name__ == "__main__":
    asyncio.run(main())
